{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_program import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_encoder = LabelEncoder()\n",
    "categories_encoder.fit(list(unique_categories))\n",
    "user_id_encoder = LabelEncoder()\n",
    "business_id_encoder = LabelEncoder()\n",
    "business_geohash_encoder = LabelEncoder()\n",
    "\n",
    "user_scaler = StandardScaler()\n",
    "business_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_features_scaled = user_continuous_features_scaled.set_index(user_df['user_id_encoded'].values)\n",
    "business_continuous_features_scaled = business_continuous_features_scaled.set_index(business_df['business_id_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df.merge(user_df[['user_id_encoded', 'average_stars']], on='user_id_encoded', how='left')\n",
    "\n",
    "review_df['stars'] = review_df['stars']/review_df['average_stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features = ['user_id_encoded', 'business_id_encoded', 'stars', 'label']\n",
    "\n",
    "dropped_review = review_df[keep_features]\n",
    "dropped_review = dropped_review.join(user_continuous_features_scaled, on='user_id_encoded', rsuffix='_user')\n",
    "dropped_review = dropped_review.join(business_continuous_features_scaled, on='business_id_encoded', rsuffix='_business')\n",
    "\n",
    "continuous_features = dropped_review.columns.difference(keep_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepfm_mixed_model(num_continuous_features, categorical_info, deep_units=[64, 32, 16], dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Build a DeepFM model that uses both continuous and categorical features.\n",
    "    \n",
    "    Args:\n",
    "      num_continuous_features: Integer, the number of continuous features.\n",
    "      categorical_info: Dictionary mapping categorical feature names to (vocab_size, embed_dim).\n",
    "                        For example: {'user_id_encoded': (num_users, 8), 'business_id_encoded': (num_businesses, 8)}\n",
    "      deep_units: List of integers, sizes of the hidden layers in the deep part.\n",
    "      dropout_rate: Float, dropout rate for the deep layers.\n",
    "    \n",
    "    Returns:\n",
    "      A compiled Keras model with a regression output using MSE loss.\n",
    "    \"\"\"\n",
    "    # Input layer for continuous features.\n",
    "    input_cont = Input(shape=(num_continuous_features,), name=\"continuous_input\")\n",
    "    \n",
    "    # Process categorical features: create an input and embedding layer for each.\n",
    "    categorical_inputs = []\n",
    "    categorical_embeddings = []\n",
    "    for feature_name, (vocab_size, embed_dim) in categorical_info.items():\n",
    "        inp = Input(shape=(1,), name=feature_name)\n",
    "        emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, name=f\"{feature_name}_emb\")(inp)\n",
    "        emb_flat = layers.Flatten(name=f\"{feature_name}_flat\")(emb)\n",
    "        categorical_inputs.append(inp)\n",
    "        categorical_embeddings.append(emb_flat)\n",
    "    \n",
    "    # Combine continuous features with flattened categorical embeddings.\n",
    "    if categorical_embeddings:\n",
    "        deep_input = layers.Concatenate(name=\"deep_concat\")([input_cont] + categorical_embeddings)\n",
    "    else:\n",
    "        deep_input = input_cont\n",
    "    \n",
    "    # --- Linear Part ---\n",
    "    # For simplicity, the linear part uses only the continuous features.\n",
    "    linear_part = layers.Dense(1, activation=None, name=\"linear_part\")(input_cont)\n",
    "    \n",
    "    # --- Deep Part ---\n",
    "    deep = deep_input\n",
    "    for i, units in enumerate(deep_units):\n",
    "        deep = layers.Dense(units, activation='relu', name=f\"deep_dense_{i}\")(deep)\n",
    "        deep = layers.Dropout(dropout_rate, name=f\"deep_dropout_{i}\")(deep)\n",
    "    deep_output = layers.Dense(1, activation=None, name=\"deep_output\")(deep)\n",
    "    \n",
    "    # --- Combine Linear and Deep Parts ---\n",
    "    combined_logit = layers.Add(name=\"combined\")([linear_part, deep_output])\n",
    "    \n",
    "    # For regression (rating prediction), we use a linear output.\n",
    "    output = combined_logit\n",
    "    \n",
    "    # Build the model including both continuous and categorical inputs.\n",
    "    inputs = [input_cont] + categorical_inputs\n",
    "    model = Model(inputs=inputs, outputs=output, name=\"DeepFM_mixed\")\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Suppose:\n",
    "#   - The number of continuous features is determined by your DataFrame, e.g., len(continuous_features)\n",
    "#   - categorical_info includes:\n",
    "#       'user_id_encoded': (num_users, 8)\n",
    "#       'business_id_encoded': (num_businesses, 8)\n",
    "categorical_info = {\n",
    "    'user_id_encoded': (num_users, 8), \n",
    "    'business_id_encoded': (num_businesses, 8)\n",
    "}\n",
    "\n",
    "num_cont_features = len(continuous_features)  # Your continuous feature columns from dropped_review\n",
    "\n",
    "model = build_deepfm_mixed_model(num_cont_features, categorical_info, dropout_rate=0.3)\n",
    "# model.summary()\n",
    "\n",
    "# For continuous features:\n",
    "X_cont = dropped_review[continuous_features].values\n",
    "\n",
    "# For categorical features (ensuring they are arrays of shape (num_samples, 1)):\n",
    "X_user = dropped_review['user_id_encoded'].values.reshape(-1, 1)\n",
    "X_business = dropped_review['business_id_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "# Prepare labels, e.g., the 'stars' column.\n",
    "y = dropped_review['stars'].values\n",
    "\n",
    "X_cont_train, X_cont_test, \\\n",
    "X_user_train, X_user_test, \\\n",
    "X_business_train, X_business_test, \\\n",
    "y_train, y_test = train_test_split(X_cont, X_user, X_business, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784333"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_cont_train, X_user_train, X_business_train], y_train, epochs=3, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = 'Saved_DeepFM/'\n",
    "\n",
    "# Save the models\n",
    "model.save(save_folder_path + 'DeepFM.keras')\n",
    "\n",
    "# Save the label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_id_encoder, f)\n",
    "\n",
    "# with open(save_folder_path + 'categories_encoder.pkl', 'wb') as f:\n",
    "#     pickle.dump(categories_encoder, f)\n",
    "    \n",
    "# Save the scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(business_scaler, f)\n",
    "\n",
    "# Save the user continuous features\n",
    "np.save(save_folder_path + 'user_continuous_features.npy', user_continuous_features_scaled)\n",
    "\n",
    "# Save the business continuous features\n",
    "np.save(save_folder_path + 'business_continuous_features.npy', business_continuous_features_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_users_with_negatives(\n",
    "    test_df: pd.DataFrame,\n",
    "    all_business_ids: np.ndarray,\n",
    "    N: int = 100,\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each user in test_df, randomly sample additional businesses\n",
    "    (not already in that user's rows) until they have N total rows.\n",
    "    The new rows get stars = NaN.\n",
    "\n",
    "    Args:\n",
    "        test_df: DataFrame with columns ['user_id_encoded', 'business_id_encoded', 'stars'].\n",
    "        all_business_ids: array of all possible business_id_encoded values.\n",
    "        N: desired number of records per user.\n",
    "        seed: random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        padded_df: original rows plus sampled negatives with stars=NaN.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out_rows = []\n",
    "\n",
    "    # Group existing by user\n",
    "    grouped = test_df.groupby('user_id_encoded')\n",
    "    for user_id, group in grouped:\n",
    "        existing = set(group['business_id_encoded'])\n",
    "        n_existing = len(group)\n",
    "        n_to_sample = max(0, N - n_existing)\n",
    "\n",
    "        # Sample from the complement\n",
    "        candidates = np.setdiff1d(all_business_ids, list(existing), assume_unique=True)\n",
    "        sampled = rng.choice(candidates, size=n_to_sample, replace=False)\n",
    "\n",
    "        # Build DataFrame of sampled negatives\n",
    "        neg_df = pd.DataFrame({\n",
    "            'user_id_encoded': user_id,\n",
    "            'business_id_encoded': sampled,\n",
    "            'stars': np.nan  # unknown\n",
    "        })\n",
    "\n",
    "        # Append existing + negatives\n",
    "        out_rows.append(group)\n",
    "        out_rows.append(neg_df)\n",
    "\n",
    "    # Combine and return\n",
    "    padded_df = pd.concat(out_rows, ignore_index=True)\n",
    "    return padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build initial test_df\n",
    "test_df = pd.DataFrame({\n",
    "    'user_id_encoded': X_user_test.flatten(),\n",
    "    'business_id_encoded': X_business_test.flatten(),\n",
    "    'stars':            y_test.flatten()\n",
    "})\n",
    "# keep only users with at least one interaction\n",
    "test_df = test_df[test_df['stars'] >= 1.1]\n",
    "\n",
    "# 2) Get list of all business IDs\n",
    "all_business_ids = business_df['business_id_encoded'].unique()\n",
    "\n",
    "# 3) Pad each user up to 100 records\n",
    "padded_test_df = pad_users_with_negatives(\n",
    "    test_df,\n",
    "    all_business_ids=all_business_ids,\n",
    "    N=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "counts = padded_test_df.groupby('user_id_encoded').size()\n",
    "assert counts.min() == 100 and counts.max() == 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test_df = padded_test_df.join(user_continuous_features_scaled, on='user_id_encoded', rsuffix='_user')\n",
    "padded_test_df = padded_test_df.join(business_continuous_features_scaled, on='business_id_encoded', rsuffix='_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m135925/135925\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 948us/step\n"
     ]
    }
   ],
   "source": [
    "# Assume 'df_filtered' contains your test set with all users.\n",
    "keep_features = ['user_id_encoded', 'business_id_encoded', 'stars']\n",
    "testing_features = padded_test_df.columns.difference(keep_features)\n",
    "\n",
    "# Prepare features and predict ratings for all rows.\n",
    "X_cont_test = padded_test_df[testing_features].values\n",
    "X_user_test = padded_test_df['user_id_encoded'].values.reshape(-1, 1)\n",
    "X_business_test = padded_test_df['business_id_encoded'].values.reshape(-1, 1)\n",
    "\n",
    "padded_test_df['predicted_rating'] = model.predict([X_cont_test, X_user_test, X_business_test])\n",
    "\n",
    "# Compute ranking within each user group.\n",
    "padded_test_df['ranking'] = padded_test_df.groupby('user_id_encoded')['predicted_rating'] \\\n",
    "                                      .rank(method='min', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43496"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_test_df['user_id_encoded'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Evaluation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>First Relevant Rank</th>\n",
       "      <th>NDCG@10</th>\n",
       "      <th>NDCG_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2091</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1651</td>\n",
       "      <td>0.3335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MRR  First Relevant Rank  NDCG@10  NDCG_all\n",
       "0  0.2091                  5.0   0.1651    0.3335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose K for NDCG and entropy\n",
    "K = 10\n",
    "\n",
    "# Helper: NDCG@K for one user\n",
    "def ndcg_at_k(ranked_labels, k):\n",
    "    dcg = 0.0\n",
    "    for i, rel in enumerate(ranked_labels[:k], start=1):\n",
    "        dcg += (2**rel - 1) / np.log2(i + 1)\n",
    "    # ideal DCG: all positives first\n",
    "    ideal = sorted(ranked_labels, reverse=True)\n",
    "    idcg = sum((2**rel - 1) / np.log2(i + 1)\n",
    "               for i, rel in enumerate(ideal[:k], start=1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# Helper: entropy of top-K labels\n",
    "def list_entropy(labels):\n",
    "    p_pos = labels.mean()\n",
    "    p_neg = 1 - p_pos\n",
    "    ent = 0.0\n",
    "    for p in (p_pos, p_neg):\n",
    "        if p > 0:\n",
    "            ent -= p * np.log2(p)\n",
    "    return ent\n",
    "\n",
    "# Containers\n",
    "mrrs, ndcgs, ndcgs_all = [], [], []\n",
    "\n",
    "# assign the real_label column to the padded_test_df if stars is not NaN\n",
    "padded_test_df['real_label'] = np.where(padded_test_df['stars'].isna(), 0, 1)\n",
    "\n",
    "# Iterate per user\n",
    "for user_id, group in padded_test_df.groupby('user_id_encoded'):\n",
    "    # Sort by predicted_rating descending\n",
    "    grp = group.sort_values('predicted_rating', ascending=False)\n",
    "    labels = grp['real_label'].values\n",
    "    \n",
    "    # MRR: reciprocal rank of first positive\n",
    "    pos_indices = np.where(labels == 1)[0]\n",
    "    if len(pos_indices) > 0:\n",
    "        mrrs.append(1.0 / (pos_indices[0] + 1))\n",
    "    else:\n",
    "        mrrs.append(0.0)\n",
    "    \n",
    "    # NDCG@K\n",
    "    ndcgs.append(ndcg_at_k(labels, K))\n",
    "\n",
    "    # NDCG@K for all\n",
    "    ndcgs_all.append(ndcg_at_k(labels, len(labels)))\n",
    "\n",
    "# Aggregate\n",
    "results = {\n",
    "    'MRR':       np.mean(mrrs),\n",
    "    f'First Relevant Rank':  int(round(1/np.mean(mrrs),0)),\n",
    "    f'NDCG@{K}': np.mean(ndcgs),\n",
    "    f'NDCG_all': np.mean(ndcgs_all),\n",
    "}\n",
    "\n",
    "# Display\n",
    "results_df = pd.Series(results).round(4).to_frame().T\n",
    "print(\"Ranking Evaluation Metrics\")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
