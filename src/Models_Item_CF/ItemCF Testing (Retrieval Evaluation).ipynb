{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-base Collaborative Filtering - Retrieval Evaluation\n",
    "This notebook is used to simulate the performance of the item-based collaborative filtering algorithm in a retrieval setting. \n",
    "\n",
    "#### Pre-requisites\n",
    "- The model is trained and the index is created in the notebook `ItemCF Model & Index.ipynb`.\n",
    "- The index is saved in the file `yelp_ItemCF.db` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python file from ../utilities.py\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "\n",
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# split the data into training and test sets\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get businesses a user interacted with\n",
    "def get_user_businesses(user_id, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT business_id, stars_review FROM user_item_index WHERE user_id = ?''', (user_id,))\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-k similar businesses for a given business\n",
    "def get_top_k_similar_businesses(business_id, k, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT similarity_vector FROM item_item_similarity WHERE item_id = ?''', (business_id,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result is None:\n",
    "        return []\n",
    "\n",
    "    similarity_vector = pickle.loads(result[0])\n",
    "    indices, data = similarity_vector\n",
    "\n",
    "    # Get top-k similar businesses\n",
    "    top_k = sorted(zip(indices, data), key=lambda x: -x[1])[:k]\n",
    "\n",
    "    # Map indices to business ids\n",
    "    similar_businesses = [(list(business_mapping.keys())[idx], score) for idx, score in top_k]\n",
    "\n",
    "    return similar_businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict user interests based on similar businesses\n",
    "def predict_user_interests(user_id, k=10, conn=conn):       # k is the number of recommendations to make\n",
    "    user_businesses = get_user_businesses(user_id, conn)\n",
    "\n",
    "    recommended_businesses = {}\n",
    "    for business_id, _ in user_businesses:\n",
    "        similar_businesses = get_top_k_similar_businesses(business_id, k, conn)\n",
    "\n",
    "        for similar_business_id, score in similar_businesses:\n",
    "            if similar_business_id in recommended_businesses:\n",
    "                recommended_businesses[similar_business_id] += score\n",
    "            else:\n",
    "                recommended_businesses[similar_business_id] = score\n",
    "\n",
    "    # Sort recommendations by score\n",
    "    recommended_businesses = sorted(recommended_businesses.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return recommended_businesses[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 136473\n",
      "Number of negative reviews: 32929\n",
      "Total number of reviews: 197147\n",
      "Ratio of positive to negative reviews: 4.14\n"
     ]
    }
   ],
   "source": [
    "# get the number of positive and negative reviews in the test data\n",
    "positive_reviews = test_data[test_data['stars_review'] >= 4]\n",
    "negative_reviews = test_data[test_data['stars_review'] <= 2]\n",
    "\n",
    "print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Total number of reviews: {len(test_data)}\")\n",
    "print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_test_data(positive_reviews, negative_reviews):\n",
    "    # down-sample the positive reviews to balance the dataset\n",
    "    positive_reviews_downsampled = positive_reviews.sample(n=len(negative_reviews), random_state=42)\n",
    "\n",
    "    # combine the down-sampled positive reviews with the negative reviews\n",
    "    balanced_test_data = pd.concat([positive_reviews_downsampled, negative_reviews], ignore_index=True)\n",
    "\n",
    "    # shuffle the balanced test data\n",
    "    balanced_test_data = balanced_test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # new statistics for the balanced test data\n",
    "    positive_reviews = balanced_test_data[balanced_test_data['stars_review'] >= 4]\n",
    "    negative_reviews = balanced_test_data[balanced_test_data['stars_review'] <= 2]\n",
    "\n",
    "    print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "    print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "    print(f\"Total number of reviews: {len(balanced_test_data)}\")\n",
    "    print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n",
    "    return balanced_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 32929\n",
      "Number of negative reviews: 32929\n",
      "Total number of reviews: 65858\n",
      "Ratio of positive to negative reviews: 1.00\n"
     ]
    }
   ],
   "source": [
    "# balance the test data, comment this line to use the original test data\n",
    "test_data = balance_test_data(positive_reviews, negative_reviews)\n",
    "\n",
    "# group the test data by user_id and get the business_id\n",
    "test_data_grouped = test_data.groupby('user_id')['business_id'].apply(list).reset_index()\n",
    "\n",
    "# get the recommendations for each user in the test data\n",
    "recommendations = {}\n",
    "\n",
    "i = 0\n",
    "for user_id in test_data_grouped['user_id']:\n",
    "    recommendation = predict_user_interests(user_id, k=300, conn=conn)\n",
    "    business_ids, scores = [], []\n",
    "    for business_id, score in recommendation:\n",
    "        business_ids.append(business_id)\n",
    "        scores.append(score)\n",
    "    recommendations[user_id] = (business_ids, scores) \n",
    "    i += 1\n",
    "    # i is used to limit the number of recommendations to display\n",
    "    if i == 1000:\n",
    "        break\n",
    "\n",
    "# run time: 21 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recommendations(recommendations, test_data_grouped, pos=4):\n",
    "    total = 0\n",
    "    total_positive = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    ranks = []\n",
    "    for i, row in test_data_grouped.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        business_ids = row['business_id']\n",
    "        rank = 0\n",
    "        if user_id in recommendations:\n",
    "            recommended_businesses = recommendations[user_id][0]\n",
    "            for business_id in business_ids:\n",
    "                star_rating = test_data[(test_data['user_id'] == user_id) & (test_data['business_id'] == business_id)]['stars_review'].values[0]\n",
    "                if star_rating >= pos:\n",
    "                    total_positive += 1\n",
    "                if business_id in recommended_businesses:\n",
    "                    if star_rating >= pos:\n",
    "                        true_positive += 1\n",
    "                    else:\n",
    "                        false_positive += 1\n",
    "                    # get the rank of the business_id in the recommendations\n",
    "                    rank = recommended_businesses.index(business_id) + 1\n",
    "                else:\n",
    "                    if star_rating < pos:\n",
    "                        true_negative += 1\n",
    "                    else:\n",
    "                        false_negative += 1\n",
    "            total += len(business_ids)\n",
    "        ranks.append(rank)\n",
    "    return true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks = check_recommendations(recommendations, test_data_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Positive</th>\n",
       "      <th>Total Negative</th>\n",
       "      <th>Total</th>\n",
       "      <th>Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832</td>\n",
       "      <td>840</td>\n",
       "      <td>1672</td>\n",
       "      <td>0.497608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Positive  Total Negative  Total     Ratio\n",
       "0             832             840   1672  0.497608"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F-beta Score</th>\n",
       "      <th>Mean Reciprocal Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5012</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  F-beta Score  Mean Reciprocal Rank\n",
       "0    0.5012     0.3333  0.0024    0.0048         0.003                0.0703"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Positive</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>836</td>\n",
       "      <td>4</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True Positive  True Negative  False Positive  False Negative\n",
       "0              2            836               4             830"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = (true_positive + true_negative) / float(total) if total > 0 else 0\n",
    "precision = true_positive / float(true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "recall = true_positive / float(total_positive) if total_positive > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Mean Reciprocal Rank (MRR) - safer handling\n",
    "mean_reciprocal_rank = np.mean([1 / rank for rank in ranks if rank is not None and rank > 0]) if ranks else 0\n",
    "\n",
    "# Weighted FÎ²-score\n",
    "beta = 2\n",
    "f_beta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (beta**2 * precision + recall) > 0 else 0\n",
    "\n",
    "# Compute dataset statistics\n",
    "total_negative = total - total_positive if total > 0 else 0\n",
    "background_stats = pd.DataFrame({\n",
    "    'Total Positive': [total_positive],\n",
    "    'Total Negative': [total_negative],\n",
    "    'Total': [total],\n",
    "    'Ratio': [total_positive / float(total) if total > 0 else 0],\n",
    "})\n",
    "\n",
    "print(\"Testing Data Statistics\")\n",
    "display(background_stats)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation_metric = pd.DataFrame({\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1 Score': [f1_score],\n",
    "    'F-beta Score': [f_beta],\n",
    "    'Mean Reciprocal Rank': [mean_reciprocal_rank],\n",
    "}).apply(lambda x: round(x, 4))\n",
    "\n",
    "print(\"Evaluation Metrics\")\n",
    "display(evaluation_metric)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = pd.DataFrame({\n",
    "    'True Positive': [true_positive],\n",
    "    'True Negative': [true_negative],\n",
    "    'False Positive': [false_positive],\n",
    "    'False Negative': [false_negative]\n",
    "})\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
