{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-base Collaborative Filtering - Model & Index\n",
    "This notebook demonstrates how to build a Item-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_ItemCF.db` file.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_ItemCF.db` file to the `../../data/processed_data` folder.\n",
    "2. Update the `ItemCF.py` file in the `../backend/models` folder if there is changes in retrieval process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python file from ../utilities.py\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "\n",
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 0.0000000005\n",
    "\n",
    "current_timestamp = int(time.time())\n",
    "\n",
    "df_review['timestamp'] = pd.to_datetime(df_review['date']).astype(int) // 10**9\n",
    "df_review['timestamp'] = np.exp(-LAMBDA * (current_timestamp - df_review[\"timestamp\"]))\n",
    "df_review['stars'] = df_review['timestamp'] * df_review['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_business_with_time(df_business, df_review):\n",
    "    df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "    user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]\n",
    "\n",
    "    user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "    business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}    \n",
    "    return user_mapping, business_mapping, user_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping, business_mapping, user_business = get_user_business_with_time(df_business, df_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)\n",
    "\n",
    "user_business = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix using weighted_stars\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace NaN values in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert ratings to binary (1 if interacted, 0 otherwise)\n",
    "# binary_user_item_sparse = (user_item_sparse > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "    \n",
    "    # Compute the top N cosine similarities in a sparse format\n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_similarity_topn(A, top_n=50, threshold=0.01):\n",
    "#     \"\"\"\n",
    "#     Compute Jaccard similarity for items in a sparse user-item matrix efficiently.\n",
    "#     Returns a sparse matrix containing only the top N similar items per item.\n",
    "#     \"\"\"\n",
    "#     # Convert to binary interactions\n",
    "#     A_bin = (A > 0).astype(int)\n",
    "\n",
    "#     # Compute intersection (co-occurrence): A_bin.T @ A_bin\n",
    "#     intersection = A_bin.T @ A_bin\n",
    "\n",
    "#     # Compute item-wise interaction counts (sparse)\n",
    "#     item_sums = np.array(A_bin.sum(axis=0)).flatten()\n",
    "\n",
    "#     # Compute union using broadcasting (sparse)\n",
    "#     row_indices, col_indices = intersection.nonzero()  # Get non-zero indices\n",
    "#     intersection_values = intersection.data  # Co-occurrence values\n",
    "\n",
    "#     # Compute union: |A| + |B| - |A âˆ© B|\n",
    "#     union_values = item_sums[row_indices] + item_sums[col_indices] - intersection_values\n",
    "\n",
    "#     # Compute Jaccard similarity\n",
    "#     jaccard_values = intersection_values / union_values\n",
    "\n",
    "#     # Apply thresholding\n",
    "#     mask = jaccard_values >= threshold\n",
    "#     row_indices, col_indices, jaccard_values = row_indices[mask], col_indices[mask], jaccard_values[mask]\n",
    "\n",
    "#     # Create sparse matrix for efficient storage\n",
    "#     jaccard_sim_sparse = sp.csr_matrix(\n",
    "#         (jaccard_values, (row_indices, col_indices)), shape=(A.shape[1], A.shape[1])\n",
    "#     )\n",
    "\n",
    "#     # Keep only top-N similar items\n",
    "#     jaccard_sim_sparse = sp_matmul_topn(jaccard_sim_sparse, jaccard_sim_sparse, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "#     return jaccard_sim_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute item similarity\n",
    "item_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_db(conn):\n",
    "    \"\"\"Apply SQLite performance optimizations.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def insert_user_item(user_business, conn, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for user-item interactions.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_records = len(user_business)\n",
    "    data = user_business[['user_id', 'business_id', 'stars_review']].values.tolist()\n",
    "\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''', batch)\n",
    "\n",
    "        if i % (batch_size * 5) == 0:  # Commit every 5 batches\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} user-item records.\")\n",
    "\n",
    "    conn.commit()  # Final commit\n",
    "    print(f\"Total {total_records} user-item records inserted.\")\n",
    "\n",
    "\n",
    "def insert_item_vectors(item_similarity_sparse, business_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    \"\"\"Optimized batch insert for item similarity vectors.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    business_keys = list(business_mapping.keys())  # Convert keys to list for faster indexing\n",
    "\n",
    "    for row_idx in range(item_similarity_sparse.shape[0]):\n",
    "        row_vector = item_similarity_sparse.getrow(row_idx)\n",
    "        row_indices = row_vector.indices\n",
    "        row_data = row_vector.data\n",
    "\n",
    "        serialized_row = pickle.dumps((row_indices, row_data))\n",
    "        item_id = business_keys[row_idx]  # Faster lookup\n",
    "\n",
    "        batch.append((item_id, serialized_row))\n",
    "\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} item vectors...\")\n",
    "\n",
    "            batch = []\n",
    "\n",
    "    if batch:  # Insert remaining records\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        total_inserted += len(batch)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_inserted} item vectors inserted.\")\n",
    "\n",
    "\n",
    "def insert_mappings(business_mapping, conn, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for business mappings.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    data = list(business_mapping.items())\n",
    "    total_records = len(data)\n",
    "\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO business_mapping (business_id, business_idx)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "\n",
    "        if i % (batch_size * 5) == 0:  # Commit every 5 batches\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} business mappings.\")\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} business mappings inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "# Create tables for user-item and item-item indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS item_item_similarity (\n",
    "    item_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_item_similarity ON item_item_similarity(item_id)''')\n",
    "\n",
    "# cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "#     user_id TEXT PRIMARY KEY,\n",
    "#     user_idx INTEGER\n",
    "# )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY,\n",
    "    business_idx INTEGER\n",
    ")''')\n",
    "\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 788585 user-item records.\n",
      "Inserted 300000 / 788585 user-item records.\n",
      "Inserted 550000 / 788585 user-item records.\n",
      "Inserted 788585 / 788585 user-item records.\n",
      "Total 788585 user-item records inserted.\n",
      "Inserted 50000 item vectors...\n",
      "Total 78059 item vectors inserted.\n",
      "Inserted 50000 / 78059 business mappings.\n",
      "Total 78059 business mappings inserted.\n"
     ]
    }
   ],
   "source": [
    "insert_user_item(user_business, conn)\n",
    "insert_item_vectors(item_similarity_sparse, business_mapping, conn)\n",
    "insert_mappings(business_mapping, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection when done\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-mCG3KVaj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
