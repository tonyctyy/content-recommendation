{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a POC for predicting user and business interest using the paper from Facebook. It is using Triplet Hinge Loss as the loss function. The goal is to separate the positive and negative samples by a margin, so that the positive samples are closer to the anchor than the negative samples. \n",
    "\n",
    "## Improvement Plan\n",
    "### About the Model\n",
    "1. Categorical features will be one-hot encoded/embedded.\n",
    "2. Continuous features will be normalized and handle null value.\n",
    "3. Default values will be set for unseen values in the embedding layer.\n",
    "4. Model will be tuned and optimized.\n",
    "5. More features will be added (e.g. review content, text features, etc.)\n",
    "6. Sampling bias will be discovered\n",
    "7. Adjust the dimension of each layer\n",
    "### About using the Model\n",
    "1. The model details will be saved and loaded.\n",
    "2. How to use the model will be explained.\n",
    "3. The model will be used in a web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_encoder = LabelEncoder()\n",
    "categories_encoder.fit(list(unique_categories))\n",
    "user_id_encoder = LabelEncoder()\n",
    "business_id_encoder = LabelEncoder()\n",
    "\n",
    "user_scaler = StandardScaler()\n",
    "business_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['user_id_encoded'] = user_id_encoder.transform(review_df['user_id'])\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['business_id_encoded'] = business_id_encoder.transform(review_df['business_id'])\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Encode 'city' as a discrete feature for businesses\n",
    "# business_city_encoder = LabelEncoder()\n",
    "# business_df['city_encoded'] = business_city_encoder.fit_transform(business_df['city'])\n",
    "\n",
    "# # Save number of unique cities for embedding input_dim\n",
    "# num_cities = business_df['city_encoded'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(input_dim, output_dim, name):\n",
    "    \"\"\"Reusable function to create an embedding layer.\"\"\"\n",
    "    return layers.Embedding(input_dim=input_dim, output_dim=output_dim, name=f\"{name}_embedding\")\n",
    "\n",
    "# Create embedding layers\n",
    "user_id_embedding = create_embedding_layer(num_users, 16, \"user_id\")\n",
    "business_id_embedding = create_embedding_layer(num_businesses, 16, \"business_id\")\n",
    "category_embedding = create_embedding_layer(num_categories, 16, \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example aggregation function for TensorFlow (Mean Pooling)\n",
    "def aggregate_category_embeddings(category_indices):\n",
    "    category_indices = tf.constant(category_indices, dtype=tf.int32)\n",
    "    embeddings = category_embedding(category_indices)\n",
    "    return tf.reduce_mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    user_id_input = layers.Input(shape=(1,), name=\"user_id\")\n",
    "    user_continuous_input = layers.Input(shape=(continuous_dim,), name=\"user_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    user_id_embedded = user_id_embedding(user_id_input)\n",
    "    user_id_embedded = layers.Flatten()(user_id_embedded)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([user_id_embedded, user_continuous_input])\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    user_embedding = layers.Dense(16, activation=None, name=\"user_embedding\")(x)\n",
    "\n",
    "    return Model([user_id_input, user_continuous_input], user_embedding, name=\"UserTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    business_id_input = layers.Input(shape=(1,), name=\"business_id\")\n",
    "    business_continuous_input = layers.Input(shape=(continuous_dim,), name=\"business_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    business_id_embedded = business_id_embedding(business_id_input)\n",
    "    business_id_embedded = layers.Flatten()(business_id_embedded)\n",
    "\n",
    "    category_input = layers.Input(shape=(None,), dtype=\"int32\", name=\"category_indices\")  # Variable-length input\n",
    "    category_embeddings = category_embedding(category_input)\n",
    "    # aggregated_category_embedding = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"category_pooling\", output_shape=(16,))(category_embeddings)\n",
    "    aggregated_category_embedding = CategoryPoolingLayer(name=\"category_pooling\")(category_embeddings)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([business_id_embedded, aggregated_category_embedding, business_continuous_input])\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    business_embedding = layers.Dense(16, activation=None, name=\"business_embedding\")(x)\n",
    "\n",
    "    return Model([business_id_input, category_input, business_continuous_input], business_embedding, name=\"ItemTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "def triplet_hinge_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_pred shape: (batch_size, 3, embedding_dim)\n",
    "        anchor, positive, negative = tf.unstack(y_pred, num=3, axis=1)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        # Hinge loss: max(0, pos_dist - neg_dist + margin)\n",
    "        return tf.reduce_mean(tf.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate triplets\n",
    "def generate_triplets(review_df, num_neg_samples=1):\n",
    "    triplets = []\n",
    "    grouped = review_df.groupby('user_id_encoded')\n",
    "\n",
    "    for user_id, group in grouped:\n",
    "        positive_samples = group[group['label'] == 1]\n",
    "        negative_samples = group[group['label'] == 0]\n",
    "        \n",
    "        if positive_samples.empty or negative_samples.empty:\n",
    "            continue  # Skip users without both positive and negative samples\n",
    "        \n",
    "        for _, pos_row in positive_samples.iterrows():\n",
    "            for _, neg_row in negative_samples.sample(num_neg_samples, replace=True).iterrows():\n",
    "                triplets.append((\n",
    "                    user_id,\n",
    "                    pos_row['business_id_encoded'],\n",
    "                    neg_row['business_id_encoded']\n",
    "                ))\n",
    "    \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate triplets for training and testing\n",
    "train_triplets = generate_triplets(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs\n",
    "def prepare_triplet_inputs(triplets, user_features, business_features, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    # Replace NaN values with empty lists in `business_category_map`\n",
    "    business_category_map = business_category_map.apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    anchor_indices = triplets[:, 0]\n",
    "    positive_indices = triplets[:, 1]\n",
    "    negative_indices = triplets[:, 2]\n",
    "\n",
    "    anchor_features = [anchor_indices, user_features.take(anchor_indices, axis=0).values]\n",
    "    positive_features = [\n",
    "        positive_indices,\n",
    "        pad_sequences(business_category_map.loc[positive_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(positive_indices, axis=0).values, \n",
    "\n",
    "]\n",
    "    negative_features = [\n",
    "        negative_indices, \n",
    "        pad_sequences(business_category_map.loc[negative_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(negative_indices, axis=0).values,\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        anchor_features[0], anchor_features[1],\n",
    "        positive_features[0], positive_features[1], positive_features[2],\n",
    "        negative_features[0], negative_features[1], negative_features[2]\n",
    "    ]\n",
    "\n",
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "max_category_length = MAX_CATEGORY_LENGTH\n",
    "\n",
    "train_inputs = prepare_triplet_inputs(train_triplets, user_continuous_features_scaled, business_continuous_features_scaled, business_category_map, max_category_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate towers\n",
    "user_model = user_tower(user_continuous_features_scaled.shape[1])\n",
    "item_model = item_tower(business_continuous_features_scaled.shape[1])\n",
    "\n",
    "# Define inputs for user and business towers\n",
    "user_inputs_model = [Input(shape=(1,), dtype=tf.int32, name=\"user_id_input\"),\n",
    "                     Input(shape=(user_continuous_features_scaled.shape[1],), name=\"user_cont_features_input\")]\n",
    "\n",
    "positive_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"positive_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"positive_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"positive_cont_features_input\")\n",
    "]\n",
    "\n",
    "negative_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"negative_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"negative_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"negative_cont_features_input\")\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "anchor_embedding = user_model(user_inputs_model)\n",
    "positive_embedding = item_model(positive_inputs_model)\n",
    "negative_embedding = item_model(negative_inputs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_embeddings(embeddings):\n",
    "    # Unpack the embeddings from the list\n",
    "    anchor, positive, negative = embeddings\n",
    "    return tf.stack([anchor, positive, negative], axis=1)\n",
    "\n",
    "triplet_embeddings = Lambda(stack_embeddings, name=\"triplet_embeddings\")(\n",
    "    [anchor_embedding, positive_embedding, negative_embedding]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = user_inputs_model + positive_inputs_model + negative_inputs_model\n",
    "# Build the model\n",
    "triplet_model = Model(\n",
    "    inputs= all_inputs,\n",
    "    outputs=triplet_embeddings,\n",
    "    name=\"triplet_model\"\n",
    ")\n",
    "\n",
    "# Compile with triplet loss\n",
    "triplet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=triplet_hinge_loss(margin=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m12867/12867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 14ms/step - loss: 0.1547\n",
      "Epoch 2/3\n",
      "\u001b[1m12867/12867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 14ms/step - loss: 0.0704\n",
      "Epoch 3/3\n",
      "\u001b[1m12867/12867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 14ms/step - loss: 0.0403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19f02d648d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the triplet model\n",
    "triplet_model.fit(\n",
    "    x=train_inputs,\n",
    "    y=np.zeros(len(train_inputs[0])),  # Dummy labels, as loss is computed from embeddings\n",
    "    batch_size=32,\n",
    "    # epochs=10,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = 'Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "# Save the models\n",
    "user_model.save(save_folder_path + 'user_model.keras')\n",
    "item_model.save(save_folder_path + 'item_model.keras')\n",
    "\n",
    "# Save the label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(categories_encoder, f)\n",
    "    \n",
    "# Save the scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(business_scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following section will load the saved models and compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler = load_saved_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter test data\n",
    "test_df = test_df[\n",
    "    (test_df['user_id'].isin(user_id_encoder.classes_)) & \n",
    "    (test_df['business_id'].isin(business_id_encoder.classes_))\n",
    "]\n",
    "\n",
    "# Encode user_id and business_id using the loaded encoders\n",
    "test_df['user_id_encoded'] = user_id_encoder.transform(test_df['user_id'])\n",
    "test_df['business_id_encoded'] = business_id_encoder.transform(test_df['business_id'])\n",
    "\n",
    "# Step 2: Prepare inputs for user and business embeddings\n",
    "# Extract user features\n",
    "test_user_ids = test_df['user_id_encoded'].values\n",
    "test_user_cont_features = user_scaler.transform(user_continuous_features_scaled.loc[test_user_ids].values)\n",
    "\n",
    "# Extract business features\n",
    "test_business_ids = test_df['business_id_encoded'].values\n",
    "test_business_cont_features = business_scaler.transform(business_continuous_features_scaled.loc[test_business_ids].values)\n",
    "test_business_categories = business_category_map.loc[test_business_ids].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "test_business_category_padded = pad_sequences(test_business_categories.tolist(), maxlen=5, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6128/6128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n",
      "\u001b[1m6128/6128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Predict embeddings using the loaded models\n",
    "test_user_embeddings = user_model.predict([test_user_ids, test_user_cont_features])\n",
    "test_business_embeddings = item_model.predict([test_business_ids, test_business_category_padded, test_business_cont_features])\n",
    "\n",
    "# Step 4: Compute cosine similarity for each user-business pair\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Create a list of cosine similarities for each record in test_df\n",
    "test_df['predicted_similarity'] = [\n",
    "    compute_cosine_similarity(test_user_embeddings[i], test_business_embeddings[i])\n",
    "    for i in range(len(test_df))\n",
    "]\n",
    "\n",
    "# Step 5: Set the predicted_label based on similarity score\n",
    "test_df['predicted_label'] = (test_df['predicted_similarity'] >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59\n",
      "AUC: 0.52\n",
      "Precision: 0.69\n",
      "Recall: 0.74\n",
      "F1 Score: 0.72\n",
      "Confusion Matrix:\n",
      "[[ 15472  44398]\n",
      " [ 35633 100581]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Evaluate the performance\n",
    "accuracy = (test_df['label'] == test_df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(test_df['label'], test_df['predicted_similarity'])\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "# Compute precision and recall\n",
    "precision = precision_score(test_df['label'], test_df['predicted_label'])\n",
    "recall = recall_score(test_df['label'], test_df['predicted_label'])\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(test_df['label'], test_df['predicted_label'])\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
