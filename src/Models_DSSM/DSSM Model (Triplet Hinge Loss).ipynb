{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Triplet Hinge Loss\n",
    "This notebook is used to train the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to predict user and business interest based on the user and business features. The model is trained using the Triplet Hinge Loss as the loss function according the the paper from Facebook. The goal is to separate the positive and negative samples by a margin, so that the positive samples are closer to the anchor than the negative samples. \n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `user_model.keras` - The trained user model for retrieval. \n",
    "2. `user_id_encoder.pkl` - The label encoder for the user id.\n",
    "3. `user_scaler.pkl` - The scaler for the user features.\n",
    "4. `business_model.keras` - The trained business model for retrieval.\n",
    "5. `business_id_encoder.pkl` - The label encoder for the business id.\n",
    "6. `categories_encoder.pkl` - The label encoder for the business categories.\n",
    "7. `business_scaler.pkl` - The scaler for the business features.\n",
    "    \n",
    "#### Move to Production\n",
    "1. Before moving to production, the model should be indexed in the `DSSM Index (Faiss).ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business_details table.\n",
      "Loaded 360656 rows from business_categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_encoder = LabelEncoder()\n",
    "user_id_encoder = LabelEncoder()\n",
    "business_id_encoder = LabelEncoder()\n",
    "business_geohash_encoder = LabelEncoder()\n",
    "\n",
    "user_scaler = StandardScaler()\n",
    "business_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_features_scaled = user_continuous_features_scaled.set_index(user_df['user_id_encoded'].values)\n",
    "business_continuous_features_scaled = business_continuous_features_scaled.set_index(business_df['business_id_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Encode 'city' as a discrete feature for businesses\n",
    "# business_city_encoder = LabelEncoder()\n",
    "# business_df['city_encoded'] = business_city_encoder.fit_transform(business_df['city'])\n",
    "\n",
    "# # Save number of unique cities for embedding input_dim\n",
    "# num_cities = business_df['city_encoded'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(input_dim, output_dim, name):\n",
    "    \"\"\"Reusable function to create an embedding layer.\"\"\"\n",
    "    return layers.Embedding(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        name=f\"{name}_embedding\",\n",
    "        # embeddings_regularizer=regularizers.l2(1e-4)  # L2 regularization\n",
    "    )\n",
    "\n",
    "# Create embedding layers\n",
    "user_id_embedding = create_embedding_layer(num_users, 16, \"user_id\")\n",
    "business_id_embedding = create_embedding_layer(num_businesses, 16, \"business_id\")\n",
    "category_embedding = create_embedding_layer(num_categories, 16, \"category\")\n",
    "business_geohash_embedding = create_embedding_layer(num_geohashes, 16, \"geohash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    user_id_input = layers.Input(shape=(1,), name=\"user_id\")\n",
    "    user_continuous_input = layers.Input(shape=(continuous_dim,), name=\"user_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    user_id_embedded = user_id_embedding(user_id_input)\n",
    "    user_id_embedded = layers.Flatten()(user_id_embedded)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([user_id_embedded, user_continuous_input])\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    user_embedding = layers.Dense(16, activation=None, name=\"user_embedding\")(x)\n",
    "\n",
    "    return Model([user_id_input, user_continuous_input], user_embedding, name=\"UserTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    business_id_input = layers.Input(shape=(1,), name=\"business_id\")\n",
    "    business_continuous_input = layers.Input(shape=(continuous_dim,), name=\"business_continuous\")\n",
    "    category_input = layers.Input(shape=(None,), dtype=\"int32\", name=\"category_indices\")  # Variable-length input\n",
    "    # geohash_input = layers.Input(shape=(1,), name=\"geohash\")\n",
    "\n",
    "    # Embedding\n",
    "    business_id_embedded = business_id_embedding(business_id_input)\n",
    "    business_id_embedded = layers.Flatten()(business_id_embedded)\n",
    "\n",
    "    # category_embeddings = category_embedding(category_input)\n",
    "    # aggregated_category_embedding = CategoryPoolingLayer(name=\"category_pooling\")(category_embeddings)\n",
    "\n",
    "    # geohash_embedding = business_geohash_embedding(geohash_input)\n",
    "    # geohash_embedding = layers.Flatten()(geohash_embedding)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([business_id_embedded,\n",
    "                                    # aggregated_category_embedding, \n",
    "                                    # geohash_embedding, \n",
    "                                    business_continuous_input])\n",
    "\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    business_embedding = layers.Dense(16, activation=None, name=\"business_embedding\")(x)\n",
    "\n",
    "    return Model([business_id_input, \n",
    "                #   category_input,\n",
    "                    # geohash_input,\n",
    "                    business_continuous_input],\n",
    "                    business_embedding, name=\"ItemTower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "def triplet_hinge_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_pred shape: (batch_size, 3, embedding_dim)\n",
    "        anchor, positive, negative = tf.unstack(y_pred, num=3, axis=1)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        # Hinge loss: max(0, pos_dist - neg_dist + margin)\n",
    "        return tf.reduce_mean(tf.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.2, random_state=42)\n",
    "# train_df = review_df\n",
    "\n",
    "# Generate triplets for training and testing\n",
    "train_triplets = generate_triplets(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544433 239900 411744\n"
     ]
    }
   ],
   "source": [
    "num_pos_train = sum(train_df['label'])\n",
    "num_neg_train = len(train_df) - num_pos_train\n",
    "print(num_pos_train, num_neg_train, len(train_triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs\n",
    "def prepare_triplet_inputs(triplets, user_features, business_features, business_geohash_map, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    # Replace NaN values with empty lists in `business_category_map`\n",
    "    business_category_map = business_category_map.apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    anchor_indices = triplets[:, 0]\n",
    "    positive_indices = triplets[:, 1]\n",
    "    negative_indices = triplets[:, 2]\n",
    "\n",
    "    anchor_features = [anchor_indices, user_features.take(anchor_indices, axis=0).values]\n",
    "    positive_features = [\n",
    "        positive_indices,\n",
    "        # pad_sequences(business_category_map.loc[positive_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        # business_geohash_map.take(positive_indices).values,\n",
    "        business_features.take(positive_indices, axis=0).values, \n",
    "\n",
    "]\n",
    "    negative_features = [\n",
    "        negative_indices, \n",
    "        # pad_sequences(business_category_map.loc[negative_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        # business_geohash_map.take(negative_indices).values,\n",
    "        business_features.take(negative_indices, axis=0).values,\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        anchor_features[0], anchor_features[1],\n",
    "        positive_features[0], positive_features[1], \n",
    "        # positive_features[2], \n",
    "        # positive_features[3],\n",
    "        negative_features[0], negative_features[1], \n",
    "        # negative_features[2], \n",
    "        # negative_features[3]\n",
    "    ]\n",
    "\n",
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "business_geohash_map = business_df.set_index('business_id_encoded')['geohash_encoded']\n",
    "\n",
    "max_category_length = MAX_CATEGORY_LENGTH\n",
    "\n",
    "train_inputs = prepare_triplet_inputs(train_triplets, user_continuous_features_scaled, business_continuous_features_scaled, business_geohash_map, business_category_map, max_category_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate towers\n",
    "user_model = user_tower(user_continuous_features_scaled.shape[1])\n",
    "item_model = item_tower(business_continuous_features_scaled.shape[1])\n",
    "\n",
    "# Define inputs for user and business towers\n",
    "user_inputs_model = [Input(shape=(1,), dtype=tf.int32, name=\"user_id_input\"),\n",
    "                     Input(shape=(user_continuous_features_scaled.shape[1],), name=\"user_cont_features_input\")]\n",
    "\n",
    "positive_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"positive_id_input\"),\n",
    "    # Input(shape=(max_category_length,), dtype=tf.int32, name=\"positive_category_input\"),\n",
    "    # Input(shape=(1,), dtype=tf.int32, name=\"positive_geohash_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"positive_cont_features_input\")\n",
    "]\n",
    "\n",
    "negative_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"negative_id_input\"),\n",
    "    # Input(shape=(max_category_length,), dtype=tf.int32, name=\"negative_category_input\"),\n",
    "    # Input(shape=(1,), dtype=tf.int32, name=\"negative_geohash_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"negative_cont_features_input\")\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "anchor_embedding = user_model(user_inputs_model)\n",
    "positive_embedding = item_model(positive_inputs_model)\n",
    "negative_embedding = item_model(negative_inputs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def stack_embeddings(embeddings):\n",
    "    # Unpack the embeddings from the list\n",
    "    anchor, positive, negative = embeddings\n",
    "    return tf.stack([anchor, positive, negative], axis=1)\n",
    "\n",
    "triplet_embeddings = Lambda(stack_embeddings, name=\"triplet_embeddings\")(\n",
    "    [anchor_embedding, positive_embedding, negative_embedding]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = user_inputs_model + positive_inputs_model + negative_inputs_model\n",
    "# Build the model\n",
    "triplet_model = Model(\n",
    "    inputs= all_inputs,\n",
    "    outputs=triplet_embeddings,\n",
    "    name=\"triplet_model\"\n",
    ")\n",
    "\n",
    "# Compile with triplet loss\n",
    "triplet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=triplet_hinge_loss(margin=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m10294/10294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 21ms/step - loss: 0.1628 - val_loss: 0.1632\n",
      "Epoch 2/3\n",
      "\u001b[1m10294/10294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 17ms/step - loss: 0.0710 - val_loss: 0.1672\n",
      "Epoch 3/3\n",
      "\u001b[1m10294/10294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 17ms/step - loss: 0.0411 - val_loss: 0.1743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13bc29a3a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the triplet model\n",
    "triplet_model.fit(\n",
    "    x=train_inputs,\n",
    "    y=np.zeros(len(train_inputs[0])),  # Dummy labels, as loss is computed from embeddings\n",
    "    batch_size=32,\n",
    "    # epochs=10,\n",
    "    # epochs=5,\n",
    "    epochs=3,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = 'Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "# Save the models\n",
    "user_model.save(save_folder_path + 'user_model.keras')\n",
    "item_model.save(save_folder_path + 'item_model.keras')\n",
    "\n",
    "# Save the label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(categories_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_geohash_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_geohash_encoder, f)\n",
    "    \n",
    "# Save the scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(business_scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
