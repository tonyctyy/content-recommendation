{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Triplet Hinge Loss\n",
    "This notebook is used to train the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to predict user and business interest based on the user and business features. The model is trained using the Triplet Hinge Loss as the loss function according the the paper from Facebook. The goal is to separate the positive and negative samples by a margin, so that the positive samples are closer to the anchor than the negative samples. \n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `user_model.keras` - The trained user model for retrieval. \n",
    "2. `user_id_encoder.pkl` - The label encoder for the user id.\n",
    "3. `user_scaler.pkl` - The scaler for the user features.\n",
    "4. `business_model.keras` - The trained business model for retrieval.\n",
    "5. `business_id_encoder.pkl` - The label encoder for the business id.\n",
    "6. `categories_encoder.pkl` - The label encoder for the business categories.\n",
    "7. `business_scaler.pkl` - The scaler for the business features.\n",
    "    \n",
    "#### Move to Production\n",
    "1. Before moving to production, the model should be indexed in the `DSSM Index (Faiss).ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_program import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_encoder = LabelEncoder()\n",
    "categories_encoder.fit(list(unique_categories))\n",
    "user_id_encoder = LabelEncoder()\n",
    "business_id_encoder = LabelEncoder()\n",
    "\n",
    "user_scaler = StandardScaler()\n",
    "business_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Encode 'city' as a discrete feature for businesses\n",
    "# business_city_encoder = LabelEncoder()\n",
    "# business_df['city_encoded'] = business_city_encoder.fit_transform(business_df['city'])\n",
    "\n",
    "# # Save number of unique cities for embedding input_dim\n",
    "# num_cities = business_df['city_encoded'].max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(input_dim, output_dim, name):\n",
    "    \"\"\"Reusable function to create an embedding layer.\"\"\"\n",
    "    return layers.Embedding(input_dim=input_dim, output_dim=output_dim, name=f\"{name}_embedding\")\n",
    "\n",
    "# Create embedding layers\n",
    "user_id_embedding = create_embedding_layer(num_users, 16, \"user_id\")\n",
    "business_id_embedding = create_embedding_layer(num_businesses, 16, \"business_id\")\n",
    "category_embedding = create_embedding_layer(num_categories, 16, \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example aggregation function for TensorFlow (Mean Pooling)\n",
    "def aggregate_category_embeddings(category_indices):\n",
    "    category_indices = tf.constant(category_indices, dtype=tf.int32)\n",
    "    embeddings = category_embedding(category_indices)\n",
    "    return tf.reduce_mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    user_id_input = layers.Input(shape=(1,), name=\"user_id\")\n",
    "    user_continuous_input = layers.Input(shape=(continuous_dim,), name=\"user_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    user_id_embedded = user_id_embedding(user_id_input)\n",
    "    user_id_embedded = layers.Flatten()(user_id_embedded)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([user_id_embedded, user_continuous_input])\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    user_embedding = layers.Dense(16, activation=None, name=\"user_embedding\")(x)\n",
    "\n",
    "    return Model([user_id_input, user_continuous_input], user_embedding, name=\"UserTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    business_id_input = layers.Input(shape=(1,), name=\"business_id\")\n",
    "    business_continuous_input = layers.Input(shape=(continuous_dim,), name=\"business_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    business_id_embedded = business_id_embedding(business_id_input)\n",
    "    business_id_embedded = layers.Flatten()(business_id_embedded)\n",
    "\n",
    "    category_input = layers.Input(shape=(None,), dtype=\"int32\", name=\"category_indices\")  # Variable-length input\n",
    "    category_embeddings = category_embedding(category_input)\n",
    "    # aggregated_category_embedding = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"category_pooling\", output_shape=(16,))(category_embeddings)\n",
    "    aggregated_category_embedding = CategoryPoolingLayer(name=\"category_pooling\")(category_embeddings)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([business_id_embedded, aggregated_category_embedding, business_continuous_input])\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    business_embedding = layers.Dense(16, activation=None, name=\"business_embedding\")(x)\n",
    "\n",
    "    return Model([business_id_input, category_input, business_continuous_input], business_embedding, name=\"ItemTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "def triplet_hinge_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_pred shape: (batch_size, 3, embedding_dim)\n",
    "        anchor, positive, negative = tf.unstack(y_pred, num=3, axis=1)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        # Hinge loss: max(0, pos_dist - neg_dist + margin)\n",
    "        return tf.reduce_mean(tf.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate triplets\n",
    "def generate_triplets(review_df, num_neg_samples=1):\n",
    "    triplets = []\n",
    "    grouped = review_df.groupby('user_id_encoded')\n",
    "\n",
    "    for user_id, group in grouped:\n",
    "        positive_samples = group[group['label'] == 1]\n",
    "        negative_samples = group[group['label'] == 0]\n",
    "        \n",
    "        if positive_samples.empty or negative_samples.empty:\n",
    "            continue  # Skip users without both positive and negative samples\n",
    "        \n",
    "        for _, pos_row in positive_samples.iterrows():\n",
    "            for _, neg_row in negative_samples.sample(num_neg_samples, replace=True).iterrows():\n",
    "                triplets.append((\n",
    "                    user_id,\n",
    "                    pos_row['business_id_encoded'],\n",
    "                    neg_row['business_id_encoded']\n",
    "                ))\n",
    "    \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate triplets for training and testing\n",
    "train_triplets = generate_triplets(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs\n",
    "def prepare_triplet_inputs(triplets, user_features, business_features, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    # Replace NaN values with empty lists in `business_category_map`\n",
    "    business_category_map = business_category_map.apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    anchor_indices = triplets[:, 0]\n",
    "    positive_indices = triplets[:, 1]\n",
    "    negative_indices = triplets[:, 2]\n",
    "\n",
    "    anchor_features = [anchor_indices, user_features.take(anchor_indices, axis=0).values]\n",
    "    positive_features = [\n",
    "        positive_indices,\n",
    "        pad_sequences(business_category_map.loc[positive_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(positive_indices, axis=0).values, \n",
    "\n",
    "]\n",
    "    negative_features = [\n",
    "        negative_indices, \n",
    "        pad_sequences(business_category_map.loc[negative_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(negative_indices, axis=0).values,\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        anchor_features[0], anchor_features[1],\n",
    "        positive_features[0], positive_features[1], positive_features[2],\n",
    "        negative_features[0], negative_features[1], negative_features[2]\n",
    "    ]\n",
    "\n",
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "max_category_length = MAX_CATEGORY_LENGTH\n",
    "\n",
    "train_inputs = prepare_triplet_inputs(train_triplets, user_continuous_features_scaled, business_continuous_features_scaled, business_category_map, max_category_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate towers\n",
    "user_model = user_tower(user_continuous_features_scaled.shape[1])\n",
    "item_model = item_tower(business_continuous_features_scaled.shape[1])\n",
    "\n",
    "# Define inputs for user and business towers\n",
    "user_inputs_model = [Input(shape=(1,), dtype=tf.int32, name=\"user_id_input\"),\n",
    "                     Input(shape=(user_continuous_features_scaled.shape[1],), name=\"user_cont_features_input\")]\n",
    "\n",
    "positive_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"positive_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"positive_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"positive_cont_features_input\")\n",
    "]\n",
    "\n",
    "negative_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"negative_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"negative_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"negative_cont_features_input\")\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "anchor_embedding = user_model(user_inputs_model)\n",
    "positive_embedding = item_model(positive_inputs_model)\n",
    "negative_embedding = item_model(negative_inputs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_embeddings(embeddings):\n",
    "    # Unpack the embeddings from the list\n",
    "    anchor, positive, negative = embeddings\n",
    "    return tf.stack([anchor, positive, negative], axis=1)\n",
    "\n",
    "triplet_embeddings = Lambda(stack_embeddings, name=\"triplet_embeddings\")(\n",
    "    [anchor_embedding, positive_embedding, negative_embedding]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = user_inputs_model + positive_inputs_model + negative_inputs_model\n",
    "# Build the model\n",
    "triplet_model = Model(\n",
    "    inputs= all_inputs,\n",
    "    outputs=triplet_embeddings,\n",
    "    name=\"triplet_model\"\n",
    ")\n",
    "\n",
    "# Compile with triplet loss\n",
    "triplet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=triplet_hinge_loss(margin=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 15ms/step - loss: 1.1598\n",
      "Epoch 2/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 14ms/step - loss: 0.5441\n",
      "Epoch 3/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 14ms/step - loss: 0.3176\n",
      "Epoch 4/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 15ms/step - loss: 0.2114\n",
      "Epoch 5/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 15ms/step - loss: 0.1520\n",
      "Epoch 6/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 15ms/step - loss: 0.1175\n",
      "Epoch 7/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 15ms/step - loss: 0.0925\n",
      "Epoch 8/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 15ms/step - loss: 0.0761\n",
      "Epoch 9/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 15ms/step - loss: 0.0636\n",
      "Epoch 10/10\n",
      "\u001b[1m12868/12868\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 15ms/step - loss: 0.0529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ab9e7dc950>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the triplet model\n",
    "triplet_model.fit(\n",
    "    x=train_inputs,\n",
    "    y=np.zeros(len(train_inputs[0])),  # Dummy labels, as loss is computed from embeddings\n",
    "    batch_size=32,\n",
    "    # epochs=10,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = 'Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "# Save the models\n",
    "user_model.save(save_folder_path + 'user_model.keras')\n",
    "item_model.save(save_folder_path + 'item_model.keras')\n",
    "\n",
    "# Save the label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(categories_encoder, f)\n",
    "    \n",
    "# Save the scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(business_scaler, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
