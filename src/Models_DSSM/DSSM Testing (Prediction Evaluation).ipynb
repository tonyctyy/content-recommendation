{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Prediction Evaluation\n",
    "This notebook is used to evaluate the prediction performance of the DSSM model. It is different from the real time recommendation. \n",
    "\n",
    "#### Pre-requisites\n",
    "- The model is trained and the index is created in the notebooks `DSSM Model.ipynb` and `DSSM Index (Faiss).ipynb`.\n",
    "- All required files are saved in the `Saved_Triplet_Hinge_Loss` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business_details table.\n",
      "Loaded 360656 rows from business_categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_folder_path='Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path=save_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage='test')\n",
    "\n",
    "# check if business_category_map\n",
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "# check if business_geohash_map\n",
    "business_geohash_map = business_df.set_index('business_id_encoded')['geohash_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_features_scaled = user_continuous_features_scaled.set_index(user_df['user_id_encoded'].values)\n",
    "business_continuous_features_scaled = business_continuous_features_scaled.set_index(business_df['business_id_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 136214\n",
      "Number of negative reviews: 59870\n",
      "Total number of reviews: 196084\n",
      "Ratio of positive to negative reviews: 2.28\n"
     ]
    }
   ],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_data, test_data = train_test_split(review_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test set into positive and negative samples\n",
    "positive_reviews = test_data[test_data['stars'] >= 4]\n",
    "negative_reviews = test_data[test_data['stars'] < 4]\n",
    "\n",
    "print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Total number of reviews: {len(test_data)}\")\n",
    "print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_test_data(positive_reviews, negative_reviews):\n",
    "    # down-sample the positive reviews to balance the dataset\n",
    "    positive_reviews_downsampled = positive_reviews.sample(n=len(negative_reviews), random_state=42)\n",
    "\n",
    "    # combine the down-sampled positive reviews with the negative reviews\n",
    "    balanced_test_data = pd.concat([positive_reviews_downsampled, negative_reviews], ignore_index=True)\n",
    "\n",
    "    # shuffle the balanced test data\n",
    "    balanced_test_data = balanced_test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # new statistics for the balanced test data\n",
    "    positive_reviews = balanced_test_data[balanced_test_data['stars'] >= 4]\n",
    "    negative_reviews = balanced_test_data[balanced_test_data['stars'] < 4]\n",
    "\n",
    "    print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "    print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "    print(f\"Total number of reviews: {len(balanced_test_data)}\")\n",
    "    print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n",
    "    return balanced_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 59870\n",
      "Number of negative reviews: 59870\n",
      "Total number of reviews: 119740\n",
      "Ratio of positive to negative reviews: 1.00\n"
     ]
    }
   ],
   "source": [
    "# balance the test data, comment this line to use the original test data\n",
    "test_data = balance_test_data(positive_reviews, negative_reviews)\n",
    "\n",
    "# group the test data by user_id and get the business_id\n",
    "test_data_grouped = test_data.groupby('user_id')['business_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_users = 1000\n",
    "# get until the number of target_users as test data\n",
    "test_data_grouped = test_data_grouped[:target_users]\n",
    "\n",
    "# update the test data with the grouped test data\n",
    "test_data = test_data[test_data['user_id'].isin(test_data_grouped['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare inputs for user and business embeddings\n",
    "# Extract user features\n",
    "test_user_ids = test_data['user_id_encoded'].values\n",
    "test_user_cont_features = user_scaler.transform(user_continuous_features_scaled.loc[test_user_ids].values)\n",
    "\n",
    "# Extract business features\n",
    "test_business_ids = test_data['business_id_encoded'].values\n",
    "test_business_cont_features = business_scaler.transform(business_continuous_features_scaled.loc[test_business_ids].values)\n",
    "test_business_categories = business_category_map.loc[test_business_ids].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "test_business_category_padded = pad_sequences(test_business_categories.tolist(), maxlen=5, padding=\"post\")\n",
    "\n",
    "test_business_geohashes = business_geohash_map.take(test_business_ids).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Predict embeddings using the loaded models\n",
    "test_user_embeddings = user_model.predict([test_user_ids, test_user_cont_features])\n",
    "\n",
    "test_business_embeddings = item_model.predict([test_business_ids, \n",
    "                                           test_business_category_padded, \n",
    "                                            # test_business_geohashes,\n",
    "                                            test_business_cont_features])\n",
    "\n",
    "# Step 4: Compute cosine similarity for each user-business pair\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Create a list of cosine similarities for each record in test_data\n",
    "test_data['predicted_similarity'] = [\n",
    "    compute_cosine_similarity(test_user_embeddings[i], test_business_embeddings[i])\n",
    "    for i in range(len(test_data))\n",
    "]\n",
    "\n",
    "# Step 5: Set the predicted_label based on similarity score\n",
    "test_data['predicted_label'] = (test_data['predicted_similarity'] >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Positive</th>\n",
       "      <th>Total Negative</th>\n",
       "      <th>Total</th>\n",
       "      <th>Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1045</td>\n",
       "      <td>1143</td>\n",
       "      <td>2188</td>\n",
       "      <td>0.477605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Positive  Total Negative  Total     Ratio\n",
       "0            1045            1143   2188  0.477605"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F-beta Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5672</td>\n",
       "      <td>0.5456</td>\n",
       "      <td>0.5608</td>\n",
       "      <td>0.5531</td>\n",
       "      <td>0.5577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision  Recall  F1 Score  F-beta Score\n",
       "0    0.5672     0.5456  0.5608    0.5531        0.5577"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Positive</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>586</td>\n",
       "      <td>655</td>\n",
       "      <td>488</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True Positive  True Negative  False Positive  False Negative\n",
       "0            586            655             488             459"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract ground truth labels and predicted scores\n",
    "y_true = test_data['label'].values\n",
    "y_pred_scores = test_data['predicted_similarity'].values  # Cosine similarity scores\n",
    "\n",
    "# Convert similarity scores into binary predictions (threshold = 0 for cosine similarity)\n",
    "y_pred_labels = (y_pred_scores >= 0).astype(int)\n",
    "\n",
    "# Compute confusion matrix elements\n",
    "true_positive = np.sum((y_true == 1) & (y_pred_labels == 1))\n",
    "true_negative = np.sum((y_true == 0) & (y_pred_labels == 0))\n",
    "false_positive = np.sum((y_true == 0) & (y_pred_labels == 1))\n",
    "false_negative = np.sum((y_true == 1) & (y_pred_labels == 0))\n",
    "\n",
    "# Compute dataset statistics\n",
    "total_positive = np.sum(y_true)\n",
    "total = len(y_true)\n",
    "total_negative = total - total_positive\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = (true_positive + true_negative) / total if total > 0 else 0\n",
    "precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "recall = true_positive / total_positive if total_positive > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# Weighted Fβ-score\n",
    "beta = 2\n",
    "f_beta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall) if (beta**2 * precision + recall) > 0 else 0\n",
    "\n",
    "# Compute dataset statistics\n",
    "background_stats = pd.DataFrame({\n",
    "    'Total Positive': [total_positive],\n",
    "    'Total Negative': [total_negative],\n",
    "    'Total': [total],\n",
    "    'Ratio': [total_positive / total if total > 0 else 0],\n",
    "})\n",
    "\n",
    "print(\"Testing Data Statistics\")\n",
    "display(background_stats)\n",
    "\n",
    "# Evaluation Metrics\n",
    "evaluation_metric = pd.DataFrame({\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1 Score': [f1_score],\n",
    "    'F-beta Score': [f_beta],\n",
    "    # 'Mean Reciprocal Rank': [mean_reciprocal_rank],\n",
    "}).apply(lambda x: round(x, 4))\n",
    "\n",
    "print(\"Evaluation Metrics\")\n",
    "display(evaluation_metric)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = pd.DataFrame({\n",
    "    'True Positive': [true_positive],\n",
    "    'True Negative': [true_negative],\n",
    "    'False Positive': [false_positive],\n",
    "    'False Negative': [false_negative]\n",
    "})\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
