{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Index Building\n",
    "This notebook is used to build the index for the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to retrieve similar businesses based on the business name and categories. The model is built using the `faiss` library which is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed `user_model.keras`, `scalers.pkl` and `encoder.pkl` in the `./Saved_Triplet_Hinge_Loss` folder.\n",
    "2. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "3. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `faiss_index.bin` - The index file that is used to retrieve similar businesses based on the business name and categories.\n",
    "2. `business_ids.npy` - The business ids that are used to retrieve the business details from the Yelp dataset.\n",
    "3. `user_continuous_features.pkl` - The user continuous features that are used to retrieve the user details from the Yelp dataset. (Temporary file)\n",
    "\n",
    "#### Move to Production\n",
    "1. Gather all the files in the `./Saved_Triplet_Hinge_Loss` folder and move them to the `../../data/processed_data/DSSM` folder. The files are:\n",
    "    - `user_model.keras`\n",
    "    - `user_scalers.pkl`\n",
    "    - `user_id_encoder.pkl`\n",
    "    - `user_continous_features.pkl` (Temporary file)\n",
    "    - `faiss_index.bin` (This replaces the business model)\n",
    "    - `business_ids.npy`\n",
    "    - `business_id_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_program import *\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_folder_path='Saved_BCE_Loss/'\n",
    "save_folder_path='Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path=save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  easy_positive_df['label'] = 1\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:140: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  positive_df = pd.concat([positive_df, easy_positive_df])\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:143: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  easy_negative_df['label'] = 0\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:145: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  negative_df = pd.concat([negative_df, easy_negative_df])\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hard_positive_df['label'] = 1\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hard_positive_df['difficulty'] = 1\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hard_negative_df['label'] = 0\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hard_negative_df['difficulty'] = 1\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, label_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "business_geohash_map = business_df.set_index('business_id_encoded')['geohash_encoded']\n",
    "# business_geohash_map = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_geohash_map, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "\n",
    "    business_categories = business_category_map.loc[business_ids].astype(object).tolist()\n",
    "    # business_category_padded = pad_sequences(business_categories, maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    business_geohashes = business_geohash_map.take(business_ids).values\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, \n",
    "                                            #   business_category_padded,\n",
    "                                                business_geohashes, \n",
    "                                              business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, business_geohash_map, business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_k(user_id, user_model, faiss_index, business_ids, k=100):\n",
    "    # Check if the user_id is in the user_id_encoder\n",
    "    if user_id not in user_id_encoder.classes_:\n",
    "        # raise ValueError(\"User ID is not in the encoder\")\n",
    "        user_id = \"default_user\"\n",
    "\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict the user's embedding\n",
    "    # user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features], verbose=0)\n",
    "\n",
    "    user_embedding = user_model.predict([user_id_encoded.reshape(1, -1), user_cont_features], verbose=0)\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "\n",
    "    # valid_indices = indices[indices != -1].flatten()\n",
    "    # top_k_business_ids = business_ids[valid_indices]\n",
    "\n",
    "    return top_k_business_ids, distances.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9HQLEChkam3GMBQn0SmvVw\n",
      "                business_id  similarity_score\n",
      "0    hExi86DTBlmIhB2FLvqrHg          0.945442\n",
      "1    fgtnOag-DaTsZTHPsgnWSQ          0.942739\n",
      "2    JxC9SCQF4aS2lHhRNkKjsQ          0.931843\n",
      "3    kpK6SmxIiNoGPNhlEGiL4w          0.922977\n",
      "4    I13HqIkWfR6CydGbo9xv0A          0.921429\n",
      "..                      ...               ...\n",
      "295  yAXiqUXVDdzWAXMFAobuCQ          0.840128\n",
      "296  WdS2zyg6lJNeTLMIvkmWAQ          0.840007\n",
      "297  R-uXbjABQHnbvigEiX4eVQ          0.839937\n",
      "298  hdh1tm7ZyCHc4S-2nST61A          0.839904\n",
      "299  hlHkDOaRT1gmqpUi2vqkzQ          0.839900\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "# Check if the user_id is in the encoder\n",
    "if user_id not in user_id_encoder.classes_:\n",
    "    user_id = \"default_user\"\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, business_ids, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(user_id)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder_path+\"faiss_index.bin\")\n",
    "faiss_index = None  # Free memory\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder_path+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder_path + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
