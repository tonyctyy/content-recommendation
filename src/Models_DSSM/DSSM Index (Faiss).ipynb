{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Index Building\n",
    "This notebook is used to build the index for the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to retrieve similar businesses based on the business name and categories. The model is built using the `faiss` library which is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed `user_model.keras`, `scalers.pkl` and `encoder.pkl` in the `./Saved_Triplet_Hinge_Loss` folder.\n",
    "2. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "3. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `faiss_index.bin` - The index file that is used to retrieve similar businesses based on the business name and categories.\n",
    "2. `business_ids.npy` - The business ids that are used to retrieve the business details from the Yelp dataset.\n",
    "3. `user_continuous_features.pkl` - The user continuous features that are used to retrieve the user details from the Yelp dataset. (Temporary file)\n",
    "\n",
    "#### Move to Production\n",
    "1. Gather all the files in the `./Saved_Triplet_Hinge_Loss` folder and move them to the `../../data/processed_data/DSSM` folder. The files are:\n",
    "    - `user_model.keras`\n",
    "    - `user_scalers.pkl`\n",
    "    - `user_id_encoder.pkl`\n",
    "    - `user_continous_features.pkl` (Temporary file)\n",
    "    - `faiss_index.bin` (This replaces the business model)\n",
    "    - `business_ids.npy`\n",
    "    - `business_id_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business_details table.\n",
      "Loaded 360656 rows from business_categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_folder_path='Saved_Triplet_Hinge_Loss/'\n",
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path=save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    # business_categories = business_category_map.loc[business_ids].apply(\n",
    "    #     lambda x: x if isinstance(x, list) else []\n",
    "    # )\n",
    "    # business_category_padded = pad_sequences(business_categories.tolist(), maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    business_categories = business_category_map.loc[business_ids].astype(object).tolist()\n",
    "    business_category_padded = pad_sequences(business_categories, maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    \n",
    "    # print(business_category_padded)\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, business_category_padded, business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "\n",
    "    # index = faiss.IndexHNSWFlat(business_embeddings_normalized.shape[1], 32)  # 32 neighbors\n",
    "    # index.hnsw.efConstruction = 200  # Controls recall/accuracy tradeoff\n",
    "    # index.add(business_embeddings_normalized)\n",
    "\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, \n",
    "    business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_k(user_id, user_model, faiss_index, business_ids, k=100):\n",
    "    # Check if the user_id is in the user_id_encoder\n",
    "    if user_id not in user_id_encoder.classes_:\n",
    "        # raise ValueError(\"User ID is not in the encoder\")\n",
    "        user_id = \"default_user\"\n",
    "\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict the user's embedding\n",
    "    # user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features], verbose=0)\n",
    "\n",
    "    user_embedding = user_model.predict([user_id_encoded.reshape(1, -1), user_cont_features], verbose=0)\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "\n",
    "    # valid_indices = indices[indices != -1].flatten()\n",
    "    # top_k_business_ids = business_ids[valid_indices]\n",
    "\n",
    "    return top_k_business_ids, distances.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9HQLEChkam3GMBQn0SmvVw\n",
      "                business_id  similarity_score\n",
      "0    G503RdNVGztzLuBfNY4l3A          0.919918\n",
      "1    cQlpkV5PLr6g_pwyb4M3Mw          0.919611\n",
      "2    ARKtuM_CFDs427PjEXKwQw          0.918689\n",
      "3    wAgw6Ufvgnnh6HoTzHvM1Q          0.915781\n",
      "4    2gVxo1YubGEhfwNhNd6DAQ          0.909420\n",
      "..                      ...               ...\n",
      "295  uMbeb4sQLjqm2rlx3Kss2g          0.807704\n",
      "296  p_w3VYR50sPbN-mAp7UqVg          0.807535\n",
      "297  DXn6UxlpT4R41hqpWbXcLA          0.807359\n",
      "298  _wZbh1bLXGxXQwzs1_5zsg          0.807282\n",
      "299  J1O2KJ57jnAZlsdaNQCyCQ          0.807072\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "# Check if the user_id is in the encoder\n",
    "if user_id not in user_id_encoder.classes_:\n",
    "    user_id = \"default_user\"\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, business_ids, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(user_id)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder_path+\"faiss_index.bin\")\n",
    "faiss_index = None  # Free memory\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder_path+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder_path + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
