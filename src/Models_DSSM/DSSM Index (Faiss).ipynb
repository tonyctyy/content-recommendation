{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Index Building\n",
    "This notebook is used to build the index for the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to retrieve similar businesses based on the business name and categories. The model is built using the `faiss` library which is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed `user_model.keras`, `scalers.pkl` and `encoder.pkl` in the `./Saved_Triplet_Hinge_Loss` folder.\n",
    "2. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "3. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `faiss_index.bin` - The index file that is used to retrieve similar businesses based on the business name and categories.\n",
    "2. `business_ids.npy` - The business ids that are used to retrieve the business details from the Yelp dataset.\n",
    "3. `user_continuous_features.pkl` - The user continuous features that are used to retrieve the user details from the Yelp dataset. (Temporary file)\n",
    "\n",
    "#### Move to Production\n",
    "1. Gather all the files in the `./Saved_Triplet_Hinge_Loss` folder and move them to the `../../data/processed_data/DSSM` folder. The files are:\n",
    "    - `user_model.keras`\n",
    "    - `user_scalers.pkl`\n",
    "    - `user_id_encoder.pkl`\n",
    "    - `user_continous_features.pkl` (Temporary file)\n",
    "    - `faiss_index.bin` (This replaces the business model)\n",
    "    - `business_ids.npy`\n",
    "    - `business_id_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_program import *\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_folder_path='Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path=save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: np.str_('Dance Wear')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    173\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(\u001b[43m[\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m, device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    173\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device\u001b[38;5;241m=\u001b[39mdevice(values))\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\_encode.py:167\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 167\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: np.str_('Dance Wear')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_id_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_geohash_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:135\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage, default_user_id, default_business_id)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_data\u001b[39m(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, default_user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_user\u001b[39m\u001b[38;5;124m'\u001b[39m, default_business_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_business\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;66;03m# user_df, business_df = assign_default_ids(user_df, business_df, default_user_id, default_business_id)\u001b[39;00m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;66;03m# user_df = assign_default_ids(user_df, business_df, default_user_id, default_business_id)\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m         all_user_ids \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([user_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], review_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m    136\u001b[0m         user_id_encoder\u001b[38;5;241m.\u001b[39mfit(all_user_ids\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    137\u001b[0m         all_business_ids \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([business_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], review_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\_encode.py:237\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: np.str_('Dance Wear')"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "business_geohash_map = business_df.set_index('business_id_encoded')['geohash_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_features_scaled = user_continuous_features_scaled.set_index(user_df['user_id_encoded'].values)\n",
    "business_continuous_features_scaled = business_continuous_features_scaled.set_index(business_df['business_id_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"ItemTower\" expects 3 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32,) dtype=int64>, <tf.Tensor 'data_1:0' shape=(32, 5) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index, business_embeddings_normalized\n\u001b[0;32m     22\u001b[0m business_ids \u001b[38;5;241m=\u001b[39m business_continuous_features_scaled\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 23\u001b[0m faiss_index, business_embeddings_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_faiss_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbusiness_continuous_features_scaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_geohash_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_category_map\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m, in \u001b[0;36mcreate_faiss_index\u001b[1;34m(item_model, business_ids, business_cont_features, business_geohash_map, business_category_map, max_category_length)\u001b[0m\n\u001b[0;32m      7\u001b[0m business_geohashes \u001b[38;5;241m=\u001b[39m business_geohash_map\u001b[38;5;241m.\u001b[39mtake(business_ids)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Predict embeddings\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m business_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mitem_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbusiness_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m#   business_category_padded,\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;66;43;03m# business_geohashes, \u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mbusiness_cont_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m business_embeddings_normalized \u001b[38;5;241m=\u001b[39m normalize(business_embeddings, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a Faiss index for cosine similarity (using inner product)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"ItemTower\" expects 3 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32,) dtype=int64>, <tf.Tensor 'data_1:0' shape=(32, 5) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_geohash_map, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "\n",
    "    business_categories = business_category_map.loc[business_ids].astype(object).tolist()\n",
    "    business_category_padded = pad_sequences(business_categories, maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    business_geohashes = business_geohash_map.take(business_ids).values\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, \n",
    "                                            #   business_category_padded,\n",
    "                                                # business_geohashes, \n",
    "                                              business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, business_geohash_map, business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_k(user_id, user_model, faiss_index, business_ids, k=100):\n",
    "    # Check if the user_id is in the user_id_encoder\n",
    "    if user_id not in user_id_encoder.classes_:\n",
    "        # raise ValueError(\"User ID is not in the encoder\")\n",
    "        user_id = \"default_user\"\n",
    "\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict the user's embedding\n",
    "    # user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features], verbose=0)\n",
    "\n",
    "    user_embedding = user_model.predict([user_id_encoded.reshape(1, -1), user_cont_features], verbose=0)\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "\n",
    "    # valid_indices = indices[indices != -1].flatten()\n",
    "    # top_k_business_ids = business_ids[valid_indices]\n",
    "\n",
    "    return top_k_business_ids, distances.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9HQLEChkam3GMBQn0SmvVw\n",
      "                business_id  similarity_score\n",
      "0    N18rU96Qr_XQtSqfQynZMg          0.766232\n",
      "1    lJjS6vLnf0Rqtx_gicq24g          0.686055\n",
      "2    MhPtYT3FDpTViIcIpMepMg          0.682905\n",
      "3    unjm4rIGc-ZzRxdzEPnc1g          0.681057\n",
      "4    pEw7LP0CItZ5TPA9PkBzwA          0.680482\n",
      "..                      ...               ...\n",
      "295  S-uXuwLZjgWIv4M8tGtpIw          0.451607\n",
      "296  dbn2SoD5uPG15Yd4UcmEzA          0.451209\n",
      "297  w45o3wnYLCUDp-fPRuSh6g          0.451064\n",
      "298  MuQ-irulfi6V8N9Ka6XBHQ          0.450886\n",
      "299  GjK8fBJO3sE9vYkK0RE7Jw          0.450738\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "# Check if the user_id is in the encoder\n",
    "if user_id not in user_id_encoder.classes_:\n",
    "    user_id = \"default_user\"\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, business_ids, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(user_id)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder_path+\"faiss_index.bin\")\n",
    "faiss_index = None  # Free memory\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder_path+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder_path + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
