{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business_details table.\n",
      "Loaded 360656 rows from business_categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path='Saved_Triplet_Hinge_Loss (ver0.1)/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['user_id_encoded'] = user_id_encoder.transform(review_df['user_id'])\n",
      "c:\\Code\\FYP\\content-recommendation\\src\\Models_DSSM\\general_program.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['business_id_encoded'] = business_id_encoder.transform(review_df['business_id'])\n"
     ]
    }
   ],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    business_categories = business_category_map.loc[business_ids].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    business_category_padded = pad_sequences(business_categories.tolist(), maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, business_category_padded, business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, \n",
    "    business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Query top-k businesses for a given user\n",
    "def query_top_k(user_id, user_model, faiss_index, k=5):\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict and normalize the user's embedding\n",
    "    user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features])\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "    return top_k_business_ids, distances.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "                business_id  similarity_score\n",
      "0    -qtgI1xDDSqxtTtPn3ERHw          0.864202\n",
      "1    CeQtgiR1EuGedqwh1uyLQQ          0.861182\n",
      "2    atZ_olNKXOG4rEr6mccN8g          0.858346\n",
      "3    kfW3-LmZlKrXq3RndVXxdg          0.848915\n",
      "4    E8NgBaDyaVPWxmyDvHSP0g          0.848732\n",
      "..                      ...               ...\n",
      "295  1_ZVtdiZpBNsXaO4ObPtbw          0.709518\n",
      "296  5mDneH5wP5VN_7GVe-PudQ          0.709467\n",
      "297  UMHuKs1sO-wq3XqKaejXeA          0.709385\n",
      "298  CBWmYHLgtFrOJs7SCcQn0g          0.709275\n",
      "299  JX8KhTInMNfQVs4Fn8mSSQ          0.709046\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"production/\"\n",
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder+\"faiss_index.bin\")\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
