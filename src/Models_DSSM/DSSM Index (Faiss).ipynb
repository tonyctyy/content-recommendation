{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Index Building\n",
    "This notebook is used to build the index for the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to retrieve similar businesses based on the business name and categories. The model is built using the `faiss` library which is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed `user_model.keras`, `scalers.pkl` and `encoder.pkl` in the `./Saved_Triplet_Hinge_Loss` folder.\n",
    "2. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "3. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `faiss_index.bin` - The index file that is used to retrieve similar businesses based on the business name and categories.\n",
    "2. `business_ids.npy` - The business ids that are used to retrieve the business details from the Yelp dataset.\n",
    "3. `user_continuous_features.pkl` - The user continuous features that are used to retrieve the user details from the Yelp dataset. (Temporary file)\n",
    "\n",
    "#### Move to Production\n",
    "1. Gather all the files in the `./Saved_Triplet_Hinge_Loss` folder and move them to the `../../data/processed_data/DSSM` folder. The files are:\n",
    "    - `user_model.keras`\n",
    "    - `user_scalers.pkl`\n",
    "    - `user_id_encoder.pkl`\n",
    "    - `user_continous_features.pkl` (Temporary file)\n",
    "    - `faiss_index.bin` (This replaces the business model)\n",
    "    - `business_ids.npy`\n",
    "    - `business_id_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_program import *\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path='Saved_Triplet_Hinge_Loss/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "    business_categories = business_category_map.loc[business_ids].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    business_category_padded = pad_sequences(business_categories.tolist(), maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, business_category_padded, business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, \n",
    "    business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Query top-k businesses for a given user\n",
    "def query_top_k(user_id, user_model, faiss_index, business_ids, k=5):\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict and normalize the user's embedding\n",
    "    user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features], verbose=0)\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "    return top_k_business_ids, distances.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                business_id  similarity_score\n",
      "0    sQhh7JCGpqNgf0hHWc4m8g          0.762543\n",
      "1    TV58NdbRgHq2IMxyzHWDbQ          0.732087\n",
      "2    lwJllJ5e4CLHdliOPEfgGg          0.729557\n",
      "3    vUrTGX_7HxqeoQ_6QCVz6g          0.728677\n",
      "4    mBgfK8HLthPOMPkEbYLW-A          0.725245\n",
      "..                      ...               ...\n",
      "295  O1XwKgUYNI_YBXnOqgaTaw          0.598820\n",
      "296  ETAxDtQbcCmy5ibQ5Y6Glg          0.598788\n",
      "297  9A5Gw0At6so0x-vWM0_JZw          0.598568\n",
      "298  hWuLvI5QqPyQ1x9ww0HeRw          0.597822\n",
      "299  _6BDxk8486ZYiRwpPmQewg          0.597797\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, business_ids, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"Saved_Triplet_Hinge_Loss/\"\n",
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder+\"faiss_index.bin\")\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
