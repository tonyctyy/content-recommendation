{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Structured Semantic Model - Index Building\n",
    "This notebook is used to build the index for the Deep Structured Semantic Model (DSSM) using the Yelp dataset. The DSSM model is used to retrieve similar businesses based on the business name and categories. The model is built using the `faiss` library which is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed `user_model.keras`, `scalers.pkl` and `encoder.pkl` in the `./Saved_Triplet_Hinge_Loss` folder.\n",
    "2. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "3. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Output\n",
    "1. `faiss_index.bin` - The index file that is used to retrieve similar businesses based on the business name and categories.\n",
    "2. `business_ids.npy` - The business ids that are used to retrieve the business details from the Yelp dataset.\n",
    "3. `user_continuous_features.pkl` - The user continuous features that are used to retrieve the user details from the Yelp dataset. (Temporary file)\n",
    "\n",
    "#### Move to Production\n",
    "1. Gather all the files in the `./Saved_Triplet_Hinge_Loss` folder and move them to the `../../data/processed_data/DSSM` folder. The files are:\n",
    "    - `user_model.keras`\n",
    "    - `user_scalers.pkl`\n",
    "    - `user_id_encoder.pkl`\n",
    "    - `user_continous_features.pkl` (Temporary file)\n",
    "    - `faiss_index.bin` (This replaces the business model)\n",
    "    - `business_ids.npy`\n",
    "    - `business_id_encoder.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business_details table.\n",
      "Loaded 360656 rows from business_categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "from general_program import *\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path='Saved_Triplet_Hinge_Loss/'\n",
    "\n",
    "user_model, item_model, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler = load_saved_models(save_folder_path=save_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df, business_df, review_df, user_continuous_features_scaled, business_continuous_features_scaled, num_users, num_businesses, num_categories, num_geohashes = prepare_data(user_df, business_df, review_df, categories_df, user_id_encoder, business_id_encoder, categories_encoder, business_geohash_encoder, user_scaler, business_scaler, use_stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "business_geohash_map = business_df.set_index('business_id_encoded')['geohash_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_continuous_features_scaled = user_continuous_features_scaled.set_index(user_df['user_id_encoded'].values)\n",
    "business_continuous_features_scaled = business_continuous_features_scaled.set_index(business_df['business_id_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_geohash_map, business_category_map, max_category_length=MAX_CATEGORY_LENGTH):\n",
    "\n",
    "    business_categories = business_category_map.loc[business_ids].astype(object).tolist()\n",
    "    business_category_padded = pad_sequences(business_categories, maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    business_geohashes = business_geohash_map.take(business_ids).values\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, \n",
    "                                            #   business_category_padded,\n",
    "                                                # business_geohashes, \n",
    "                                              business_cont_features])\n",
    "\n",
    "    business_embeddings_normalized = normalize(business_embeddings, axis=1)\n",
    "    # Create a Faiss index for cosine similarity (using inner product)\n",
    "    index = faiss.IndexFlatIP(business_embeddings_normalized.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings_normalized)\n",
    "\n",
    "    return index, business_embeddings_normalized\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings_normalized = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, business_geohash_map, business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_k(user_id, user_model, faiss_index, business_ids, k=100):\n",
    "    # Check if the user_id is in the user_id_encoder\n",
    "    if user_id not in user_id_encoder.classes_:\n",
    "        # raise ValueError(\"User ID is not in the encoder\")\n",
    "        user_id = \"default_user\"\n",
    "\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict the user's embedding\n",
    "    # user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features], verbose=0)\n",
    "\n",
    "    user_embedding = user_model.predict([user_id_encoded.reshape(1, -1), user_cont_features], verbose=0)\n",
    "    user_embedding_normalized = normalize(user_embedding, axis=1)\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding_normalized, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "\n",
    "    # valid_indices = indices[indices != -1].flatten()\n",
    "    # top_k_business_ids = business_ids[valid_indices]\n",
    "\n",
    "    return top_k_business_ids, distances.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9HQLEChkam3GMBQn0SmvVw\n",
      "                business_id  similarity_score\n",
      "0    DzzVSYXadZ1_XgfGz_Loyw          0.615018\n",
      "1    i2ofWDYFEd9-ZzpNcyZtGQ          0.613131\n",
      "2    DytKODMqcvQ7MWA0NN2uNw          0.605784\n",
      "3    93K-xlLwLnwcFuV2r2MI6Q          0.595534\n",
      "4    wUnLSg_GKfEIQ5CQQ770_g          0.594550\n",
      "..                      ...               ...\n",
      "295  nnj8APV33vrflphc6f50nA          0.487010\n",
      "296  3fWk-oy9u_o70pRZdOYZXA          0.486877\n",
      "297  jRLskcm_icZIKs81mYC4iQ          0.486839\n",
      "298  G3qHuySzimTTf1_6uBxF2g          0.486438\n",
      "299  Jbm9hgnWZGPtWEf-_iNilg          0.486419\n",
      "\n",
      "[300 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "# Check if the user_id is in the encoder\n",
    "if user_id not in user_id_encoder.classes_:\n",
    "    user_id = \"default_user\"\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, business_ids, k=300)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(user_id)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Faiss index to a file\n",
    "faiss.write_index(faiss_index, save_folder_path+\"faiss_index.bin\")\n",
    "faiss_index = None  # Free memory\n",
    "\n",
    "# Save business IDs\n",
    "np.save(save_folder_path+\"business_ids.npy\", business_ids)\n",
    "\n",
    "# Save user continuous features (temporal solution)\n",
    "with open(save_folder_path + \"user_continuous_features_scaled.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_continuous_features_scaled, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
