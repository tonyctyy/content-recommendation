{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_dataset(file_lists, prefix_path, chunk_size=10000):\\n    df_dict = {}\\n    prefix_path += \"sampled_\"\\n    for file in file_lists:\\n        try:\\n            df_chunks = []\\n            total_records = 0\\n\\n            for chunk in pd.read_json(prefix_path + file, lines=True, chunksize=chunk_size):\\n                df_chunks.append(chunk)\\n                total_records += chunk.shape[0]\\n\\n            df = pd.concat(df_chunks, ignore_index=True)\\n            df_dict[file] = df\\n            print(f\"Total records in {file}: {df.shape[0]}.\")\\n\\n        except Exception as e:\\n            print(f\"Error: {e}\")\\n            continue\\n    return df_dict\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def load_dataset(file_lists, prefix_path, chunk_size=10000):\n",
    "    df_dict = {}\n",
    "    prefix_path += \"sampled_\"\n",
    "    for file in file_lists:\n",
    "        try:\n",
    "            df_chunks = []\n",
    "            total_records = 0\n",
    "\n",
    "            for chunk in pd.read_json(prefix_path + file, lines=True, chunksize=chunk_size):\n",
    "                df_chunks.append(chunk)\n",
    "                total_records += chunk.shape[0]\n",
    "\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df_dict[file] = df\n",
    "            print(f\"Total records in {file}: {df.shape[0]}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    return df_dict\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfolder_path = \\'../data/\\'\\ntransit_bucket = \\'raw_datasets/\\'\\ntarget_bucket = \\'yelp/\\'\\nprefix_path = folder_path + transit_bucket + target_bucket\\nfile_list = [\\n    \"yelp_academic_dataset_business.json\",\\n    \"yelp_academic_dataset_review.json\",\\n]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "folder_path = '../data/'\n",
    "transit_bucket = 'raw_datasets/'\n",
    "target_bucket = 'yelp/'\n",
    "prefix_path = folder_path + transit_bucket + target_bucket\n",
    "file_list = [\n",
    "    \"yelp_academic_dataset_business.json\",\n",
    "    \"yelp_academic_dataset_review.json\",\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sparse cosine similarity with top N items\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "\n",
    "    # # Compute the top N cosine similarities in a sparse format\n",
    "    \n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the user_business DataFrame to avoid issues with slicing\n",
    "user_business = train_data.copy()\n",
    "\n",
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix (csr_matrix)\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace any NaN values with 0 in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Compute sparse cosine similarity matrix with top 10 most similar items\n",
    "item_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for user-item and item-item indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS item_item_similarity (\n",
    "    item_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_item_similarity ON item_item_similarity(item_id)''')\n",
    "\n",
    "# cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "#     user_id TEXT PRIMARY KEY,\n",
    "#     user_idx INTEGER\n",
    "# )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY,\n",
    "    business_idx INTEGER\n",
    ")''')\n",
    "\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_user_item(user_business, conn, batch_size=10000):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    # Insert user-item interactions in batches\n",
    "    total_records = len(user_business)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = user_business.iloc[i:i + batch_size]\n",
    "\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''',\n",
    "                           batch[['user_id', 'business_id', 'stars_review']].values.tolist())\n",
    "\n",
    "        # Show progress\n",
    "        print(f\"{i + len(batch)} / {total_records} records stored in user_item_index\")\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_item_vectors(item_similarity_sparse, business_mapping, conn, batch_size=1000, progress_interval=100000):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "\n",
    "    # Iterate over each row (item) in the sparse matrix\n",
    "    for row_idx in range(item_similarity_sparse.shape[0]):\n",
    "        # Get the row as a sparse vector (csr_matrix row)\n",
    "        row_vector = item_similarity_sparse.getrow(row_idx)\n",
    "\n",
    "        # Extract indices and data from the sparse vector\n",
    "        row_indices = row_vector.indices\n",
    "        row_data = row_vector.data\n",
    "\n",
    "        # Serialize only indices and data (not the full matrix)\n",
    "        serialized_row = pickle.dumps((row_indices, row_data))\n",
    "\n",
    "        # Get the item id (business_id)\n",
    "        item_id = list(business_mapping.keys())[row_idx]\n",
    "\n",
    "        # Add the item and its vector to the batch\n",
    "        batch.append((item_id, serialized_row))\n",
    "\n",
    "        # Insert in batches to reduce the number of commits\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "\n",
    "            # Print progress every progress_interval records\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} item vectors so far...\")\n",
    "\n",
    "            batch = []  # Clear the batch after committing\n",
    "\n",
    "    total_inserted += len(batch)  # Add any remaining records\n",
    "    # Insert any remaining records after the loop\n",
    "    if batch:\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Final progress message\n",
    "    print(f\"Total {total_inserted} item vectors inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mappings(business_mapping, conn):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    # Insert user mappings\n",
    "    # cursor.executemany('''INSERT OR REPLACE INTO user_mapping (user_id, user_idx) VALUES (?, ?)''',\n",
    "    #                    [(user_id, idx) for user_id, idx in user_mapping.items()])\n",
    "\n",
    "    # Insert business mappings\n",
    "    cursor.executemany('''INSERT OR REPLACE INTO business_mapping (business_id, business_idx) VALUES (?, ?)''',\n",
    "                       [(business_id, idx) for business_id, idx in business_mapping.items()])\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Inserted {len(business_mapping)} business mappings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 / 788585 records stored in user_item_index\n",
      "20000 / 788585 records stored in user_item_index\n",
      "30000 / 788585 records stored in user_item_index\n",
      "40000 / 788585 records stored in user_item_index\n",
      "50000 / 788585 records stored in user_item_index\n",
      "60000 / 788585 records stored in user_item_index\n",
      "70000 / 788585 records stored in user_item_index\n",
      "80000 / 788585 records stored in user_item_index\n",
      "90000 / 788585 records stored in user_item_index\n",
      "100000 / 788585 records stored in user_item_index\n",
      "110000 / 788585 records stored in user_item_index\n",
      "120000 / 788585 records stored in user_item_index\n",
      "130000 / 788585 records stored in user_item_index\n",
      "140000 / 788585 records stored in user_item_index\n",
      "150000 / 788585 records stored in user_item_index\n",
      "160000 / 788585 records stored in user_item_index\n",
      "170000 / 788585 records stored in user_item_index\n",
      "180000 / 788585 records stored in user_item_index\n",
      "190000 / 788585 records stored in user_item_index\n",
      "200000 / 788585 records stored in user_item_index\n",
      "210000 / 788585 records stored in user_item_index\n",
      "220000 / 788585 records stored in user_item_index\n",
      "230000 / 788585 records stored in user_item_index\n",
      "240000 / 788585 records stored in user_item_index\n",
      "250000 / 788585 records stored in user_item_index\n",
      "260000 / 788585 records stored in user_item_index\n",
      "270000 / 788585 records stored in user_item_index\n",
      "280000 / 788585 records stored in user_item_index\n",
      "290000 / 788585 records stored in user_item_index\n",
      "300000 / 788585 records stored in user_item_index\n",
      "310000 / 788585 records stored in user_item_index\n",
      "320000 / 788585 records stored in user_item_index\n",
      "330000 / 788585 records stored in user_item_index\n",
      "340000 / 788585 records stored in user_item_index\n",
      "350000 / 788585 records stored in user_item_index\n",
      "360000 / 788585 records stored in user_item_index\n",
      "370000 / 788585 records stored in user_item_index\n",
      "380000 / 788585 records stored in user_item_index\n",
      "390000 / 788585 records stored in user_item_index\n",
      "400000 / 788585 records stored in user_item_index\n",
      "410000 / 788585 records stored in user_item_index\n",
      "420000 / 788585 records stored in user_item_index\n",
      "430000 / 788585 records stored in user_item_index\n",
      "440000 / 788585 records stored in user_item_index\n",
      "450000 / 788585 records stored in user_item_index\n",
      "460000 / 788585 records stored in user_item_index\n",
      "470000 / 788585 records stored in user_item_index\n",
      "480000 / 788585 records stored in user_item_index\n",
      "490000 / 788585 records stored in user_item_index\n",
      "500000 / 788585 records stored in user_item_index\n",
      "510000 / 788585 records stored in user_item_index\n",
      "520000 / 788585 records stored in user_item_index\n",
      "530000 / 788585 records stored in user_item_index\n",
      "540000 / 788585 records stored in user_item_index\n",
      "550000 / 788585 records stored in user_item_index\n",
      "560000 / 788585 records stored in user_item_index\n",
      "570000 / 788585 records stored in user_item_index\n",
      "580000 / 788585 records stored in user_item_index\n",
      "590000 / 788585 records stored in user_item_index\n",
      "600000 / 788585 records stored in user_item_index\n",
      "610000 / 788585 records stored in user_item_index\n",
      "620000 / 788585 records stored in user_item_index\n",
      "630000 / 788585 records stored in user_item_index\n",
      "640000 / 788585 records stored in user_item_index\n",
      "650000 / 788585 records stored in user_item_index\n",
      "660000 / 788585 records stored in user_item_index\n",
      "670000 / 788585 records stored in user_item_index\n",
      "680000 / 788585 records stored in user_item_index\n",
      "690000 / 788585 records stored in user_item_index\n",
      "700000 / 788585 records stored in user_item_index\n",
      "710000 / 788585 records stored in user_item_index\n",
      "720000 / 788585 records stored in user_item_index\n",
      "730000 / 788585 records stored in user_item_index\n",
      "740000 / 788585 records stored in user_item_index\n",
      "750000 / 788585 records stored in user_item_index\n",
      "760000 / 788585 records stored in user_item_index\n",
      "770000 / 788585 records stored in user_item_index\n",
      "780000 / 788585 records stored in user_item_index\n",
      "788585 / 788585 records stored in user_item_index\n",
      "Total 74698 item vectors inserted.\n",
      "Inserted 74698 business mappings.\n"
     ]
    }
   ],
   "source": [
    "# Insert user-item index with progress\n",
    "insert_user_item(user_business, conn)\n",
    "\n",
    "# Insert item vectors into the database\n",
    "insert_item_vectors(item_similarity_sparse, business_mapping, conn)\n",
    "\n",
    "# Insert user and business mappings\n",
    "insert_mappings(business_mapping, conn)\n",
    "\n",
    "# Close the connection when done\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get businesses a user interacted with\n",
    "def get_user_businesses(user_id, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT business_id, stars_review FROM user_item_index WHERE user_id = ?''', (user_id,))\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-k similar businesses for a given business\n",
    "def get_top_k_similar_businesses(business_id, k, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT similarity_vector FROM item_item_similarity WHERE item_id = ?''', (business_id,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result is None:\n",
    "        return []\n",
    "\n",
    "    similarity_vector = pickle.loads(result[0])\n",
    "    indices, data = similarity_vector\n",
    "\n",
    "    # Get top-k similar businesses\n",
    "    top_k = sorted(zip(indices, data), key=lambda x: -x[1])[:k]\n",
    "\n",
    "    # Map indices to business ids\n",
    "    similar_businesses = [(list(business_mapping.keys())[idx], score) for idx, score in top_k]\n",
    "\n",
    "    return similar_businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict user interests based on similar businesses\n",
    "def predict_user_interests(user_id, k=10, conn=conn):\n",
    "    user_businesses = get_user_businesses(user_id, conn)\n",
    "\n",
    "    recommended_businesses = {}\n",
    "    for business_id, _ in user_businesses:\n",
    "        similar_businesses = get_top_k_similar_businesses(business_id, k, conn)\n",
    "\n",
    "        for similar_business_id, score in similar_businesses:\n",
    "            if similar_business_id in recommended_businesses:\n",
    "                recommended_businesses[similar_business_id] += score\n",
    "            else:\n",
    "                recommended_businesses[similar_business_id] = score\n",
    "\n",
    "    # Sort recommendations by score\n",
    "    recommended_businesses = sorted(recommended_businesses.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return recommended_businesses[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 136473\n",
      "Number of negative reviews: 32929\n",
      "Total number of reviews: 197147\n",
      "Ratio of positive to negative reviews: 4.14\n",
      "Number of positive reviews: 32929\n",
      "Number of negative reviews: 32929\n",
      "Total number of reviews: 65858\n",
      "Ratio of positive to negative reviews: 1.00\n"
     ]
    }
   ],
   "source": [
    "# get the number of positive and negative reviews in the test data\n",
    "positive_reviews = test_data[test_data['stars_review'] >= 4]\n",
    "negative_reviews = test_data[test_data['stars_review'] <= 2]\n",
    "\n",
    "print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Total number of reviews: {len(test_data)}\")\n",
    "print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n",
    "\n",
    "# down-sample the positive reviews to balance the dataset\n",
    "positive_reviews_downsampled = positive_reviews.sample(n=len(negative_reviews), random_state=42)\n",
    "\n",
    "# combine the down-sampled positive reviews with the negative reviews\n",
    "balanced_test_data = pd.concat([positive_reviews_downsampled, negative_reviews], ignore_index=True)\n",
    "\n",
    "# shuffle the balanced test data\n",
    "balanced_test_data = balanced_test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# new statistics for the balanced test data\n",
    "positive_reviews = balanced_test_data[balanced_test_data['stars_review'] >= 4]\n",
    "negative_reviews = balanced_test_data[balanced_test_data['stars_review'] <= 2]\n",
    "\n",
    "print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Total number of reviews: {len(balanced_test_data)}\")\n",
    "print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# group the test data by user_id and get the business_id\n",
    "test_data_grouped = balanced_test_data.groupby('user_id')['business_id'].apply(list).reset_index()\n",
    "\n",
    "# get the recommendations for each user in the test data\n",
    "recommendations = {}\n",
    "\n",
    "i = 0\n",
    "for user_id in test_data_grouped['user_id']:\n",
    "    recommendation = predict_user_interests(user_id, k=300, conn=conn)\n",
    "    business_ids, scores = [], []\n",
    "    for business_id, score in recommendation:\n",
    "        business_ids.append(business_id)\n",
    "        scores.append(score)\n",
    "    recommendations[user_id] = (business_ids, scores) \n",
    "    i += 1\n",
    "    if i == 1000:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_recommendations(recommendations, test_data_grouped, pos=4):\n",
    "    total = 0\n",
    "    total_positive = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    ranks = []\n",
    "    for i, row in test_data_grouped.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        business_ids = row['business_id']\n",
    "        rank = 0\n",
    "        if user_id in recommendations:\n",
    "            recommended_businesses = recommendations[user_id][0]\n",
    "            for business_id in business_ids:\n",
    "                star_rating = test_data[(test_data['user_id'] == user_id) & (test_data['business_id'] == business_id)]['stars_review'].values[0]\n",
    "                if star_rating >= pos:\n",
    "                    total_positive += 1\n",
    "                if business_id in recommended_businesses:\n",
    "                    if star_rating >= pos:\n",
    "                        true_positive += 1\n",
    "                    else:\n",
    "                        false_positive += 1\n",
    "                    # get the rank of the business_id in the recommendations\n",
    "                    rank = recommended_businesses.index(business_id) + 1\n",
    "                else:\n",
    "                    if star_rating < pos:\n",
    "                        true_negative += 1\n",
    "                    else:\n",
    "                        false_negative += 1\n",
    "            total += len(business_ids)\n",
    "        ranks.append(rank)\n",
    "    return true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks = check_recommendations(recommendations, test_data_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Data Statistics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Positive</th>\n",
       "      <th>Total Negative</th>\n",
       "      <th>Total</th>\n",
       "      <th>Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>833</td>\n",
       "      <td>839</td>\n",
       "      <td>1672</td>\n",
       "      <td>0.498206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Positive  Total Negative  Total     Ratio\n",
       "0             833             839   1672  0.498206"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>F-beta Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557416</td>\n",
       "      <td>0.633238</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.373942</td>\n",
       "      <td>0.323063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  F-beta Score\n",
       "0  0.557416   0.633238  0.265306  0.373942      0.323063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True Positive</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221</td>\n",
       "      <td>711</td>\n",
       "      <td>128</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   True Positive  True Negative  False Positive  False Negative\n",
       "0            221            711             128             612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the evaluation metrics\n",
    "accuracy = (true_positive + true_negative) / total\n",
    "precision = true_positive / (true_positive + false_positive) \n",
    "recall = true_positive / total_positive\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "# mean_reciprocal_rank = np.mean([1 / rank for rank in ranks])\n",
    "beta = 1.5\n",
    "f_beta = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)\n",
    "\n",
    "\n",
    "total_negative = total - total_positive\n",
    "background_stats = pd.DataFrame({\n",
    "    'Total Positive': [total_positive],\n",
    "    'Total Negative': [total_negative],\n",
    "    'Total': [total],\n",
    "    'Ratio': [total_positive / total],\n",
    "})\n",
    "\n",
    "print(\"Testing Data Statistics\")\n",
    "display(background_stats)\n",
    "\n",
    "\n",
    "evaluation_metric = pd.DataFrame({\n",
    "    'Accuracy': [accuracy],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1 Score': [f1_score],\n",
    "    # 'Mean Reciprocal Rank': [mean_reciprocal_rank],\n",
    "    'F-beta Score': [f_beta]\n",
    "})\n",
    "\n",
    "print(\"Evaluation Metrics\")\n",
    "display(evaluation_metric)\n",
    "\n",
    "\n",
    "confusion_matrix = pd.DataFrame({\n",
    "    'True Positive': [true_positive],\n",
    "    'True Negative': [true_negative],\n",
    "    'False Positive': [false_positive],\n",
    "    'False Negative': [false_negative]\n",
    "})\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
