{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-base Collaborative Filtering - Prediction Evaluation\n",
    "This notebook is used to evaluate the prediction performance of the item-based collaborative filtering model. It is different from the real time recommendation. \n",
    "\n",
    "#### Pre-requisites\n",
    "- The model is trained and the index is created in the notebook `ItemCF Model & Index.ipynb`.\n",
    "- The index is saved in the file `yelp_ItemCF.db` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")\n",
    "\n",
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]\n",
    "\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# split the data into training and test sets\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get businesses a user interacted with\n",
    "def get_user_businesses(user_id, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT business_id, stars_review FROM user_item_index WHERE user_id = ?''', (user_id,))\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top-k similar businesses for a given business\n",
    "def get_top_k_similar_businesses(business_id, k, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''SELECT similarity_vector FROM item_item_similarity WHERE item_id = ?''', (business_id,))\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result is None:\n",
    "        return []\n",
    "\n",
    "    similarity_vector = pickle.loads(result[0])\n",
    "    indices, data = similarity_vector\n",
    "\n",
    "    # Get top-k similar businesses\n",
    "    top_k = sorted(zip(indices, data), key=lambda x: -x[1])[:k]\n",
    "\n",
    "    # Map indices to business ids\n",
    "    similar_businesses = [(list(business_mapping.keys())[idx], score) for idx, score in top_k]\n",
    "\n",
    "    return similar_businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_business_interest(user_id, business_id, conn):\n",
    "    user_businesses = get_user_businesses(user_id, conn)\n",
    "\n",
    "    if not user_businesses:\n",
    "        return 0  # User has no previous interactions\n",
    "\n",
    "    # Convert user_businesses to a dictionary for fast lookup\n",
    "    user_ratings_dict = {biz_id: rating for biz_id, rating in user_businesses}\n",
    "\n",
    "    # Compute the user's average rating\n",
    "    user_avg_rating = np.mean(list(user_ratings_dict.values()))\n",
    "\n",
    "    # Get top-K similar businesses\n",
    "    similar_businesses = get_top_k_similar_businesses(business_id, k=100, conn=conn)\n",
    "\n",
    "    weighted_sum = 0\n",
    "    similarity_sum = 0\n",
    "\n",
    "    for similar_biz, similarity in similar_businesses:\n",
    "        if similar_biz in user_ratings_dict:\n",
    "            rating = user_ratings_dict[similar_biz]\n",
    "            weighted_sum += similarity * (rating - user_avg_rating)\n",
    "            similarity_sum += similarity\n",
    "\n",
    "    # Return user_avg_rating if no similar business has been rated\n",
    "    if similarity_sum == 0:\n",
    "        # return user_avg_rating\n",
    "        return -1\n",
    "    return user_avg_rating + (weighted_sum / similarity_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict user interests based on similar businesses\n",
    "def predict_user_interests(user_id, k=10, conn=conn):\n",
    "    user_businesses = get_user_businesses(user_id, conn)\n",
    "\n",
    "    recommended_businesses = {}\n",
    "    for business_id, _ in user_businesses:\n",
    "        similar_businesses = get_top_k_similar_businesses(business_id, k, conn)\n",
    "\n",
    "        for similar_business_id, score in similar_businesses:\n",
    "            if similar_business_id in recommended_businesses:\n",
    "                recommended_businesses[similar_business_id] += score\n",
    "            else:\n",
    "                recommended_businesses[similar_business_id] = score\n",
    "\n",
    "    # Sort recommendations by score\n",
    "    recommended_businesses = sorted(recommended_businesses.items(), key=lambda x: -x[1])\n",
    "\n",
    "    return recommended_businesses[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 136473\n",
      "Number of negative reviews: 32929\n",
      "Total number of reviews: 197147\n",
      "Ratio of positive to negative reviews: 4.14\n"
     ]
    }
   ],
   "source": [
    "# get the number of positive and negative reviews in the test data\n",
    "positive_reviews = test_data[test_data['stars_review'] >= 4]\n",
    "negative_reviews = test_data[test_data['stars_review'] <= 2]\n",
    "\n",
    "print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "print(f\"Total number of reviews: {len(test_data)}\")\n",
    "print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_test_data(positive_reviews, negative_reviews):\n",
    "    # down-sample the positive reviews to balance the dataset\n",
    "    positive_reviews_downsampled = positive_reviews.sample(n=len(negative_reviews), random_state=42)\n",
    "\n",
    "    # combine the down-sampled positive reviews with the negative reviews\n",
    "    balanced_test_data = pd.concat([positive_reviews_downsampled, negative_reviews], ignore_index=True)\n",
    "\n",
    "    # shuffle the balanced test data\n",
    "    balanced_test_data = balanced_test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # new statistics for the balanced test data\n",
    "    positive_reviews = balanced_test_data[balanced_test_data['stars_review'] >= 4]\n",
    "    negative_reviews = balanced_test_data[balanced_test_data['stars_review'] <= 2]\n",
    "\n",
    "    print(f\"Number of positive reviews: {len(positive_reviews)}\")\n",
    "    print(f\"Number of negative reviews: {len(negative_reviews)}\")\n",
    "    print(f\"Total number of reviews: {len(balanced_test_data)}\")\n",
    "    print(f\"Ratio of positive to negative reviews: {len(positive_reviews) / len(negative_reviews):.2f}\")\n",
    "    return balanced_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the test data, comment this line to use the original test data\n",
    "# test_data = balance_test_data(positive_reviews, negative_reviews)\n",
    "\n",
    "# group the test data by user_id and get the business_id\n",
    "test_data_grouped = test_data.groupby('user_id')['business_id'].apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized code (run time: 4m)\n",
    "# Initialize lists to store predictions and actual values\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "unrated_count = 0  # Count how many times -1 is returned\n",
    "postivie_count = 0\n",
    "negative_count = 0\n",
    "null_count = 0\n",
    "k = min(1000, len(test_data_grouped))  # Ensure k does not exceed available data\n",
    "\n",
    "# Convert test_data into a dictionary for fast lookups\n",
    "test_data_dict = {\n",
    "    (row['user_id'], row['business_id']): row['stars_review']\n",
    "    for _, row in test_data.iterrows()\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over user-business pairs\n",
    "for i in range(k):\n",
    "    user_id = test_data_grouped['user_id'].iloc[i]\n",
    "    business_ids = test_data_grouped['business_id'].iloc[i]\n",
    "\n",
    "    for business_id in business_ids:\n",
    "        predicted_rating = get_business_interest(user_id, business_id, conn)\n",
    "        \n",
    "        # Lookup actual rating using dictionary for O(1) access\n",
    "        actual_rating = test_data_dict.get((user_id, business_id), None)\n",
    "        \n",
    "        if actual_rating is None:\n",
    "            null_count += 1\n",
    "        else:\n",
    "            if actual_rating >= 4:\n",
    "                postivie_count += 1\n",
    "            else:\n",
    "                negative_count += 1\n",
    "        \n",
    "        if actual_rating is None or predicted_rating == -1:\n",
    "            unrated_count += 1\n",
    "            continue  # Skip evaluation for unrated items\n",
    "\n",
    "        # Convert ratings to binary labels (positive: 1, negative: 0)\n",
    "        predicted_labels.append(predicted_rating >= 4)\n",
    "        actual_labels.append(actual_rating >= 4)\n",
    "\n",
    "# Convert to NumPy arrays for potential vectorized operations later\n",
    "predicted_labels = np.array(predicted_labels, dtype=np.int8)\n",
    "actual_labels = np.array(actual_labels, dtype=np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy:  0.9104\n",
      "Precision: 0.9643\n",
      "Recall:    0.9039\n",
      "F1-score:  0.9331\n",
      "F-beta (β=2): 0.9154\n",
      "Unrated items (predicted -1): 1 (0.04%)\n",
      "\n",
      "Confusion Matrix Breakdown:\n",
      "True Positives (TP):  1646\n",
      "True Negatives (TN):  751\n",
      "False Positives (FP): 61\n",
      "False Negatives (FN): 175\n",
      "\n",
      "Review Breakdown:\n",
      "Positive reviews: 1821 (69.13%)\n",
      "Negative reviews: 813 (30.87%)\n",
      "Null reviews: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure we don't divide by zero\n",
    "if len(predicted_labels) > 0:\n",
    "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "    precision = precision_score(actual_labels, predicted_labels)\n",
    "    recall = recall_score(actual_labels, predicted_labels)\n",
    "    f1 = f1_score(actual_labels, predicted_labels)\n",
    "    f_beta = fbeta_score(actual_labels, predicted_labels, beta=2)  # Adjust beta as needed\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(actual_labels, predicted_labels).ravel()\n",
    "else:\n",
    "    accuracy = precision = recall = f1 = 0  # No valid predictions\n",
    "    tn = fp = fn = tp = 0\n",
    "    \n",
    "Total = len(predicted_labels) + unrated_count\n",
    "# Print results\n",
    "print(f\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n",
    "print(f\"F-beta (β=2): {f_beta:.4f}\")\n",
    "print(f\"Unrated items (predicted -1): {unrated_count} ({unrated_count / Total:.2%})\")\n",
    "\n",
    "# Print confusion matrix values\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Positives (TP):  {tp}\")   # Model correctly predicted positive\n",
    "print(f\"True Negatives (TN):  {tn}\")   # Model correctly predicted negative\n",
    "print(f\"False Positives (FP): {fp}\")   # Model incorrectly predicted positive\n",
    "print(f\"False Negatives (FN): {fn}\")   # Model incorrectly predicted negative\n",
    "\n",
    "print(\"\\nReview Breakdown:\")\n",
    "print(f\"Positive reviews: {postivie_count} ({postivie_count / Total:.2%})\")\n",
    "print(f\"Negative reviews: {negative_count} ({negative_count / Total:.2%})\")\n",
    "print(f\"Null reviews: {null_count} ({null_count / Total:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-mCG3KVaj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
