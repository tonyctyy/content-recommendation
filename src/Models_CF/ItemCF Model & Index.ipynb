{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-base Collaborative Filtering - Model & Index\n",
    "This notebook demonstrates how to build a Item-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_ItemCF.db` file.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_ItemCF.db` file to the `../../data/processed_data` folder.\n",
    "2. Update the `ItemCF.py` file in the `../backend/models` folder if there is changes in retrieval process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "df_concat[\"timestamp\"] = pd.to_datetime(df_concat[\"date\"]).astype(int) // 10**9\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>razUB7ciYZluvxWM6shmtw</td>\n",
       "      <td>--30_8IhuyMHbSOcNWd6DQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3YhG4h4Ok654iVfqdmkuRg</td>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VyC2fG4dcMG07nrxh4jLnw</td>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q5jOFJYhIsN8ouJ1rnsLQQ</td>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gdcRlubKDmslUYFPHUp1Cg</td>\n",
       "      <td>--8IbOsAAxjKRoYsBFL-PA</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985727</th>\n",
       "      <td>TkwnhxZfy7AFW1cEIn5u1A</td>\n",
       "      <td>zznJox6-nmXlGYNWgTDwQQ</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985728</th>\n",
       "      <td>weuxfeOxeGs8InkBS1ivbQ</td>\n",
       "      <td>zznJox6-nmXlGYNWgTDwQQ</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985729</th>\n",
       "      <td>Gix3hMYtxiiQd4Pg626GfQ</td>\n",
       "      <td>zznJox6-nmXlGYNWgTDwQQ</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985730</th>\n",
       "      <td>rB1vREB0x_uynI0ADMs2iA</td>\n",
       "      <td>zztOG2cKm87I6Iw_tleZsQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985731</th>\n",
       "      <td>7UEjHw0g1Wm13Q-MngsdUQ</td>\n",
       "      <td>zztOG2cKm87I6Iw_tleZsQ</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>985732 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  stars_review\n",
       "0       razUB7ciYZluvxWM6shmtw  --30_8IhuyMHbSOcNWd6DQ           5.0\n",
       "1       3YhG4h4Ok654iVfqdmkuRg  --7PUidqRWpRSpXebiyxTg           2.0\n",
       "2       VyC2fG4dcMG07nrxh4jLnw  --7PUidqRWpRSpXebiyxTg           1.0\n",
       "3       Q5jOFJYhIsN8ouJ1rnsLQQ  --7PUidqRWpRSpXebiyxTg           1.0\n",
       "4       gdcRlubKDmslUYFPHUp1Cg  --8IbOsAAxjKRoYsBFL-PA           2.0\n",
       "...                        ...                     ...           ...\n",
       "985727  TkwnhxZfy7AFW1cEIn5u1A  zznJox6-nmXlGYNWgTDwQQ           4.0\n",
       "985728  weuxfeOxeGs8InkBS1ivbQ  zznJox6-nmXlGYNWgTDwQQ           3.0\n",
       "985729  Gix3hMYtxiiQd4Pg626GfQ  zznJox6-nmXlGYNWgTDwQQ           1.0\n",
       "985730  rB1vREB0x_uynI0ADMs2iA  zztOG2cKm87I6Iw_tleZsQ           5.0\n",
       "985731  7UEjHw0g1Wm13Q-MngsdUQ  zztOG2cKm87I6Iw_tleZsQ           5.0\n",
       "\n",
       "[985732 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)\n",
    "\n",
    "user_business = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix using weighted_stars\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace NaN values in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Convert ratings to binary (1 if interacted, 0 otherwise)\n",
    "binary_user_item_sparse = (user_item_sparse > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "    \n",
    "    # Compute the top N cosine similarities in a sparse format\n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_topn(A, top_n=50, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity for items in a sparse user-item matrix efficiently.\n",
    "    Returns a sparse matrix containing only the top N similar items per item.\n",
    "    \"\"\"\n",
    "    # Convert to binary interactions\n",
    "    A_bin = (A > 0).astype(int)\n",
    "\n",
    "    # Compute intersection (co-occurrence): A_bin.T @ A_bin\n",
    "    intersection = A_bin.T @ A_bin\n",
    "\n",
    "    # Compute item-wise interaction counts (sparse)\n",
    "    item_sums = np.array(A_bin.sum(axis=0)).flatten()\n",
    "\n",
    "    # Compute union using broadcasting (sparse)\n",
    "    row_indices, col_indices = intersection.nonzero()  # Get non-zero indices\n",
    "    intersection_values = intersection.data  # Co-occurrence values\n",
    "\n",
    "    # Compute union: |A| + |B| - |A âˆ© B|\n",
    "    union_values = item_sums[row_indices] + item_sums[col_indices] - intersection_values\n",
    "\n",
    "    # Compute Jaccard similarity\n",
    "    jaccard_values = intersection_values / union_values\n",
    "\n",
    "    # Apply thresholding\n",
    "    mask = jaccard_values >= threshold\n",
    "    row_indices, col_indices, jaccard_values = row_indices[mask], col_indices[mask], jaccard_values[mask]\n",
    "\n",
    "    # Create sparse matrix for efficient storage\n",
    "    jaccard_sim_sparse = sp.csr_matrix(\n",
    "        (jaccard_values, (row_indices, col_indices)), shape=(A.shape[1], A.shape[1])\n",
    "    )\n",
    "\n",
    "    # Keep only top-N similar items\n",
    "    jaccard_sim_sparse = sp_matmul_topn(jaccard_sim_sparse, jaccard_sim_sparse, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return jaccard_sim_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute item similarity using Jaccard\n",
    "item_similarity_sparse = jaccard_similarity_topn(binary_user_item_sparse, top_n=50, threshold=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_db(conn):\n",
    "    \"\"\"Apply SQLite performance optimizations.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def insert_user_item(user_business, conn, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for user-item interactions.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_records = len(user_business)\n",
    "    data = user_business[['user_id', 'business_id', 'stars_review']].values.tolist()\n",
    "\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''', batch)\n",
    "\n",
    "        if i % (batch_size * 5) == 0:  # Commit every 5 batches\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} user-item records.\")\n",
    "\n",
    "    conn.commit()  # Final commit\n",
    "    print(f\"Total {total_records} user-item records inserted.\")\n",
    "\n",
    "\n",
    "def insert_item_vectors(item_similarity_sparse, business_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    \"\"\"Optimized batch insert for item similarity vectors.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    business_keys = list(business_mapping.keys())  # Convert keys to list for faster indexing\n",
    "\n",
    "    for row_idx in range(item_similarity_sparse.shape[0]):\n",
    "        row_vector = item_similarity_sparse.getrow(row_idx)\n",
    "        row_indices = row_vector.indices\n",
    "        row_data = row_vector.data\n",
    "\n",
    "        serialized_row = pickle.dumps((row_indices, row_data))\n",
    "        item_id = business_keys[row_idx]  # Faster lookup\n",
    "\n",
    "        batch.append((item_id, serialized_row))\n",
    "\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} item vectors...\")\n",
    "\n",
    "            batch = []\n",
    "\n",
    "    if batch:  # Insert remaining records\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        total_inserted += len(batch)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_inserted} item vectors inserted.\")\n",
    "\n",
    "\n",
    "def insert_mappings(business_mapping, conn, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for business mappings.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    data = list(business_mapping.items())\n",
    "    total_records = len(data)\n",
    "\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO business_mapping (business_id, business_idx)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "\n",
    "        if i % (batch_size * 5) == 0:  # Commit every 5 batches\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} business mappings.\")\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} business mappings inserted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "db_path = './yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "# Create tables for user-item and item-item indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS item_item_similarity (\n",
    "    item_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_item_similarity ON item_item_similarity(item_id)''')\n",
    "\n",
    "# cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "#     user_id TEXT PRIMARY KEY,\n",
    "#     user_idx INTEGER\n",
    "# )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY,\n",
    "    business_idx INTEGER\n",
    ")''')\n",
    "\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 788585 user-item records.\n",
      "Inserted 300000 / 788585 user-item records.\n",
      "Inserted 550000 / 788585 user-item records.\n",
      "Inserted 788585 / 788585 user-item records.\n",
      "Total 788585 user-item records inserted.\n",
      "Inserted 50000 item vectors...\n",
      "Total 74698 item vectors inserted.\n",
      "Inserted 50000 / 74698 business mappings.\n",
      "Total 74698 business mappings inserted.\n"
     ]
    }
   ],
   "source": [
    "insert_user_item(user_business, conn)\n",
    "insert_item_vectors(item_similarity_sparse, business_mapping, conn)\n",
    "insert_mappings(business_mapping, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection when done\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-mCG3KVaj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
