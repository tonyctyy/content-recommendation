{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-base Collaborative Filtering - Model & Index\n",
    "This notebook demonstrates how to build a User-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_UserCF.db` file.\n",
    "\n",
    "Objective: Build a basic UserCF model for retrieval and prediction.  \n",
    "Strategy: Use cosine similarity on training data with time-decay function; store in yelp_UserCF.db.  \n",
    "Note: Test data balancing (50% +ve) handled in evaluation notebook.  \n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_UserCF.db` file to the `../../data/processed_data` folder.\n",
    "2. Update the `UserCF.py` file in the `../backend/models` folder if there is changes in retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utilities and dependencies\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "\n",
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dataframes\n",
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply time decay to ratings\n",
    "LAMBDA = 0.0000000005\n",
    "current_timestamp = int(time.time())\n",
    "df_review['timestamp'] = pd.to_datetime(df_review['date']).astype(int) // 10**9\n",
    "df_review['timestamp'] = np.exp(-LAMBDA * (current_timestamp - df_review[\"timestamp\"]))\n",
    "df_review['stars'] = df_review['timestamp'] * df_review['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare user-business interaction data\n",
    "def get_user_business_with_time(df_business, df_review):\n",
    "    df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "    user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]\n",
    "    user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "    business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}    \n",
    "    return user_mapping, business_mapping, user_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mappings and interaction data\n",
    "user_mapping, business_mapping, user_business = get_user_business_with_time(df_business, df_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (80%) and test (20%)\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)\n",
    "user_business = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to indices and create sparse matrix\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute top-N cosine similarities\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    C = sp_matmul_topn(A, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user-user similarity\n",
    "user_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database optimization function\n",
    "def optimize_db(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "# Batch insert for user-item interactions\n",
    "def insert_user_item(user_business, conn, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_records = len(user_business)\n",
    "    data = user_business[['user_id', 'business_id', 'stars_review']].values.tolist()\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''', batch)\n",
    "        if i % (batch_size * 5) == 0:\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} user-item records.\")\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} user-item records inserted.\")\n",
    "\n",
    "# Batch insert for user similarity vectors\n",
    "def insert_user_vectors(user_similarity_sparse, user_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    user_keys = list(user_mapping.keys())\n",
    "    for row_idx in range(user_similarity_sparse.shape[0]):\n",
    "        row_vector = user_similarity_sparse.getrow(row_idx)\n",
    "        row_indices = row_vector.indices\n",
    "        row_data = row_vector.data\n",
    "        serialized_row = pickle.dumps((row_indices, row_data))\n",
    "        user_id = user_keys[row_idx]\n",
    "        batch.append((user_id, serialized_row))\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} user vectors...\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        total_inserted += len(batch)\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_inserted} user vectors inserted.\")\n",
    "\n",
    "# Batch insert for user mappings\n",
    "def insert_mappings(user_mapping, conn, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    data = list(user_mapping.items())\n",
    "    total_records = len(data)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO user_mapping (user_id, user_idx)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        if i % (batch_size * 5) == 0:\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} user mappings.\")\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} user mappings inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite and create tables\n",
    "db_path = './yelp_UserCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_user_similarity (\n",
    "    user_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_user_similarity ON user_user_similarity(user_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "    user_id TEXT PRIMARY KEY,\n",
    "    user_idx INTEGER\n",
    ")''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 788585 user-item records.\n",
      "Inserted 300000 / 788585 user-item records.\n",
      "Inserted 550000 / 788585 user-item records.\n",
      "Inserted 788585 / 788585 user-item records.\n",
      "Total 788585 user-item records inserted.\n",
      "Inserted 50000 user vectors...\n",
      "Inserted 100000 user vectors...\n",
      "Inserted 150000 user vectors...\n",
      "Total 162080 user vectors inserted.\n",
      "Inserted 50000 / 162080 user mappings.\n",
      "Total 162080 user mappings inserted.\n"
     ]
    }
   ],
   "source": [
    "# Insert data into database\n",
    "insert_user_item(user_business, conn)\n",
    "insert_user_vectors(user_similarity_sparse, user_mapping, conn)\n",
    "insert_mappings(user_mapping, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-HkY1UuQH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
