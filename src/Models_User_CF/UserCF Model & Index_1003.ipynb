{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-base Collaborative Filtering - Model & Index\n",
    "This notebook demonstrates how to build a User-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_UserCF.db` file.\n",
    "\n",
    "Objective: Build a basic UserCF model for retrieval and prediction.  \n",
    "Strategy: Use Jaccard similarity on training data; store in yelp_UserCF.db.\n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_UserCF.db` file to the `../../data/processed_data` folder.\n",
    "2. Update the `UserCF.py` file in the `../backend/models` folder if there is changes in retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python file from ../utilities.py\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "\n",
    "import scipy.sparse as sp\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load Yelp data\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "df_concat[\"timestamp\"] = pd.to_datetime(df_concat[\"date\"]).astype(int) // 10**9\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (80%) and test (20%); use train for model\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)\n",
    "user_business = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix using stars_review\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace NaN values in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Convert ratings to binary (1 if interacted, 0 otherwise) for Jaccard\n",
    "binary_user_item_sparse = (user_item_sparse > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user-user Jaccard similarity\n",
    "def jaccard_similarity_topn(A, top_n=50, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity for users in a sparse user-item matrix efficiently.\n",
    "    Returns a sparse matrix containing only the top N similar users per user.\n",
    "    \"\"\"\n",
    "    # Convert to binary interactions\n",
    "    A_bin = (A > 0).astype(int)\n",
    "\n",
    "    # Compute intersection (co-occurrence): A @ A.T\n",
    "    intersection = A_bin @ A_bin.T\n",
    "\n",
    "    # Compute user-wise interaction counts (sparse)\n",
    "    user_sums = np.array(A_bin.sum(axis=1)).flatten()\n",
    "\n",
    "    # Compute union using non-zero indices\n",
    "    row_indices, col_indices = intersection.nonzero()\n",
    "    intersection_values = intersection.data\n",
    "\n",
    "    # Compute union: |A| + |B| - |A âˆ© B|\n",
    "    union_values = user_sums[row_indices] + user_sums[col_indices] - intersection_values\n",
    "\n",
    "    # Compute Jaccard similarity\n",
    "    jaccard_values = intersection_values / union_values\n",
    "\n",
    "    # Apply thresholding\n",
    "    mask = jaccard_values >= threshold\n",
    "    row_indices, col_indices, jaccard_values = row_indices[mask], col_indices[mask], jaccard_values[mask]\n",
    "\n",
    "    # Create sparse matrix for efficient storage\n",
    "    jaccard_sim_sparse = csr_matrix(\n",
    "        (jaccard_values, (row_indices, col_indices)), shape=(A.shape[0], A.shape[0])\n",
    "    )\n",
    "\n",
    "    # Keep only top-N similar users using sparse_dot_topn\n",
    "    jaccard_sim_sparse = sp_matmul_topn(jaccard_sim_sparse, jaccard_sim_sparse, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return jaccard_sim_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user similarity using Jaccard\n",
    "user_similarity_sparse = jaccard_similarity_topn(binary_user_item_sparse, top_n=50, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_db(conn):\n",
    "    \"\"\"Apply SQLite performance optimizations.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "# Optimized batch insert for user-item interactions\n",
    "def insert_user_item(user_business, conn, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for user-item interactions.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_records = len(user_business)\n",
    "    data = user_business[['user_id', 'business_id', 'stars_review']].values.tolist()\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''', batch)\n",
    "        if i % (batch_size * 5) == 0:\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} user-item records.\")\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} user-item records inserted.\")\n",
    "\n",
    "# Optimized batch insert for user similarity vectors\n",
    "def insert_user_vectors(user_similarity_sparse, user_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    \"\"\"Optimized batch insert for user similarity vectors.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    user_keys = list(user_mapping.keys())\n",
    "    for row_idx in range(user_similarity_sparse.shape[0]):\n",
    "        row_vector = user_similarity_sparse.getrow(row_idx)\n",
    "        serialized_row = pickle.dumps((row_vector.indices, row_vector.data))\n",
    "        user_id = user_keys[row_idx]\n",
    "        batch.append((user_id, serialized_row))\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} user vectors...\")\n",
    "            batch = []\n",
    "    if batch:\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        total_inserted += len(batch)\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_inserted} user vectors inserted.\")\n",
    "\n",
    "# Optimized batch insert for mappings\n",
    "def insert_mappings(mapping, conn, table_name, key_col, val_col, batch_size=50000):\n",
    "    \"\"\"Optimized batch insert for mappings.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    data = list(mapping.items())\n",
    "    total_records = len(data)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        cursor.executemany(f'''INSERT OR REPLACE INTO {table_name} ({key_col}, {val_col})\n",
    "                              VALUES (?, ?)''', batch)\n",
    "        if i % (batch_size * 5) == 0:\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {i + len(batch)} / {total_records} {table_name} records.\")\n",
    "    conn.commit()\n",
    "    print(f\"Total {total_records} {table_name} records inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite and set up database\n",
    "db_path = './yelp_UserCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "# Drop existing indexes if they exist\n",
    "cursor.execute('''DROP INDEX IF EXISTS idx_user_item''')\n",
    "cursor.execute('''DROP INDEX IF EXISTS idx_user_similarity''')\n",
    "\n",
    "# Create tables for user-item and user-user indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_user_similarity (\n",
    "    user_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX idx_user_similarity ON user_user_similarity(user_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "    user_id TEXT PRIMARY KEY,\n",
    "    user_idx INTEGER\n",
    ")''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY,\n",
    "    business_idx INTEGER\n",
    ")''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 788585 user-item records.\n",
      "Inserted 300000 / 788585 user-item records.\n",
      "Inserted 550000 / 788585 user-item records.\n",
      "Inserted 788585 / 788585 user-item records.\n",
      "Total 788585 user-item records inserted.\n",
      "Inserted 50000 user vectors...\n",
      "Inserted 100000 user vectors...\n",
      "Total 148521 user vectors inserted.\n",
      "Inserted 50000 / 148521 user_mapping records.\n",
      "Total 148521 user_mapping records inserted.\n",
      "Inserted 50000 / 74698 business_mapping records.\n",
      "Total 74698 business_mapping records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Insert data into database\n",
    "insert_user_item(user_business, conn)\n",
    "insert_user_vectors(user_similarity_sparse, user_mapping, conn)\n",
    "insert_mappings(user_mapping, conn, 'user_mapping', 'user_id', 'user_idx')\n",
    "insert_mappings(business_mapping, conn, 'business_mapping', 'business_id', 'business_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-HkY1UuQH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
