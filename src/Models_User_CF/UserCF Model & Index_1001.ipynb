{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-base Collaborative Filtering - Model & Index\n",
    "This notebook demonstrates how to build a User-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_UserCF.db` file.\n",
    "\n",
    "Objective: Build a basic UserCF model for retrieval and prediction.  \n",
    "Strategy: Use cosine similarity on training data; store in yelp_UserCF.db.  \n",
    "Note: Test data balancing (50% +ve) handled in evaluation notebook.  \n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.\n",
    "2. Have the virtual environment setup and used for the notebook.\n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_UserCF.db` file to the `../../data/processed_data` folder.\n",
    "2. Update the `UserCF.py` file in the `../backend/models` folder if there is changes in retrieval process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python file from ../utilities.py\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load Yelp data\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "user_mapping, business_mapping, user_business = get_user_business(df_business, df_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (80%) and test (20%); use train for model\n",
    "train_data, test_data = train_test_split(user_business, test_size=0.2, random_state=42)\n",
    "user_business = train_data.copy()  # Model built on training data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to indices (using full mappings for consistency with retrieval)\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse user-item matrix\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user-user similarity (baseline: cosine similarity)\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    C = sp_matmul_topn(A, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database optimization and insertion functions\n",
    "def optimize_db(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def insert_user_item(user_business, conn, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_records = len(user_business)\n",
    "    data = user_business[['user_id', 'business_id', 'stars_review']].values.tolist()\n",
    "    try:\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                                  VALUES (?, ?, ?)''', batch)\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {i + len(batch)} / {total_records} user-item records.\")\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_records} user-item records inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting user-item records: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_user_vectors(user_similarity_sparse, user_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    user_keys = list(user_mapping.keys())\n",
    "    try:\n",
    "        for row_idx in range(user_similarity_sparse.shape[0]):\n",
    "            row_vector = user_similarity_sparse.getrow(row_idx)\n",
    "            serialized_row = pickle.dumps((row_vector.indices, row_vector.data))\n",
    "            user_id = user_keys[row_idx]\n",
    "            batch.append((user_id, serialized_row))\n",
    "            if len(batch) >= batch_size:\n",
    "                cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                                      VALUES (?, ?)''', batch)\n",
    "                total_inserted += len(batch)\n",
    "                if total_inserted % progress_interval == 0:\n",
    "                    print(f\"Inserted {total_inserted} user vectors...\")\n",
    "                batch = []\n",
    "        if batch:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO user_user_similarity (user_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_inserted} user vectors inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting user vectors: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_mappings(mapping, conn, table_name, key_col, val_col, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    data = list(mapping.items())\n",
    "    total_records = len(data)\n",
    "    try:\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            cursor.executemany(f'''INSERT OR REPLACE INTO {table_name} ({key_col}, {val_col})\n",
    "                                   VALUES (?, ?)''', batch)\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {i + len(batch)} / {total_records} {table_name} records.\")\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_records} {table_name} records inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting {table_name} records: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up database\n",
    "db_path = './yelp_UserCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT, business_id TEXT, stars_review REAL, PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_user_similarity (\n",
    "    user_id TEXT PRIMARY KEY, similarity_vector BLOB\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_user_similarity ON user_user_similarity(user_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "    user_id TEXT PRIMARY KEY, user_idx INTEGER\n",
    ")''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY, business_idx INTEGER\n",
    ")''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 788585 user-item records.\n",
      "Inserted 300000 / 788585 user-item records.\n",
      "Inserted 550000 / 788585 user-item records.\n",
      "Inserted 788585 / 788585 user-item records.\n",
      "Total 788585 user-item records inserted.\n",
      "Inserted 50000 user vectors...\n",
      "Inserted 100000 user vectors...\n",
      "Inserted 150000 user vectors...\n",
      "Total 162080 user vectors inserted.\n",
      "Inserted 50000 / 162080 user_mapping records.\n",
      "Total 162080 user_mapping records inserted.\n",
      "Inserted 50000 / 78059 business_mapping records.\n",
      "Total 78059 business_mapping records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "insert_user_item(user_business, conn)\n",
    "insert_user_vectors(user_similarity_sparse, user_mapping, conn)\n",
    "insert_mappings(user_mapping, conn, 'user_mapping', 'user_id', 'user_idx')\n",
    "insert_mappings(business_mapping, conn, 'business_mapping', 'business_id', 'business_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-HkY1UuQH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
