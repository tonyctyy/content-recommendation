{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform Recommendation Baseline,\n",
    "    This notebook implements a UserCF & clustering model for the Yelp dataset. It divides users into clusters, recommends a set of uniform businesses to each cluster by using UserCF and evaluates the performance using retrieval metrics.\n",
    "    \n",
    "#### Pre-requisites,\n",
    "- The Yelp dataset is loaded from the processed data folder (`../../data/processed_data/yelp_data/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from hdbscan import HDBSCAN\n",
    "from IPython.display import display\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "\n",
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data['business'] # Business details\n",
    "df_categories = yelp_data['categories'] # Business categories\n",
    "df_review = yelp_data['review'] # Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review['label'] = df_review['stars'].apply(lambda x: 1 if x > 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge review and business data\n",
    "df_review_merged = df_review.merge(df_business, on='business_id', how='left', suffixes=('_review', '_business'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare category features (top 5 categories as binary indicators)\n",
    "top_categories = df_categories['category'].value_counts().index[:5]\n",
    "for cat in top_categories:\n",
    "    df_review_merged[f'category_{cat}'] = df_review_merged['business_id'].isin(\n",
    "        df_categories[df_categories['category'] == cat]['business_id']\n",
    "    ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for clustering with corrected column names\n",
    "features = ['stars_review', 'useful', 'funny', 'cool', 'latitude', 'longitude', \n",
    "            'stars_business', 'review_count'] + [f'category_{cat}' for cat in top_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in user_features: 162079\n"
     ]
    }
   ],
   "source": [
    "# Aggregate features at the user level (mean values)\n",
    "user_features = df_review_merged.groupby('user_id')[features].mean().reset_index()\n",
    "print(f\"Number of unique users in user_features: {len(user_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train_data: 784334\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(df_review_merged, test_size=0.2, random_state=42)\n",
    "print(f\"Number of rows in train_data: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X for clustering: (162079, 13)\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix for clustering (using training data)\n",
    "X = user_features[features].fillna(0).values\n",
    "print(f\"Shape of X for clustering: {X.shape}\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_pca_reduced: (162079, 10)\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA (10 components explaining 90% variance)\n",
    "pca = PCA(n_components=10)\n",
    "X_pca_reduced = pca.fit_transform(X_scaled)\n",
    "print(f\"Shape of X_pca_reduced: {X_pca_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gigi Wan\\.virtualenvs\\content-recommendation-HkY1UuQH\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Gigi Wan\\.virtualenvs\\content-recommendation-HkY1UuQH\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of user_clusters: 162079\n"
     ]
    }
   ],
   "source": [
    "# Perform HDBSCAN clustering on users\n",
    "hdbscan = HDBSCAN(min_cluster_size=5, min_samples=5, cluster_selection_epsilon=0.3, metric='euclidean')\n",
    "user_clusters = hdbscan.fit_predict(X_pca_reduced.astype(np.float32))\n",
    "print(f\"Length of user_clusters: {len(user_clusters)}\")  # Should match 162,079\n",
    "user_features['cluster'] = user_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster labels back to review data\n",
    "train_data = train_data.merge(user_features[['user_id', 'cluster']], on='user_id', how='left')\n",
    "test_data = test_data.merge(user_features[['user_id', 'cluster']], on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix shape: (148394, 70480), non-zero elements: 768999\n"
     ]
    }
   ],
   "source": [
    "# Create sparse user-item matrix for training data\n",
    "users = train_data['user_id'].unique()\n",
    "businesses = train_data['business_id'].unique()\n",
    "user_id_map = {uid: i for i, uid in enumerate(users)}\n",
    "business_id_map = {bid: i for i, bid in enumerate(businesses)}\n",
    "\n",
    "rows = train_data['user_id'].map(user_id_map)\n",
    "cols = train_data['business_id'].map(business_id_map)\n",
    "data = train_data['stars_review']\n",
    "user_item_matrix = csr_matrix((data, (rows, cols)), shape=(len(users), len(businesses)))\n",
    "print(f\"Sparse matrix shape: {user_item_matrix.shape}, non-zero elements: {user_item_matrix.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute UserCF predictions within clusters using sparse matrix\n",
    "def predict_usercf_clustered(user_id, business_id, user_item_matrix, user_features, cluster_id, user_id_map, business_id_map, k=10):\n",
    "    if user_id not in user_id_map:\n",
    "        return np.mean(user_item_matrix.data)  # Cold-start: return average of non-zero ratings\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    \n",
    "    # Get users in the same cluster\n",
    "    cluster_users = user_features[user_features['cluster'] == cluster_id]['user_id']\n",
    "    cluster_indices = [user_id_map[uid] for uid in cluster_users if uid in user_id_map]\n",
    "    \n",
    "    if len(cluster_indices) <= 1:  # If cluster is too small, use all users\n",
    "        cluster_indices = list(range(user_item_matrix.shape[0]))\n",
    "    \n",
    "    # Extract cluster submatrix\n",
    "    cluster_matrix = user_item_matrix[cluster_indices]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    user_vector = user_item_matrix[user_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(user_vector, cluster_matrix).flatten()\n",
    "    similarity_dict = dict(zip(cluster_indices, similarities))\n",
    "    \n",
    "    # Sort by similarity and take top k\n",
    "    top_k_users = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    top_k_indices = [u for u, _ in top_k_users]\n",
    "    top_k_similarities = [s for _, s in top_k_users]\n",
    "    \n",
    "    # Predict rating for the business\n",
    "    if business_id not in business_id_map:\n",
    "        return np.mean(user_item_matrix.data)  # Cold-start for unseen business\n",
    "    \n",
    "    business_idx = business_id_map[business_id]\n",
    "    ratings = user_item_matrix[top_k_indices, business_idx].toarray().flatten()\n",
    "    weighted_sum = sum(s * r for s, r in zip(top_k_similarities, ratings) if r > 0)\n",
    "    sim_sum = sum(s for s, r in zip(top_k_similarities, ratings) if r > 0)\n",
    "    \n",
    "    return weighted_sum / sim_sum if sim_sum > 0 else np.mean(user_item_matrix.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m predicted_ratings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m business_id \u001b[38;5;129;01min\u001b[39;00m candidate_businesses:\n\u001b[1;32m---> 18\u001b[0m     rating \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_usercf_clustered\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_item_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbusiness_id_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     predicted_ratings\u001b[38;5;241m.\u001b[39mappend((business_id, rating))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Sort by predicted rating and take top k\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36mpredict_usercf_clustered\u001b[1;34m(user_id, business_id, user_item_matrix, user_features, cluster_id, user_id_map, business_id_map, k)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get users in the same cluster\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cluster_users \u001b[38;5;241m=\u001b[39m user_features[user_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cluster_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m cluster_indices \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43muser_id_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43muid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcluster_users\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_id_map\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster_indices) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# If cluster is too small, use all users\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     cluster_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(user_item_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get users in the same cluster\u001b[39;00m\n\u001b[0;32m      9\u001b[0m cluster_users \u001b[38;5;241m=\u001b[39m user_features[user_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m cluster_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m cluster_indices \u001b[38;5;241m=\u001b[39m [user_id_map[uid] \u001b[38;5;28;01mfor\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m cluster_users \u001b[38;5;28;01mif\u001b[39;00m uid \u001b[38;5;129;01min\u001b[39;00m user_id_map]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster_indices) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# If cluster is too small, use all users\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     cluster_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(user_item_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate recommendations for test users\n",
    "test_data_grouped = test_data.groupby('user_id')['business_id'].apply(list).reset_index()\n",
    "recommendations = {}\n",
    "k_recommendations = 300\n",
    "max_users = min(1000, len(test_data_grouped))\n",
    "\n",
    "for i in range(max_users):\n",
    "    user_id = test_data_grouped['user_id'].iloc[i]\n",
    "    cluster_id = user_features[user_features['user_id'] == user_id]['cluster'].values[0]\n",
    "    # Get all businesses not yet reviewed by the user\n",
    "    reviewed_businesses = set(train_data[train_data['user_id'] == user_id]['business_id'])\n",
    "    all_businesses = set(business_id_map.keys())\n",
    "    candidate_businesses = all_businesses - reviewed_businesses\n",
    "    \n",
    "    # Predict ratings for candidate businesses\n",
    "    predicted_ratings = []\n",
    "    for business_id in candidate_businesses:\n",
    "        rating = predict_usercf_clustered(user_id, business_id, user_item_matrix, user_features, cluster_id, user_id_map, business_id_map)\n",
    "        predicted_ratings.append((business_id, rating))\n",
    "    \n",
    "    # Sort by predicted rating and take top k\n",
    "    predicted_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k = predicted_ratings[:k_recommendations]\n",
    "    business_ids, scores = zip(*top_k) if top_k else ([], [])\n",
    "    recommendations[user_id] = (list(business_ids), list(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function (same as before)\n",
    "def check_recommendations(recommendations, test_data_grouped, pos=4):\n",
    "    total = 0\n",
    "    total_positive = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    ranks = []\n",
    "    for i, row in test_data_grouped.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        business_ids = row['business_id']\n",
    "        rank = 0\n",
    "        if user_id in recommendations:\n",
    "            recommended_businesses = recommendations[user_id][0]\n",
    "            for business_id in business_ids:\n",
    "                star_rating = test_data[(test_data['user_id'] == user_id) & \n",
    "                                       (test_data['business_id'] == business_id)]['stars_review'].values[0]\n",
    "                if star_rating >= pos:\n",
    "                    total_positive += 1\n",
    "                if business_id in recommended_businesses:\n",
    "                    if star_rating >= pos:\n",
    "                        true_positive += 1\n",
    "                    else:\n",
    "                        false_positive += 1\n",
    "                    rank = recommended_businesses.index(business_id) + 1\n",
    "                else:\n",
    "                    if star_rating < pos:\n",
    "                        true_negative += 1\n",
    "                    else:\n",
    "                        false_negative += 1\n",
    "            total += len(business_ids)\n",
    "        ranks.append(rank)\n",
    "    return true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate recommendations\n",
    "true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks = check_recommendations(recommendations, test_data_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric, confusion_matrix, background_stats = compute_evaluation_metric(true_positive, true_negative, false_positive, false_negative, total, total_positive, ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Data Statistics\")\n",
    "display(background_stats)\n",
    "\n",
    "print(\"Evaluation Metrics\")\n",
    "display(evaluation_metric)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-HkY1UuQH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
