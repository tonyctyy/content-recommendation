{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a POC for predicting user and business interest using the paper from Facebook. It is using Triplet Hinge Loss as the loss function. The goal is to separate the positive and negative samples by a margin, so that the positive samples are closer to the anchor than the negative samples. \n",
    "\n",
    "## Improvement Plan\n",
    "### About the Model\n",
    "1. Categorical features will be one-hot encoded/embedded.\n",
    "2. Continuous features will be normalized and handle null value.\n",
    "3. Default values will be set for unseen values in the embedding layer.\n",
    "4. Model will be tuned and optimized.\n",
    "5. More features will be added (e.g. review content, text features, etc.)\n",
    "6. Sampling bias will be discovered\n",
    "7. Adjust the dimension of each layer\n",
    "### About using the Model\n",
    "1. The model details will be saved and loaded.\n",
    "2. How to use the model will be explained.\n",
    "3. The model will be used in a web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Input, Layer, Lambda\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db', 'yelp_user_data.db', 'yelp_tip_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "        data['user'] = pd.read_sql_query(\"SELECT * FROM user_data\", conns[2])\n",
    "        data['tip'] = pd.read_sql_query(\"SELECT * FROM tip_data\", conns[3])\n",
    "        \n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_con_feature_lst = [\n",
    "                        'review_count', \n",
    "                        'useful', \n",
    "                        'funny', \n",
    "                        'cool', \n",
    "                        'fans', \n",
    "                        'average_stars'\n",
    "                        ]\n",
    "business_con_feature_lst = [\n",
    "                        'stars', \n",
    "                        'review_count', \n",
    "                        # 'latitude', \n",
    "                        # 'longitude'\n",
    "                        ]\n",
    "\n",
    "# add user features that start with 'compliment_'\n",
    "user_compliment_feature_lst = [col for col in yelp_data['user'].columns if 'compliment_' in col]\n",
    "user_con_feature_lst += user_compliment_feature_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user data\n",
    "user_df = yelp_data['user']\n",
    "user_df['yelping_since'] = pd.to_datetime(user_df['yelping_since'])\n",
    "\n",
    "# Preprocess business data\n",
    "business_df = yelp_data['business']\n",
    "business_df['is_open'] = business_df['is_open'].fillna(0).astype(int)\n",
    "\n",
    "# Preprocess review data\n",
    "review_df = yelp_data['review']\n",
    "# Create labels for review data\n",
    "review_df['label'] = (review_df['stars'] >= 4).astype(int)\n",
    "\n",
    "# Preprocess tip data\n",
    "tip_df = yelp_data['tip']\n",
    "\n",
    "# Preprocess categories data\n",
    "categories_df = yelp_data['categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all categories into a single list to fit the encoder\n",
    "unique_categories = set([cat for sublist in categories_df['category'] for cat in sublist])\n",
    "categories_encoder = LabelEncoder()\n",
    "categories_encoder.fit(list(unique_categories))\n",
    "\n",
    "categories_df['category_encoded'] = categories_encoder.transform(categories_df['category'])\n",
    "\n",
    "categories_grouped = categories_df.groupby('business_id')['category_encoded'].apply(list).reset_index()\n",
    "\n",
    "# Merge the categories with the business data, name the column 'category_encoded'\n",
    "business_df = business_df.merge(categories_grouped, on='business_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviews and average review for each business\n",
    "business_review_count = review_df.groupby('business_id').size()\n",
    "business_avg_review = review_df.groupby('business_id')['stars'].mean()\n",
    "business_df['review_count'] = business_review_count\n",
    "business_df['avg_review'] = business_avg_review # similar to stars, but this is adjusted for the number of reviews extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Extract numerical features for embedding\n",
    "user_features = user_df[user_con_feature_lst].fillna(0)\n",
    "# Example: Extract numerical features\n",
    "business_features = business_df[business_con_feature_lst].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode user_id and business_id\n",
    "user_id_encoder = LabelEncoder()\n",
    "business_id_encoder = LabelEncoder()\n",
    "\n",
    "user_df['user_id_encoded'] = user_id_encoder.fit_transform(user_df['user_id'])\n",
    "business_df['business_id_encoded'] = business_id_encoder.fit_transform(business_df['business_id'])\n",
    "\n",
    "# Save number of unique users and businesses for embedding input_dim\n",
    "num_users = user_df['user_id_encoded'].max() + 1\n",
    "num_businesses = business_df['business_id_encoded'].max() + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Encode 'city' as a discrete feature for businesses\n",
    "# business_city_encoder = LabelEncoder()\n",
    "# business_df['city_encoded'] = business_city_encoder.fit_transform(business_df['city'])\n",
    "\n",
    "# # Save number of unique cities for embedding input_dim\n",
    "# num_cities = business_df['city_encoded'].max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize user continuous features\n",
    "user_continuous_features = user_df[['review_count', 'useful', 'funny', 'cool', 'fans', 'average_stars']].fillna(0)\n",
    "user_scaler = StandardScaler()\n",
    "user_continuous_features_scaled = user_scaler.fit_transform(user_continuous_features)\n",
    "\n",
    "# Standardize business continuous features\n",
    "business_continuous_features = business_df[['stars', 'review_count', 'latitude', 'longitude']].fillna(0)\n",
    "business_scaler = StandardScaler()\n",
    "business_continuous_features_scaled = business_scaler.fit_transform(business_continuous_features)\n",
    "\n",
    "# Ensure continuous features are pandas DataFrames\n",
    "user_continuous_features_scaled = pd.DataFrame(user_continuous_features_scaled, index=user_df['user_id_encoded'])\n",
    "business_continuous_features_scaled = pd.DataFrame(business_continuous_features_scaled, index=business_df['business_id_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_layer(input_dim, output_dim, name):\n",
    "    \"\"\"Reusable function to create an embedding layer.\"\"\"\n",
    "    return layers.Embedding(input_dim=input_dim, output_dim=output_dim, name=f\"{name}_embedding\")\n",
    "\n",
    "# Create embedding layers\n",
    "user_id_embedding = create_embedding_layer(num_users, 16, \"user_id\")\n",
    "business_id_embedding = create_embedding_layer(num_businesses, 16, \"business_id\")\n",
    "# city_embedding = create_embedding_layer(num_cities, 8, \"city\")\n",
    "\n",
    "# Define the number of unique categories\n",
    "num_categories = len(categories_encoder.classes_)\n",
    "# Create an embedding layer for categories\n",
    "category_embedding = create_embedding_layer(num_categories, 16, \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example aggregation function for TensorFlow (Mean Pooling)\n",
    "def aggregate_category_embeddings(category_indices):\n",
    "    category_indices = tf.constant(category_indices, dtype=tf.int32)\n",
    "    embeddings = category_embedding(category_indices)\n",
    "    return tf.reduce_mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unseen user_id and business_id\n",
    "review_df = review_df[\n",
    "    (review_df['user_id'].isin(user_id_encoder.classes_)) & \n",
    "    (review_df['business_id'].isin(business_id_encoder.classes_))\n",
    "]\n",
    "\n",
    "# Encode user_id and business_id\n",
    "review_df['user_id_encoded'] = user_id_encoder.transform(review_df['user_id'])\n",
    "review_df['business_id_encoded'] = business_id_encoder.transform(review_df['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    user_id_input = layers.Input(shape=(1,), name=\"user_id\")\n",
    "    user_continuous_input = layers.Input(shape=(continuous_dim,), name=\"user_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    user_id_embedded = user_id_embedding(user_id_input)\n",
    "    user_id_embedded = layers.Flatten()(user_id_embedded)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([user_id_embedded, user_continuous_input])\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    user_embedding = layers.Dense(16, activation=None, name=\"user_embedding\")(x)\n",
    "\n",
    "    return Model([user_id_input, user_continuous_input], user_embedding, name=\"UserTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryPoolingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CategoryPoolingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_tower(continuous_dim):\n",
    "    # Inputs\n",
    "    business_id_input = layers.Input(shape=(1,), name=\"business_id\")\n",
    "    business_continuous_input = layers.Input(shape=(continuous_dim,), name=\"business_continuous\")\n",
    "\n",
    "    # Embedding\n",
    "    business_id_embedded = business_id_embedding(business_id_input)\n",
    "    business_id_embedded = layers.Flatten()(business_id_embedded)\n",
    "\n",
    "    category_input = layers.Input(shape=(None,), dtype=\"int32\", name=\"category_indices\")  # Variable-length input\n",
    "    category_embeddings = category_embedding(category_input)\n",
    "    # aggregated_category_embedding = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1), name=\"category_pooling\", output_shape=(16,))(category_embeddings)\n",
    "    aggregated_category_embedding = CategoryPoolingLayer(name=\"category_pooling\")(category_embeddings)\n",
    "\n",
    "    # Combine\n",
    "    concat = layers.Concatenate()([business_id_embedded, aggregated_category_embedding, business_continuous_input])\n",
    "\n",
    "    x = layers.Dense(64, activation='relu')(concat)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    business_embedding = layers.Dense(16, activation=None, name=\"business_embedding\")(x)\n",
    "\n",
    "    return Model([business_id_input, category_input, business_continuous_input], business_embedding, name=\"ItemTower\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss function\n",
    "def triplet_hinge_loss(margin=1.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        # y_pred shape: (batch_size, 3, embedding_dim)\n",
    "        anchor, positive, negative = tf.unstack(y_pred, num=3, axis=1)\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "        \n",
    "        # Hinge loss: max(0, pos_dist - neg_dist + margin)\n",
    "        return tf.reduce_mean(tf.maximum(pos_dist - neg_dist + margin, 0.0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate triplets\n",
    "def generate_triplets(review_df, num_neg_samples=1):\n",
    "    triplets = []\n",
    "    grouped = review_df.groupby('user_id_encoded')\n",
    "\n",
    "    for user_id, group in grouped:\n",
    "        positive_samples = group[group['label'] == 1]\n",
    "        negative_samples = group[group['label'] == 0]\n",
    "        \n",
    "        if positive_samples.empty or negative_samples.empty:\n",
    "            continue  # Skip users without both positive and negative samples\n",
    "        \n",
    "        for _, pos_row in positive_samples.iterrows():\n",
    "            for _, neg_row in negative_samples.sample(num_neg_samples, replace=True).iterrows():\n",
    "                triplets.append((\n",
    "                    user_id,\n",
    "                    pos_row['business_id_encoded'],\n",
    "                    neg_row['business_id_encoded']\n",
    "                ))\n",
    "    \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate triplets for training and testing\n",
    "train_triplets = generate_triplets(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and test inputs\n",
    "def prepare_triplet_inputs(triplets, user_features, business_features, business_category_map, max_category_length=5):\n",
    "    # Replace NaN values with empty lists in `business_category_map`\n",
    "    business_category_map = business_category_map.apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    anchor_indices = triplets[:, 0]\n",
    "    positive_indices = triplets[:, 1]\n",
    "    negative_indices = triplets[:, 2]\n",
    "\n",
    "    anchor_features = [anchor_indices, user_features.take(anchor_indices, axis=0).values]\n",
    "    positive_features = [\n",
    "        positive_indices,\n",
    "        pad_sequences(business_category_map.loc[positive_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(positive_indices, axis=0).values, \n",
    "\n",
    "]\n",
    "    negative_features = [\n",
    "        negative_indices, \n",
    "        pad_sequences(business_category_map.loc[negative_indices].tolist(), maxlen=max_category_length, padding=\"post\"),\n",
    "        business_features.take(negative_indices, axis=0).values,\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        anchor_features[0], anchor_features[1],\n",
    "        positive_features[0], positive_features[1], positive_features[2],\n",
    "        negative_features[0], negative_features[1], negative_features[2]\n",
    "    ]\n",
    "\n",
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']\n",
    "\n",
    "max_category_length = 5\n",
    "\n",
    "train_inputs = prepare_triplet_inputs(train_triplets, user_continuous_features_scaled, business_continuous_features_scaled, business_category_map, max_category_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate towers\n",
    "user_model = user_tower(user_continuous_features_scaled.shape[1])\n",
    "item_model = item_tower(business_continuous_features_scaled.shape[1])\n",
    "\n",
    "# Define inputs for user and business towers\n",
    "user_inputs_model = [Input(shape=(1,), dtype=tf.int32, name=\"user_id_input\"),\n",
    "                     Input(shape=(user_continuous_features_scaled.shape[1],), name=\"user_cont_features_input\")]\n",
    "\n",
    "positive_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"positive_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"positive_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"positive_cont_features_input\")\n",
    "]\n",
    "\n",
    "negative_inputs_model = [\n",
    "    Input(shape=(1,), dtype=tf.int32, name=\"negative_id_input\"),\n",
    "    Input(shape=(max_category_length,), dtype=tf.int32, name=\"negative_category_input\"),\n",
    "    Input(shape=(business_continuous_features_scaled.shape[1],), name=\"negative_cont_features_input\")\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "anchor_embedding = user_model(user_inputs_model)\n",
    "positive_embedding = item_model(positive_inputs_model)\n",
    "negative_embedding = item_model(negative_inputs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_embeddings(embeddings):\n",
    "    # Unpack the embeddings from the list\n",
    "    anchor, positive, negative = embeddings\n",
    "    return tf.stack([anchor, positive, negative], axis=1)\n",
    "\n",
    "triplet_embeddings = Lambda(stack_embeddings, name=\"triplet_embeddings\")(\n",
    "    [anchor_embedding, positive_embedding, negative_embedding]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = user_inputs_model + positive_inputs_model + negative_inputs_model\n",
    "# Build the model\n",
    "triplet_model = Model(\n",
    "    inputs= all_inputs,\n",
    "    outputs=triplet_embeddings,\n",
    "    name=\"triplet_model\"\n",
    ")\n",
    "\n",
    "# Compile with triplet loss\n",
    "triplet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=triplet_hinge_loss(margin=0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the triplet model\n",
    "triplet_model.fit(\n",
    "    x=train_inputs,\n",
    "    y=np.zeros(len(train_inputs[0])),  # Dummy labels, as loss is computed from embeddings\n",
    "    batch_size=32,\n",
    "    # epochs=10,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_path = 'DSSM_Models/Triplet_Hinge_Loss/'\n",
    "\n",
    "# Save the models\n",
    "user_model.save(save_folder_path + 'user_model.keras')\n",
    "item_model.save(save_folder_path + 'item_model.keras')\n",
    "\n",
    "# Save the label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(user_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(business_id_encoder, f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(categories_encoder, f)\n",
    "    \n",
    "# Save the scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(user_scaler, f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(business_scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following section will load the saved models and compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to saved models and encoders\n",
    "save_folder_path = 'DSSM_Models/Triplet_Hinge_Loss/'\n",
    "\n",
    "# Load the saved models\n",
    "user_model = load_model(save_folder_path + 'user_model.keras')\n",
    "item_model = load_model(save_folder_path + 'item_model.keras',                \n",
    "        custom_objects={'CategoryPoolingLayer': CategoryPoolingLayer}\n",
    "    )\n",
    "\n",
    "# Load the saved label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'rb') as f:\n",
    "    user_id_encoder = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'rb') as f:\n",
    "    business_id_encoder = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'rb') as f:\n",
    "    categories_encoder = pickle.load(f)\n",
    "\n",
    "# Load the saved scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'rb') as f:\n",
    "    user_scaler = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'rb') as f:\n",
    "    business_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter test data\n",
    "test_df = test_df[\n",
    "    (test_df['user_id'].isin(user_id_encoder.classes_)) & \n",
    "    (test_df['business_id'].isin(business_id_encoder.classes_))\n",
    "]\n",
    "\n",
    "# Encode user_id and business_id using the loaded encoders\n",
    "test_df['user_id_encoded'] = user_id_encoder.transform(test_df['user_id'])\n",
    "test_df['business_id_encoded'] = business_id_encoder.transform(test_df['business_id'])\n",
    "\n",
    "# Step 2: Prepare inputs for user and business embeddings\n",
    "# Extract user features\n",
    "test_user_ids = test_df['user_id_encoded'].values\n",
    "test_user_cont_features = user_scaler.transform(user_continuous_features_scaled.loc[test_user_ids].values)\n",
    "\n",
    "# Extract business features\n",
    "test_business_ids = test_df['business_id_encoded'].values\n",
    "test_business_cont_features = business_scaler.transform(business_continuous_features_scaled.loc[test_business_ids].values)\n",
    "test_business_categories = business_category_map.loc[test_business_ids].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "test_business_category_padded = pad_sequences(test_business_categories.tolist(), maxlen=5, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Predict embeddings using the loaded models\n",
    "test_user_embeddings = user_model.predict([test_user_ids, test_user_cont_features])\n",
    "test_business_embeddings = item_model.predict([test_business_ids, test_business_category_padded, test_business_cont_features])\n",
    "\n",
    "# Step 4: Compute cosine similarity for each user-business pair\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Create a list of cosine similarities for each record in test_df\n",
    "test_df['predicted_similarity'] = [\n",
    "    compute_cosine_similarity(test_user_embeddings[i], test_business_embeddings[i])\n",
    "    for i in range(len(test_df))\n",
    "]\n",
    "\n",
    "# Step 5: Set the predicted_label based on similarity score\n",
    "test_df['predicted_label'] = (test_df['predicted_similarity'] >= 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Evaluate the performance\n",
    "accuracy = (test_df['label'] == test_df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(test_df['label'], test_df['predicted_similarity'])\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "# Compute precision and recall\n",
    "precision = precision_score(test_df['label'], test_df['predicted_label'])\n",
    "recall = recall_score(test_df['label'], test_df['predicted_label'])\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(test_df['label'], test_df['predicted_label'])\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
