{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def load_dataset(file_lists, prefix_path, chunk_size=10000):\n",
    "    df_dict = {}\n",
    "    prefix_path += \"sampled_\"\n",
    "    for file in file_lists:\n",
    "        try:\n",
    "            df_chunks = []\n",
    "            total_records = 0\n",
    "\n",
    "            for chunk in pd.read_json(prefix_path + file, lines=True, chunksize=chunk_size):\n",
    "                df_chunks.append(chunk)\n",
    "                total_records += chunk.shape[0]\n",
    "\n",
    "            df = pd.concat(df_chunks, ignore_index=True)\n",
    "            df_dict[file] = df\n",
    "            print(f\"Total records in {file}: {df.shape[0]}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    return df_dict\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "folder_path = '../data/'\n",
    "transit_bucket = 'raw_datasets/'\n",
    "target_bucket = 'yelp/'\n",
    "prefix_path = folder_path + transit_bucket + target_bucket\n",
    "file_list = [\n",
    "    \"yelp_academic_dataset_business.json\",\n",
    "    \"yelp_academic_dataset_review.json\",\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "\n",
    "df_concat = df_business.merge(df_review, on='business_id', how='outer', suffixes=('_business', '_review'))\n",
    "\n",
    "user_business = df_concat[[\"user_id\", \"business_id\", \"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate sparse cosine similarity with top N items\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    # A is the sparse matrix (user-item matrix)\n",
    "    # ntop is the number of top similar items you want\n",
    "    # lower_bound is the minimum similarity score to consider\n",
    "\n",
    "    # # Compute the top N cosine similarities in a sparse format\n",
    "    \n",
    "    C = sp_matmul_topn(A.T, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the user_business DataFrame to avoid issues with slicing\n",
    "user_business = user_business.copy()\n",
    "\n",
    "# Create user and business index mappings\n",
    "user_mapping = {user: idx for idx, user in enumerate(user_business['user_id'].unique())}\n",
    "business_mapping = {biz: idx for idx, biz in enumerate(user_business['business_id'].unique())}\n",
    "\n",
    "# Map user_id and business_id to numerical indices\n",
    "user_business['user_idx'] = user_business['user_id'].map(user_mapping)\n",
    "user_business['business_idx'] = user_business['business_id'].map(business_mapping)\n",
    "\n",
    "# Creating the sparse user-item interaction matrix (csr_matrix)\n",
    "user_item_sparse = csr_matrix(\n",
    "    (user_business['stars_review'], (user_business['user_idx'], user_business['business_idx'])),\n",
    "    shape=(len(user_mapping), len(business_mapping))\n",
    ")\n",
    "\n",
    "# Replace any NaN values with 0 in the sparse matrix\n",
    "user_item_sparse.data = np.nan_to_num(user_item_sparse.data)\n",
    "\n",
    "# Compute sparse cosine similarity matrix with top 10 most similar items\n",
    "item_similarity_sparse = sparse_cosine_similarity_topn(user_item_sparse, top_n=50, threshold=0.01,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite (this will create a file-based database)\n",
    "db_path = '../data/processed_data/yelp_ItemCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables for user-item and item-item indexes\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS user_item_index (\n",
    "    user_id TEXT,\n",
    "    business_id TEXT,\n",
    "    stars_review REAL,\n",
    "    PRIMARY KEY (user_id, business_id)\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_user_item ON user_item_index(user_id, business_id)''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS item_item_similarity (\n",
    "    item_id TEXT PRIMARY KEY,\n",
    "    similarity_vector BLOB\n",
    ")''')\n",
    "\n",
    "cursor.execute('''CREATE INDEX idx_item_similarity ON item_item_similarity(item_id)''')\n",
    "\n",
    "# cursor.execute('''CREATE TABLE IF NOT EXISTS user_mapping (\n",
    "#     user_id TEXT PRIMARY KEY,\n",
    "#     user_idx INTEGER\n",
    "# )''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY,\n",
    "    business_idx INTEGER\n",
    ")''')\n",
    "\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_user_item(user_business, conn, batch_size=10000):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    # Insert user-item interactions in batches\n",
    "    total_records = len(user_business)\n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch = user_business.iloc[i:i + batch_size]\n",
    "\n",
    "        cursor.executemany('''INSERT OR IGNORE INTO user_item_index (user_id, business_id, stars_review)\n",
    "                              VALUES (?, ?, ?)''',\n",
    "                           batch[['user_id', 'business_id', 'stars_review']].values.tolist())\n",
    "\n",
    "        # Show progress\n",
    "        print(f\"{i + len(batch)} / {total_records} records stored in user_item_index\")\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_item_vectors(item_similarity_sparse, business_mapping, conn, batch_size=1000, progress_interval=100000):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "\n",
    "    # Iterate over each row (item) in the sparse matrix\n",
    "    for row_idx in range(item_similarity_sparse.shape[0]):\n",
    "        # Get the row as a sparse vector (csr_matrix row)\n",
    "        row_vector = item_similarity_sparse.getrow(row_idx)\n",
    "\n",
    "        # Extract indices and data from the sparse vector\n",
    "        row_indices = row_vector.indices\n",
    "        row_data = row_vector.data\n",
    "\n",
    "        # Serialize only indices and data (not the full matrix)\n",
    "        serialized_row = pickle.dumps((row_indices, row_data))\n",
    "\n",
    "        # Get the item id (business_id)\n",
    "        item_id = list(business_mapping.keys())[row_idx]\n",
    "\n",
    "        # Add the item and its vector to the batch\n",
    "        batch.append((item_id, serialized_row))\n",
    "\n",
    "        # Insert in batches to reduce the number of commits\n",
    "        if len(batch) >= batch_size:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "\n",
    "            # Print progress every progress_interval records\n",
    "            if total_inserted % progress_interval == 0:\n",
    "                print(f\"Inserted {total_inserted} item vectors so far...\")\n",
    "\n",
    "            batch = []  # Clear the batch after committing\n",
    "\n",
    "    total_inserted += len(batch)  # Add any remaining records\n",
    "    # Insert any remaining records after the loop\n",
    "    if batch:\n",
    "        cursor.executemany('''INSERT OR REPLACE INTO item_item_similarity (item_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    # Final progress message\n",
    "    print(f\"Total {total_inserted} item vectors inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_mappings(business_mapping, conn):\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Start a transaction\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "\n",
    "    # Insert user mappings\n",
    "    # cursor.executemany('''INSERT OR REPLACE INTO user_mapping (user_id, user_idx) VALUES (?, ?)''',\n",
    "    #                    [(user_id, idx) for user_id, idx in user_mapping.items()])\n",
    "\n",
    "    # Insert business mappings\n",
    "    cursor.executemany('''INSERT OR REPLACE INTO business_mapping (business_id, business_idx) VALUES (?, ?)''',\n",
    "                       [(business_id, idx) for business_id, idx in business_mapping.items()])\n",
    "\n",
    "    # Commit once at the end of the transaction\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Inserted {len(business_mapping)} business mappings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 / 985732 records stored in user_item_index\n",
      "20000 / 985732 records stored in user_item_index\n",
      "30000 / 985732 records stored in user_item_index\n",
      "40000 / 985732 records stored in user_item_index\n",
      "50000 / 985732 records stored in user_item_index\n",
      "60000 / 985732 records stored in user_item_index\n",
      "70000 / 985732 records stored in user_item_index\n",
      "80000 / 985732 records stored in user_item_index\n",
      "90000 / 985732 records stored in user_item_index\n",
      "100000 / 985732 records stored in user_item_index\n",
      "110000 / 985732 records stored in user_item_index\n",
      "120000 / 985732 records stored in user_item_index\n",
      "130000 / 985732 records stored in user_item_index\n",
      "140000 / 985732 records stored in user_item_index\n",
      "150000 / 985732 records stored in user_item_index\n",
      "160000 / 985732 records stored in user_item_index\n",
      "170000 / 985732 records stored in user_item_index\n",
      "180000 / 985732 records stored in user_item_index\n",
      "190000 / 985732 records stored in user_item_index\n",
      "200000 / 985732 records stored in user_item_index\n",
      "210000 / 985732 records stored in user_item_index\n",
      "220000 / 985732 records stored in user_item_index\n",
      "230000 / 985732 records stored in user_item_index\n",
      "240000 / 985732 records stored in user_item_index\n",
      "250000 / 985732 records stored in user_item_index\n",
      "260000 / 985732 records stored in user_item_index\n",
      "270000 / 985732 records stored in user_item_index\n",
      "280000 / 985732 records stored in user_item_index\n",
      "290000 / 985732 records stored in user_item_index\n",
      "300000 / 985732 records stored in user_item_index\n",
      "310000 / 985732 records stored in user_item_index\n",
      "320000 / 985732 records stored in user_item_index\n",
      "330000 / 985732 records stored in user_item_index\n",
      "340000 / 985732 records stored in user_item_index\n",
      "350000 / 985732 records stored in user_item_index\n",
      "360000 / 985732 records stored in user_item_index\n",
      "370000 / 985732 records stored in user_item_index\n",
      "380000 / 985732 records stored in user_item_index\n",
      "390000 / 985732 records stored in user_item_index\n",
      "400000 / 985732 records stored in user_item_index\n",
      "410000 / 985732 records stored in user_item_index\n",
      "420000 / 985732 records stored in user_item_index\n",
      "430000 / 985732 records stored in user_item_index\n",
      "440000 / 985732 records stored in user_item_index\n",
      "450000 / 985732 records stored in user_item_index\n",
      "460000 / 985732 records stored in user_item_index\n",
      "470000 / 985732 records stored in user_item_index\n",
      "480000 / 985732 records stored in user_item_index\n",
      "490000 / 985732 records stored in user_item_index\n",
      "500000 / 985732 records stored in user_item_index\n",
      "510000 / 985732 records stored in user_item_index\n",
      "520000 / 985732 records stored in user_item_index\n",
      "530000 / 985732 records stored in user_item_index\n",
      "540000 / 985732 records stored in user_item_index\n",
      "550000 / 985732 records stored in user_item_index\n",
      "560000 / 985732 records stored in user_item_index\n",
      "570000 / 985732 records stored in user_item_index\n",
      "580000 / 985732 records stored in user_item_index\n",
      "590000 / 985732 records stored in user_item_index\n",
      "600000 / 985732 records stored in user_item_index\n",
      "610000 / 985732 records stored in user_item_index\n",
      "620000 / 985732 records stored in user_item_index\n",
      "630000 / 985732 records stored in user_item_index\n",
      "640000 / 985732 records stored in user_item_index\n",
      "650000 / 985732 records stored in user_item_index\n",
      "660000 / 985732 records stored in user_item_index\n",
      "670000 / 985732 records stored in user_item_index\n",
      "680000 / 985732 records stored in user_item_index\n",
      "690000 / 985732 records stored in user_item_index\n",
      "700000 / 985732 records stored in user_item_index\n",
      "710000 / 985732 records stored in user_item_index\n",
      "720000 / 985732 records stored in user_item_index\n",
      "730000 / 985732 records stored in user_item_index\n",
      "740000 / 985732 records stored in user_item_index\n",
      "750000 / 985732 records stored in user_item_index\n",
      "760000 / 985732 records stored in user_item_index\n",
      "770000 / 985732 records stored in user_item_index\n",
      "780000 / 985732 records stored in user_item_index\n",
      "790000 / 985732 records stored in user_item_index\n",
      "800000 / 985732 records stored in user_item_index\n",
      "810000 / 985732 records stored in user_item_index\n",
      "820000 / 985732 records stored in user_item_index\n",
      "830000 / 985732 records stored in user_item_index\n",
      "840000 / 985732 records stored in user_item_index\n",
      "850000 / 985732 records stored in user_item_index\n",
      "860000 / 985732 records stored in user_item_index\n",
      "870000 / 985732 records stored in user_item_index\n",
      "880000 / 985732 records stored in user_item_index\n",
      "890000 / 985732 records stored in user_item_index\n",
      "900000 / 985732 records stored in user_item_index\n",
      "910000 / 985732 records stored in user_item_index\n",
      "920000 / 985732 records stored in user_item_index\n",
      "930000 / 985732 records stored in user_item_index\n",
      "940000 / 985732 records stored in user_item_index\n",
      "950000 / 985732 records stored in user_item_index\n",
      "960000 / 985732 records stored in user_item_index\n",
      "970000 / 985732 records stored in user_item_index\n",
      "980000 / 985732 records stored in user_item_index\n",
      "985732 / 985732 records stored in user_item_index\n",
      "Total 78059 item vectors inserted.\n",
      "Inserted 162080 user mappings and 78059 business mappings.\n"
     ]
    }
   ],
   "source": [
    "# Insert user-item index with progress\n",
    "insert_user_item(user_business, conn)\n",
    "\n",
    "# Insert item vectors into the database\n",
    "insert_item_vectors(item_similarity_sparse, business_mapping, conn)\n",
    "\n",
    "# Insert user and business mappings\n",
    "insert_mappings(business_mapping, conn)\n",
    "\n",
    "# Close the connection when done\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
