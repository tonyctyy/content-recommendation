{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d3c2c9",
   "metadata": {},
   "source": [
    "### Clustering based Collaborative Filtering - Model & Index  \n",
    "This notebook demonstrates how to build a cluster-based collaborative filtering model using Yelp dataset. You can adjust the model to add more features or change the hyperparameters to improve the model performance. The index is built and stored in the `yelp_UserCF.db` & `yelp_ClusteringCF.db` file.  \n",
    "\n",
    "#### Pre-requisites\n",
    "1. Have the processed Yelp dataset in the `../../data/processed_data/yelp_data` folder.  \n",
    "2. Have a CSV file (e.g., user_clusters.csv) in the `../../data/processed_data` folder with user_id and cluster_id columns.  \n",
    "3. Have the virtual environment set up and used for the notebook.  \n",
    "\n",
    "#### Move to Production\n",
    "1. Copy the `yelp_ClusterCF.db` file to the `../../data/processed_data`folder.\n",
    "2. Update the `ClusterCF.py` file in the `../backend/models` folder if there are changes in the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7657dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utilities import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_dot_topn import sp_matmul_topn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sqlite3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b9d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n"
     ]
    }
   ],
   "source": [
    "# Load Yelp data\n",
    "db_folder = '../../data/processed_data/yelp_data/'\n",
    "data_files = ['business', 'categories', 'review']\n",
    "yelp_data = load_data_from_db(db_folder, data_files)\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67092fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 985732 user-business interactions.\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "df_business = yelp_data[\"business\"]\n",
    "df_review = yelp_data[\"review\"]\n",
    "user_mapping, business_mapping, user_business = get_user_business(df_business, df_review)\n",
    "print(f\"Prepared {len(user_business)} user-business interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effcbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99812 cluster assignments.\n"
     ]
    }
   ],
   "source": [
    "# Load cluster assignments\n",
    "cluster_file = '../data_processing/clustered_users.xlsx'\n",
    "cluster_df = pd.read_excel(cluster_file)\n",
    "user_to_cluster = pd.DataFrame({\n",
    "    'user_id': cluster_df['user_id'],\n",
    "    'cluster_id': cluster_df['cluster'].astype(str)\n",
    "})\n",
    "print(f\"Loaded {len(cluster_df)} cluster assignments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fa2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset contains 918151 user-business-cluster records.\n"
     ]
    }
   ],
   "source": [
    "# Merge user-business interactions with cluster mappings\n",
    "user_business_clusters = user_business.merge(user_to_cluster, on='user_id', how='inner')\n",
    "print(f\"Merged dataset contains {len(user_business_clusters)} user-business-cluster records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "149d611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated to 721327 cluster-business interactions.\n"
     ]
    }
   ],
   "source": [
    "# Aggregate ratings at cluster level (mean stars_review per cluster-business pair)\n",
    "cluster_business = user_business_clusters.groupby(['cluster_id', 'business_id'])['stars_review'].mean().reset_index()\n",
    "print(f\"Aggregated to {len(cluster_business)} cluster-business interactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1f6e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 577061 records, Test data: 144266 records.\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test (use train for model)\n",
    "train_data, test_data = train_test_split(cluster_business, test_size=0.2, random_state=42)\n",
    "cluster_business = train_data.copy()\n",
    "print(f\"Training data: {len(cluster_business)} records, Test data: {len(test_data)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f09cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings: 2503 clusters, 78059 businesses.\n"
     ]
    }
   ],
   "source": [
    "# Map to indices\n",
    "cluster_mapping = {str(cid): idx for idx, cid in enumerate(cluster_business['cluster_id'].unique())}\n",
    "cluster_business['cluster_idx'] = cluster_business['cluster_id'].map(cluster_mapping)\n",
    "cluster_business['business_idx'] = cluster_business['business_id'].map(business_mapping)\n",
    "print(f\"Created mappings: {len(cluster_mapping)} clusters, {len(business_mapping)} businesses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efabe62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created sparse cluster-item matrix with shape (2503, 78059).\n"
     ]
    }
   ],
   "source": [
    "# Create sparse cluster-item matrix\n",
    "cluster_item_sparse = csr_matrix(\n",
    "    (cluster_business['stars_review'], (cluster_business['cluster_idx'], cluster_business['business_idx'])),\n",
    "    shape=(len(cluster_mapping), len(business_mapping))\n",
    ")\n",
    "cluster_item_sparse.data = np.nan_to_num(cluster_item_sparse.data)\n",
    "print(f\"Created sparse cluster-item matrix with shape {cluster_item_sparse.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51c18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster-cluster similarity (cosine similarity)\n",
    "def sparse_cosine_similarity_topn(A, top_n, threshold=0):\n",
    "    C = sp_matmul_topn(A, A.T, top_n=top_n, threshold=threshold, n_threads=4, sort=True)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeeb90e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed cluster-cluster similarity matrix with 125150 non-zero elements.\n"
     ]
    }
   ],
   "source": [
    "cluster_similarity_sparse = sparse_cosine_similarity_topn(cluster_item_sparse, top_n=50, threshold=0.01)\n",
    "print(f\"Computed cluster-cluster similarity matrix with {cluster_similarity_sparse.nnz} non-zero elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbb51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database optimization and insertion functions\n",
    "def optimize_db(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executescript('''\n",
    "        PRAGMA synchronous = OFF;\n",
    "        PRAGMA journal_mode = MEMORY;\n",
    "        PRAGMA temp_store = MEMORY;\n",
    "        PRAGMA cache_size = 1000000;\n",
    "    ''')\n",
    "    conn.commit()\n",
    "\n",
    "def insert_cluster_item(cluster_business, conn, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_records = len(cluster_business)\n",
    "    data = cluster_business[['cluster_id', 'business_id', 'stars_review']].values.tolist()\n",
    "    try:\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            cursor.executemany('''INSERT OR IGNORE INTO cluster_item_index (cluster_id, business_id, stars_review)\n",
    "                                  VALUES (?, ?, ?)''', batch)\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {i + len(batch)} / {total_records} cluster-item records.\")\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_records} cluster-item records inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting cluster-item records: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_cluster_vectors(cluster_similarity_sparse, cluster_mapping, conn, batch_size=5000, progress_interval=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    total_inserted = 0\n",
    "    batch = []\n",
    "    cluster_keys = list(cluster_mapping.keys())\n",
    "    try:\n",
    "        for row_idx in range(cluster_similarity_sparse.shape[0]):\n",
    "            row_vector = cluster_similarity_sparse.getrow(row_idx)\n",
    "            serialized_row = pickle.dumps((row_vector.indices, row_vector.data))\n",
    "            cluster_id = cluster_keys[row_idx]\n",
    "            batch.append((cluster_id, serialized_row))\n",
    "            if len(batch) >= batch_size:\n",
    "                cursor.executemany('''INSERT OR REPLACE INTO cluster_cluster_similarity (cluster_id, similarity_vector)\n",
    "                                      VALUES (?, ?)''', batch)\n",
    "                total_inserted += len(batch)\n",
    "                if total_inserted % progress_interval == 0:\n",
    "                    print(f\"Inserted {total_inserted} cluster vectors...\")\n",
    "                batch = []\n",
    "        if batch:\n",
    "            cursor.executemany('''INSERT OR REPLACE INTO cluster_cluster_similarity (cluster_id, similarity_vector)\n",
    "                                  VALUES (?, ?)''', batch)\n",
    "            total_inserted += len(batch)\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_inserted} cluster vectors inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting cluster vectors: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def insert_mappings(mapping, conn, table_name, key_col, val_col, batch_size=50000):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('BEGIN TRANSACTION')\n",
    "    data = list(mapping.items())\n",
    "    total_records = len(data)\n",
    "    try:\n",
    "        for i in range(0, total_records, batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            cursor.executemany(f'''INSERT OR REPLACE INTO {table_name} ({key_col}, {val_col})\n",
    "                                   VALUES (?, ?)''', batch)\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                conn.commit()\n",
    "                print(f\"Inserted {i + len(batch)} / {total_records} {table_name} records.\")\n",
    "        conn.commit()\n",
    "        print(f\"Total {total_records} {table_name} records inserted.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error inserting {table_name} records: {e}\")\n",
    "        conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796392db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up database\n",
    "db_path = './cluster_data/yelp_ClusterUserCF.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "optimize_db(conn)\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS cluster_item_index (\n",
    "    cluster_id TEXT, business_id TEXT, stars_review REAL, PRIMARY KEY (cluster_id, business_id)\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_cluster_item ON cluster_item_index(cluster_id, business_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS cluster_cluster_similarity (\n",
    "    cluster_id TEXT PRIMARY KEY, similarity_vector BLOB\n",
    ")''')\n",
    "cursor.execute('''CREATE INDEX IF NOT EXISTS idx_cluster_similarity ON cluster_cluster_similarity(cluster_id)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS cluster_mapping (\n",
    "    cluster_id TEXT PRIMARY KEY, cluster_idx INTEGER\n",
    ")''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS business_mapping (\n",
    "    business_id TEXT PRIMARY KEY, business_idx INTEGER\n",
    ")''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a9fe280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 / 577061 cluster-item records.\n",
      "Inserted 300000 / 577061 cluster-item records.\n",
      "Inserted 550000 / 577061 cluster-item records.\n",
      "Total 577061 cluster-item records inserted.\n",
      "Total 2503 cluster vectors inserted.\n",
      "Inserted 2503 / 2503 cluster_mapping records.\n",
      "Total 2503 cluster_mapping records inserted.\n",
      "Inserted 50000 / 78059 business_mapping records.\n",
      "Total 78059 business_mapping records inserted.\n"
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "insert_cluster_item(cluster_business, conn)\n",
    "insert_cluster_vectors(cluster_similarity_sparse, cluster_mapping, conn)\n",
    "insert_mappings(cluster_mapping, conn, 'cluster_mapping', 'cluster_id', 'cluster_idx')\n",
    "insert_mappings(business_mapping, conn, 'business_mapping', 'business_id', 'business_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186bff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950f294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
