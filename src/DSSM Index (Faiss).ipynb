{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import Input, Layer, Lambda\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryPoolingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CategoryPoolingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to saved models and encoders\n",
    "save_folder_path = 'DSSM_Models/Triplet_Hinge_Loss/'\n",
    "\n",
    "# Load the saved models\n",
    "user_model = load_model(save_folder_path + 'user_model.keras')\n",
    "item_model = load_model(save_folder_path + 'item_model.keras',                \n",
    "        custom_objects={'CategoryPoolingLayer': CategoryPoolingLayer}\n",
    "    )\n",
    "\n",
    "# Load the saved label encoders\n",
    "with open(save_folder_path + 'user_id_encoder.pkl', 'rb') as f:\n",
    "    user_id_encoder = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'business_id_encoder.pkl', 'rb') as f:\n",
    "    business_id_encoder = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'categories_encoder.pkl', 'rb') as f:\n",
    "    categories_encoder = pickle.load(f)\n",
    "\n",
    "# Load the saved scalers\n",
    "with open(save_folder_path + 'user_scaler.pkl', 'rb') as f:\n",
    "    user_scaler = pickle.load(f)\n",
    "\n",
    "with open(save_folder_path + 'business_scaler.pkl', 'rb') as f:\n",
    "    business_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database folder path and file names\n",
    "db_folder = '../data/processed_data/yelp_data/'\n",
    "db_files = ['yelp_business_data.db', 'yelp_review_data.db', 'yelp_user_data.db', 'yelp_tip_data.db']\n",
    "db_paths = [db_folder + db_file for db_file in db_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the databases and load data\n",
    "def load_data_from_db():\n",
    "    data = {}\n",
    "    \n",
    "    # Open connections and read tables\n",
    "    conns = [sqlite3.connect(db_path) for db_path in db_paths]\n",
    "    try:\n",
    "        # Load tables from the databases\n",
    "        data['business'] = pd.read_sql_query(\"SELECT * FROM business_details\", conns[0])\n",
    "        data['categories'] = pd.read_sql_query(\"SELECT * FROM business_categories\", conns[0])\n",
    "        data['review'] = pd.read_sql_query(\"SELECT * FROM review_data\", conns[1])\n",
    "        data['user'] = pd.read_sql_query(\"SELECT * FROM user_data\", conns[2])\n",
    "        data['tip'] = pd.read_sql_query(\"SELECT * FROM tip_data\", conns[3])\n",
    "        \n",
    "    finally:\n",
    "        # Close all database connections\n",
    "        for conn in conns:\n",
    "            conn.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78059 rows from business table.\n",
      "Loaded 360656 rows from categories table.\n",
      "Loaded 980418 rows from review table.\n",
      "Loaded 229447 rows from user table.\n",
      "Loaded 173085 rows from tip table.\n"
     ]
    }
   ],
   "source": [
    "# Load data into a dictionary\n",
    "yelp_data = load_data_from_db()\n",
    "\n",
    "# Check loaded data\n",
    "for table, df in yelp_data.items():\n",
    "    print(f\"Loaded {len(df)} rows from {table} table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_con_feature_lst = [\n",
    "                        'review_count', \n",
    "                        'useful', \n",
    "                        'funny', \n",
    "                        'cool', \n",
    "                        'fans', \n",
    "                        'average_stars'\n",
    "                        ]\n",
    "business_con_feature_lst = [\n",
    "                        'stars', \n",
    "                        'review_count', \n",
    "                        # 'latitude', \n",
    "                        # 'longitude'\n",
    "                        ]\n",
    "\n",
    "# add user features that start with 'compliment_'\n",
    "user_compliment_feature_lst = [col for col in yelp_data['user'].columns if 'compliment_' in col]\n",
    "user_con_feature_lst += user_compliment_feature_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user data\n",
    "user_df = yelp_data['user']\n",
    "user_df['yelping_since'] = pd.to_datetime(user_df['yelping_since'])\n",
    "\n",
    "# Preprocess business data\n",
    "business_df = yelp_data['business']\n",
    "business_df['is_open'] = business_df['is_open'].fillna(0).astype(int)\n",
    "\n",
    "# Preprocess review data\n",
    "review_df = yelp_data['review']\n",
    "# Create labels for review data\n",
    "review_df['label'] = (review_df['stars'] >= 4).astype(int)\n",
    "\n",
    "# Preprocess tip data\n",
    "tip_df = yelp_data['tip']\n",
    "\n",
    "# Preprocess categories data\n",
    "categories_df = yelp_data['categories']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yueny\\AppData\\Local\\Temp\\ipykernel_17980\\1600752797.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['user_id_encoded'] = user_id_encoder.transform(review_df['user_id'])\n",
      "C:\\Users\\yueny\\AppData\\Local\\Temp\\ipykernel_17980\\1600752797.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['business_id_encoded'] = business_id_encoder.transform(review_df['business_id'])\n"
     ]
    }
   ],
   "source": [
    "# Filter out unseen user_id and business_id\n",
    "review_df = review_df[\n",
    "    (review_df['user_id'].isin(user_id_encoder.classes_)) & \n",
    "    (review_df['business_id'].isin(business_id_encoder.classes_))\n",
    "]\n",
    "\n",
    "# Encode user_id and business_id\n",
    "review_df['user_id_encoded'] = user_id_encoder.transform(review_df['user_id'])\n",
    "review_df['business_id_encoded'] = business_id_encoder.transform(review_df['business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unseen categories\n",
    "categories_df = categories_df[categories_df['category'].isin(categories_encoder.classes_)]\n",
    "categories_df['category_encoded'] = categories_encoder.transform(categories_df['category'])\n",
    "\n",
    "categories_grouped = categories_df.groupby('business_id')['category_encoded'].apply(list).reset_index()\n",
    "\n",
    "# Merge the categories with the business data, name the column 'category_encoded'\n",
    "business_df = business_df.merge(categories_grouped, on='business_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviews and average review for each business\n",
    "business_review_count = review_df.groupby('business_id').size()\n",
    "business_avg_review = review_df.groupby('business_id')['stars'].mean()\n",
    "business_df['review_count'] = business_review_count\n",
    "business_df['avg_review'] = business_avg_review # similar to stars, but this is adjusted for the number of reviews extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Extract numerical features for embedding\n",
    "user_features = user_df[user_con_feature_lst].fillna(0)\n",
    "# Example: Extract numerical features\n",
    "business_features = business_df[business_con_feature_lst].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['user_id_encoded'] = user_id_encoder.fit_transform(user_df['user_id'])\n",
    "business_df['business_id_encoded'] = business_id_encoder.fit_transform(business_df['business_id'])\n",
    "\n",
    "# Save number of unique users and businesses for embedding input_dim\n",
    "num_users = user_df['user_id_encoded'].max() + 1\n",
    "num_businesses = business_df['business_id_encoded'].max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize user continuous features\n",
    "user_continuous_features = user_df[['review_count', 'useful', 'funny', 'cool', 'fans', 'average_stars']].fillna(0)\n",
    "user_scaler = StandardScaler()\n",
    "user_continuous_features_scaled = user_scaler.fit_transform(user_continuous_features)\n",
    "\n",
    "# Standardize business continuous features\n",
    "business_continuous_features = business_df[['stars', 'review_count', 'latitude', 'longitude']].fillna(0)\n",
    "business_scaler = StandardScaler()\n",
    "business_continuous_features_scaled = business_scaler.fit_transform(business_continuous_features)\n",
    "\n",
    "# Ensure continuous features are pandas DataFrames\n",
    "user_continuous_features_scaled = pd.DataFrame(user_continuous_features_scaled, index=user_df['user_id_encoded'])\n",
    "business_continuous_features_scaled = pd.DataFrame(business_continuous_features_scaled, index=business_df['business_id_encoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_map = business_df.set_index('business_id_encoded')['category_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split review_df into train and test sets\n",
    "train_df, test_df = train_test_split(review_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2440/2440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 991us/step\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the Faiss index for business embeddings\n",
    "def create_faiss_index(item_model, business_ids, business_cont_features, business_category_map, max_category_length=5):\n",
    "    business_categories = business_category_map.loc[business_ids].apply(\n",
    "        lambda x: x if isinstance(x, list) else []\n",
    "    )\n",
    "    business_category_padded = pad_sequences(business_categories.tolist(), maxlen=max_category_length, padding=\"post\")\n",
    "\n",
    "    # Predict embeddings\n",
    "    business_embeddings = item_model.predict([business_ids, business_category_padded, business_cont_features])\n",
    "\n",
    "    # Create a Faiss index for L2 similarity\n",
    "    index = faiss.IndexFlatL2(business_embeddings.shape[1])  # Assuming 16-dimensional embeddings\n",
    "    index.add(business_embeddings)\n",
    "    return index, business_embeddings\n",
    "\n",
    "business_ids = business_continuous_features_scaled.index.values\n",
    "faiss_index, business_embeddings = create_faiss_index(\n",
    "    item_model, business_ids, \n",
    "    business_continuous_features_scaled.values, \n",
    "    business_category_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Query top-k businesses for a given user\n",
    "def query_top_k(user_id, user_model, faiss_index, k=5):\n",
    "    # Encode user_id and get continuous features\n",
    "    user_id_encoded = user_id_encoder.transform([user_id])[0]\n",
    "    user_cont_features = user_scaler.transform(\n",
    "        user_continuous_features_scaled.loc[[user_id_encoded]].values\n",
    "    )\n",
    "\n",
    "    # Predict the user's embedding\n",
    "    user_embedding = user_model.predict([np.array([user_id_encoded]), user_cont_features])\n",
    "\n",
    "    # Perform ANN search using Faiss\n",
    "    distances, indices = faiss_index.search(user_embedding, k)\n",
    "\n",
    "    # Return top-k businesses and distances\n",
    "    top_k_business_ids = business_ids[indices.flatten()]\n",
    "    return top_k_business_ids, distances.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id_encoded</th>\n",
       "      <th>business_id_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238774</th>\n",
       "      <td>JellhrJZmZeWbmpGk1ox1A</td>\n",
       "      <td>9HQLEChkam3GMBQn0SmvVw</td>\n",
       "      <td>GvmLqW2tMkQ7F2hhOB8vSw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2017-01-01 17:38:18</td>\n",
       "      <td>My mother in law lives by Tucson Estates so at...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36725</td>\n",
       "      <td>22025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763006</th>\n",
       "      <td>yU4OqzNxSTlIh1liOTyXSQ</td>\n",
       "      <td>GKCzx6kfI1roSojfoPFsfA</td>\n",
       "      <td>tsx84z4c0B-y6J5fqfvBqg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2019-07-18 23:41:12</td>\n",
       "      <td>The food was really good. There was no wait fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62034</td>\n",
       "      <td>70580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878504</th>\n",
       "      <td>08lb0_fFuyCc01X21E_R9Q</td>\n",
       "      <td>eTzE7DauSODqmviZ5YfCTg</td>\n",
       "      <td>tM32Az6rP1L_flhNcObl0w</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-11-19 01:42:06</td>\n",
       "      <td>I think it's on the way to four stars, but not...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152054</td>\n",
       "      <td>69948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400581</th>\n",
       "      <td>U182l8gkfSM4xm7r8kRSeg</td>\n",
       "      <td>pPoQ0qeWVDzvdmJAFfH70g</td>\n",
       "      <td>0Xm1wedwnMJ1iKXz8vUDSw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014-10-26 17:19:11</td>\n",
       "      <td>Had the scotch eggs, corned beef and cabbage a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>191193</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446141</th>\n",
       "      <td>JLiLDkATwZxjnfAfCfF8rA</td>\n",
       "      <td>FCI2XcNy9zYQo0EV_pEBQQ</td>\n",
       "      <td>zuDXSlqm2veuwTD_Kl-pkw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2019-12-03 01:48:30</td>\n",
       "      <td>I thought working at Panera in high school was...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58034</td>\n",
       "      <td>77940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id                 user_id  \\\n",
       "238774  JellhrJZmZeWbmpGk1ox1A  9HQLEChkam3GMBQn0SmvVw   \n",
       "763006  yU4OqzNxSTlIh1liOTyXSQ  GKCzx6kfI1roSojfoPFsfA   \n",
       "878504  08lb0_fFuyCc01X21E_R9Q  eTzE7DauSODqmviZ5YfCTg   \n",
       "400581  U182l8gkfSM4xm7r8kRSeg  pPoQ0qeWVDzvdmJAFfH70g   \n",
       "446141  JLiLDkATwZxjnfAfCfF8rA  FCI2XcNy9zYQo0EV_pEBQQ   \n",
       "\n",
       "                   business_id  stars                 date  \\\n",
       "238774  GvmLqW2tMkQ7F2hhOB8vSw    5.0  2017-01-01 17:38:18   \n",
       "763006  tsx84z4c0B-y6J5fqfvBqg    4.0  2019-07-18 23:41:12   \n",
       "878504  tM32Az6rP1L_flhNcObl0w    3.0  2021-11-19 01:42:06   \n",
       "400581  0Xm1wedwnMJ1iKXz8vUDSw    5.0  2014-10-26 17:19:11   \n",
       "446141  zuDXSlqm2veuwTD_Kl-pkw    5.0  2019-12-03 01:48:30   \n",
       "\n",
       "                                                     text  useful  funny  \\\n",
       "238774  My mother in law lives by Tucson Estates so at...       2      0   \n",
       "763006  The food was really good. There was no wait fo...       1      0   \n",
       "878504  I think it's on the way to four stars, but not...       3      0   \n",
       "400581  Had the scotch eggs, corned beef and cabbage a...       5      1   \n",
       "446141  I thought working at Panera in high school was...       1      0   \n",
       "\n",
       "        cool  label  user_id_encoded  business_id_encoded  \n",
       "238774     1      1            36725                22025  \n",
       "763006     0      1            62034                70580  \n",
       "878504     0      0           152054                69948  \n",
       "400581     0      1           191193                 1967  \n",
       "446141     1      1            58034                77940  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "               business_id  similarity_score\n",
      "0   tttwCEpskb7HdSQS8szUFA          0.529427\n",
      "1   iSnPc_3IHqywTvi9IQqxew          0.531635\n",
      "2   C43IEVBroD_3YiumPPLLdQ          0.533646\n",
      "3   lFYqN66bnwx8MiaIAtesoA          0.536562\n",
      "4   iksVwRfpWymIUUFqw0tXpw          0.541931\n",
      "..                     ...               ...\n",
      "95  K-t2yan_iLwcxYf7-1Or5w          0.609884\n",
      "96  f8WKIeT7HMAOedo54Nrd7Q          0.610242\n",
      "97  ZgTXA6x_FX_KkRKu9KXy8A          0.610531\n",
      "98  PxCyMdTBylodcabsidGDLA          0.610602\n",
      "99  01YJCek52uMnAfmBbloX8A          0.610736\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yueny\\.virtualenvs\\content-recommendation-0SgTkEMC\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Example usage\n",
    "user_id = \"9HQLEChkam3GMBQn0SmvVw\"  # Replace with an actual user_id from your dataset\n",
    "top_k_business_ids, scores = query_top_k(user_id, user_model, faiss_index, k=100)\n",
    "\n",
    "# Decode business IDs back to their original format\n",
    "decoded_business_ids = business_id_encoder.inverse_transform(top_k_business_ids)\n",
    "result_df = pd.DataFrame({\n",
    "    'business_id': decoded_business_ids,\n",
    "    'similarity_score': scores\n",
    "})\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 20 46 76  7 81 55 34 36 93]\n",
      " [ 1 14 49 68 70 62 42 67 93  8]\n",
      " [ 2 99 21 48 67 71 34 38 35 18]\n",
      " [ 3 91 50 79 14 70 81 35 60  7]\n",
      " [ 4 93 40 28 49  6 79 67 55 64]]\n"
     ]
    }
   ],
   "source": [
    "D, I = index.search(data[:5], 10)  # search for the 10 nearest neighbors of the first 5 vectors\n",
    "print(I)  # Output the indices of the nearest neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content-recommendation-0SgTkEMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
